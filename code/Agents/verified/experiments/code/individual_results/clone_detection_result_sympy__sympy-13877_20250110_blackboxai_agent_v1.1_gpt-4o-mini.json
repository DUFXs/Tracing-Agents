{
    "instance_id": "sympy__sympy-13877",
    "agent": "20250110_blackboxai_agent_v1.1",
    "agent_patch": {
        ".gitignore": "index 13ee70ccae..4b6ee33b52 100644\n \n # pytest related data file for slow tests\n .ci/durations.log\n.aider/\n",
        "sympy/matrices/matrices.py": "index 97cde62e26..0787808cdd 100644\n             def entry(i, j):\n                 ret = (pivot_val*tmp_mat[i, j + 1] - mat[pivot_pos, j + 1]*tmp_mat[i, 0]) / cumm\n                 if not ret.is_Atom:\ntry:\nret = cancel(ret)\nexcept TypeError:\n# Skip cancellation if NaN comparison error occurs\npass\n                 return ret\n \n             return sign*bareiss(self._new(mat.rows - 1, mat.cols - 1, entry), pivot_val)\n \ntry:\nreturn cancel(bareiss(self))\nexcept TypeError:\n# If cancel fails due to NaN comparison, return uncanceled result\nreturn bareiss(self)\n \n     def _eval_det_berkowitz(self):\n         \"\"\" Use the Berkowitz algorithm to compute the determinant.\"\"\"\n",
        "sympy/matrices/matrices.py.bak": "new file mode 100644\nindex 0000000000..97cde62e26\nfrom __future__ import print_function, division\n\nimport collections\nfrom mpmath.libmp.libmpf import prec_to_dps\nfrom sympy.assumptions.refine import refine\nfrom sympy.core.add import Add\nfrom sympy.core.basic import Basic, Atom\nfrom sympy.core.expr import Expr\nfrom sympy.core.power import Pow\nfrom sympy.core.symbol import (Symbol, Dummy, symbols,\n_uniquely_named_symbol)\nfrom sympy.core.numbers import Integer, ilcm, Float\nfrom sympy.core.singleton import S\nfrom sympy.core.sympify import sympify\nfrom sympy.functions.elementary.miscellaneous import sqrt, Max, Min\nfrom sympy.functions import Abs, exp, factorial\nfrom sympy.polys import PurePoly, roots, cancel, gcd\nfrom sympy.printing import sstr\nfrom sympy.simplify import simplify as _simplify, signsimp, nsimplify\nfrom sympy.core.compatibility import reduce, as_int, string_types\n\nfrom sympy.utilities.iterables import flatten, numbered_symbols\nfrom sympy.core.decorators import call_highest_priority\nfrom sympy.core.compatibility import is_sequence, default_sort_key, range, \\\nNotIterable\n\n\nfrom types import FunctionType\n\nfrom .common import (a2idx, classof, MatrixError, ShapeError,\nNonSquareMatrixError, MatrixCommon)\n\n\ndef _iszero(x):\n\"\"\"Returns True if x is zero.\"\"\"\ntry:\nreturn x.is_zero\nexcept AttributeError:\nreturn None\n\n\nclass DeferredVector(Symbol, NotIterable):\n\"\"\"A vector whose components are deferred (e.g. for use with lambdify)\n\nExamples\n========\n\n>>> from sympy import DeferredVector, lambdify\n>>> X = DeferredVector( 'X' )\n>>> X\nX\n>>> expr = (X[0] + 2, X[2] + 3)\n>>> func = lambdify( X, expr)\n>>> func( [1, 2, 3] )\n(3, 6)\n\"\"\"\n\ndef __getitem__(self, i):\nif i == -0:\ni = 0\nif i < 0:\nraise IndexError('DeferredVector index out of range')\ncomponent_name = '%s[%d]' % (self.name, i)\nreturn Symbol(component_name)\n\ndef __str__(self):\nreturn sstr(self)\n\ndef __repr__(self):\nreturn \"DeferredVector('%s')\" % self.name\n\n\nclass MatrixDeterminant(MatrixCommon):\n\"\"\"Provides basic matrix determinant operations.\nShould not be instantiated directly.\"\"\"\n\ndef _eval_berkowitz_toeplitz_matrix(self):\n\"\"\"Return (A,T) where T the Toeplitz matrix used in the Berkowitz algorithm\ncorresponding to `self` and A is the first principal submatrix.\"\"\"\n\n# the 0 x 0 case is trivial\nif self.rows == 0 and self.cols == 0:\nreturn self._new(1,1, [S.One])\n\n#\n# Partition self = [ a_11  R ]\n#                  [ C     A ]\n#\n\na, R = self[0,0],   self[0, 1:]\nC, A = self[1:, 0], self[1:,1:]\n\n#\n# The Toeplitz matrix looks like\n#\n#  [ 1                                     ]\n#  [ -a         1                          ]\n#  [ -RC       -a        1                 ]\n#  [ -RAC     -RC       -a       1         ]\n#  [ -RA**2C -RAC      -RC      -a       1 ]\n#  etc.\n\n# Compute the diagonal entries.\n# Because multiplying matrix times vector is so much\n# more efficient than matrix times matrix, recursively\n# compute -R * A**n * C.\ndiags = [C]\nfor i in range(self.rows - 2):\ndiags.append(A * diags[i])\ndiags = [(-R*d)[0, 0] for d in diags]\ndiags = [S.One, -a] + diags\n\ndef entry(i,j):\nif j > i:\nreturn S.Zero\nreturn diags[i - j]\n\ntoeplitz = self._new(self.cols + 1, self.rows, entry)\nreturn (A, toeplitz)\n\ndef _eval_berkowitz_vector(self):\n\"\"\" Run the Berkowitz algorithm and return a vector whose entries\nare the coefficients of the characteristic polynomial of `self`.\n\nGiven N x N matrix, efficiently compute\ncoefficients of characteristic polynomials of 'self'\nwithout division in the ground domain.\n\nThis method is particularly useful for computing determinant,\nprincipal minors and characteristic polynomial when 'self'\nhas complicated coefficients e.g. polynomials. Semi-direct\nusage of this algorithm is also important in computing\nefficiently sub-resultant PRS.\n\nAssuming that M is a square matrix of dimension N x N and\nI is N x N identity matrix, then the Berkowitz vector is\nan N x 1 vector whose entries are coefficients of the\npolynomial\n\ncharpoly(M) = det(t*I - M)\n\nAs a consequence, all polynomials generated by Berkowitz\nalgorithm are monic.\n\nFor more information on the implemented algorithm refer to:\n\n[1] S.J. Berkowitz, On computing the determinant in small\nparallel time using a small number of processors, ACM,\nInformation Processing Letters 18, 1984, pp. 147-150\n\n[2] M. Keber, Division-Free computation of sub-resultants\nusing Bezout matrices, Tech. Report MPI-I-2006-1-006,\nSaarbrucken, 2006\n\"\"\"\n\n# handle the trivial cases\nif self.rows == 0 and self.cols == 0:\nreturn self._new(1, 1, [S.One])\nelif self.rows == 1 and self.cols == 1:\nreturn self._new(2, 1, [S.One, -self[0,0]])\n\nsubmat, toeplitz = self._eval_berkowitz_toeplitz_matrix()\nreturn toeplitz * submat._eval_berkowitz_vector()\n\ndef _eval_det_bareiss(self):\n\"\"\"Compute matrix determinant using Bareiss' fraction-free\nalgorithm which is an extension of the well known Gaussian\nelimination method. This approach is best suited for dense\nsymbolic matrices and will result in a determinant with\nminimal number of fractions. It means that less term\nrewriting is needed on resulting formulae.\n\nTODO: Implement algorithm for sparse matrices (SFF),\nhttp://www.eecis.udel.edu/~saunders/papers/sffge/it5.ps.\n\"\"\"\n\n# XXX included as a workaround for issue #12362.  Should use `_find_reasonable_pivot` instead\ndef _find_pivot(l):\nfor pos,val in enumerate(l):\nif val:\nreturn (pos, val, None, None)\nreturn (None, None, None, None)\n\n\n# Recursively implemented Bareiss' algorithm as per Deanna Richelle Leggett's\n# thesis http://www.math.usm.edu/perry/Research/Thesis_DRL.pdf\ndef bareiss(mat, cumm=1):\nif mat.rows == 0:\nreturn S.One\nelif mat.rows == 1:\nreturn mat[0, 0]\n\n# find a pivot and extract the remaining matrix\n# XXX should use `_find_reasonable_pivot`.  Blocked by issue #12362\npivot_pos, pivot_val, _, _ = _find_pivot(mat[:, 0])\nif pivot_pos == None:\nreturn S.Zero\n\n# if we have a valid pivot, we'll do a \"row swap\", so keep the\n# sign of the det\nsign = (-1) ** (pivot_pos % 2)\n\n# we want every row but the pivot row and every column\nrows = list(i for i in range(mat.rows) if i != pivot_pos)\ncols = list(range(mat.cols))\ntmp_mat = mat.extract(rows, cols)\n\ndef entry(i, j):\nret = (pivot_val*tmp_mat[i, j + 1] - mat[pivot_pos, j + 1]*tmp_mat[i, 0]) / cumm\nif not ret.is_Atom:\ncancel(ret)\nreturn ret\n\nreturn sign*bareiss(self._new(mat.rows - 1, mat.cols - 1, entry), pivot_val)\n\nreturn cancel(bareiss(self))\n\ndef _eval_det_berkowitz(self):\n\"\"\" Use the Berkowitz algorithm to compute the determinant.\"\"\"\nberk_vector = self._eval_berkowitz_vector()\nreturn (-1)**(len(berk_vector) - 1) * berk_vector[-1]\n\ndef _eval_det_lu(self, iszerofunc=_iszero, simpfunc=None):\n\"\"\" Computes the determinant of a matrix from its LU decomposition.\nThis function uses the LU decomposition computed by\nLUDecomposition_Simple().\n\nThe keyword arguments iszerofunc and simpfunc are passed to\nLUDecomposition_Simple().\niszerofunc is a callable that returns a boolean indicating if its\ninput is zero, or None if it cannot make the determination.\nsimpfunc is a callable that simplifies its input.\nThe default is simpfunc=None, which indicate that the pivot search\nalgorithm should not attempt to simplify any candidate pivots.\nIf simpfunc fails to simplify its input, then it must return its input\ninstead of a copy.\"\"\"\n\nif self.rows == 0:\nreturn S.One\n# sympy/matrices/tests/test_matrices.py contains a test that\n# suggests that the determinant of a 0 x 0 matrix is one, by\n# convention.\n\nlu, row_swaps = self.LUdecomposition_Simple(iszerofunc=iszerofunc, simpfunc=None)\n# P*A = L*U => det(A) = det(L)*det(U)/det(P) = det(P)*det(U).\n# Lower triangular factor L encoded in lu has unit diagonal => det(L) = 1.\n# P is a permutation matrix => det(P) in {-1, 1} => 1/det(P) = det(P).\n# LUdecomposition_Simple() returns a list of row exchange index pairs, rather\n# than a permutation matrix, but det(P) = (-1)**len(row_swaps).\n\n# Avoid forming the potentially time consuming  product of U's diagonal entries\n# if the product is zero.\n# Bottom right entry of U is 0 => det(A) = 0.\n# It may be impossible to determine if this entry of U is zero when it is symbolic.\nif iszerofunc(lu[lu.rows-1, lu.rows-1]):\nreturn S.Zero\n\n# Compute det(P)\ndet = -S.One if len(row_swaps)%2 else S.One\n\n# Compute det(U) by calculating the product of U's diagonal entries.\n# The upper triangular portion of lu is the upper triangular portion of the\n# U factor in the LU decomposition.\nfor k in range(lu.rows):\ndet *= lu[k, k]\n\n# return det(P)*det(U)\nreturn det\n\ndef _eval_determinant(self):\n\"\"\"Assumed to exist by matrix expressions; If we subclass\nMatrixDeterminant, we can fully evaluate determinants.\"\"\"\nreturn self.det()\n\ndef adjugate(self, method=\"berkowitz\"):\n\"\"\"Returns the adjugate, or classical adjoint, of\na matrix.  That is, the transpose of the matrix of cofactors.\n\n\nhttp://en.wikipedia.org/wiki/Adjugate\n\nSee Also\n========\n\ncofactor_matrix\ntranspose\n\"\"\"\nreturn self.cofactor_matrix(method).transpose()\n\ndef charpoly(self, x='lambda', simplify=_simplify):\n\"\"\"Computes characteristic polynomial det(x*I - self) where I is\nthe identity matrix.\n\nA PurePoly is returned, so using different variables for ``x`` does\nnot affect the comparison or the polynomials:\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> from sympy.abc import x, y\n>>> A = Matrix([[1, 3], [2, 0]])\n>>> A.charpoly(x) == A.charpoly(y)\nTrue\n\nSpecifying ``x`` is optional; a symbol named ``lambda`` is used by\ndefault (which looks good when pretty-printed in unicode):\n\n>>> A.charpoly().as_expr()\nlambda**2 - lambda - 6\n\nAnd if ``x`` clashes with an existing symbol, underscores will\nbe preppended to the name to make it unique:\n\n>>> A = Matrix([[1, 2], [x, 0]])\n>>> A.charpoly(x).as_expr()\n_x**2 - _x - 2*x\n\nWhether you pass a symbol or not, the generator can be obtained\nwith the gen attribute since it may not be the same as the symbol\nthat was passed:\n\n>>> A.charpoly(x).gen\n_x\n>>> A.charpoly(x).gen == x\nFalse\n\nNotes\n=====\n\nThe Samuelson-Berkowitz algorithm is used to compute\nthe characteristic polynomial efficiently and without any\ndivision operations.  Thus the characteristic polynomial over any\ncommutative ring without zero divisors can be computed.\n\nSee Also\n========\n\ndet\n\"\"\"\n\nif self.rows != self.cols:\nraise NonSquareMatrixError()\n\nberk_vector = self._eval_berkowitz_vector()\nx = _uniquely_named_symbol(x, berk_vector)\nreturn PurePoly([simplify(a) for a in berk_vector], x)\n\ndef cofactor(self, i, j, method=\"berkowitz\"):\n\"\"\"Calculate the cofactor of an element.\n\nSee Also\n========\n\ncofactor_matrix\nminor\nminor_submatrix\n\"\"\"\n\nif self.rows != self.cols or self.rows < 1:\nraise NonSquareMatrixError()\n\nreturn (-1)**((i + j) % 2) * self.minor(i, j, method)\n\ndef cofactor_matrix(self, method=\"berkowitz\"):\n\"\"\"Return a matrix containing the cofactor of each element.\n\nSee Also\n========\n\ncofactor\nminor\nminor_submatrix\nadjugate\n\"\"\"\n\nif self.rows != self.cols or self.rows < 1:\nraise NonSquareMatrixError()\n\nreturn self._new(self.rows, self.cols,\nlambda i, j: self.cofactor(i, j, method))\n\ndef det(self, method=\"bareiss\"):\n\"\"\"Computes the determinant of a matrix.  If the matrix\nis at most 3x3, a hard-coded formula is used.\nOtherwise, the determinant using the method `method`.\n\n\nPossible values for \"method\":\nbareis\nberkowitz\nlu\n\"\"\"\n\n# sanitize `method`\nmethod = method.lower()\nif method == \"bareis\":\nmethod = \"bareiss\"\nif method == \"det_lu\":\nmethod = \"lu\"\nif method not in (\"bareiss\", \"berkowitz\", \"lu\"):\nraise ValueError(\"Determinant method '%s' unrecognized\" % method)\n\n# if methods were made internal and all determinant calculations\n# passed through here, then these lines could be factored out of\n# the method routines\nif self.rows != self.cols:\nraise NonSquareMatrixError()\n\nn = self.rows\nif n == 0:\nreturn S.One\nelif n == 1:\nreturn self[0,0]\nelif n == 2:\nreturn self[0, 0] * self[1, 1] - self[0, 1] * self[1, 0]\nelif n == 3:\nreturn  (self[0, 0] * self[1, 1] * self[2, 2]\n+ self[0, 1] * self[1, 2] * self[2, 0]\n+ self[0, 2] * self[1, 0] * self[2, 1]\n- self[0, 2] * self[1, 1] * self[2, 0]\n- self[0, 0] * self[1, 2] * self[2, 1]\n- self[0, 1] * self[1, 0] * self[2, 2])\n\nif method == \"bareiss\":\nreturn self._eval_det_bareiss()\nelif method == \"berkowitz\":\nreturn self._eval_det_berkowitz()\nelif method == \"lu\":\nreturn self._eval_det_lu()\n\ndef minor(self, i, j, method=\"berkowitz\"):\n\"\"\"Return the (i,j) minor of `self`.  That is,\nreturn the determinant of the matrix obtained by deleting\nthe `i`th row and `j`th column from `self`.\n\nSee Also\n========\n\nminor_submatrix\ncofactor\ndet\n\"\"\"\n\nif self.rows != self.cols or self.rows < 1:\nraise NonSquareMatrixError()\n\nreturn self.minor_submatrix(i, j).det(method=method)\n\ndef minor_submatrix(self, i, j):\n\"\"\"Return the submatrix obtained by removing the `i`th row\nand `j`th column from `self`.\n\nSee Also\n========\n\nminor\ncofactor\n\"\"\"\n\nif i < 0:\ni += self.rows\nif j < 0:\nj += self.cols\n\nif not 0 <= i < self.rows or not 0 <= j < self.cols:\nraise ValueError(\"`i` and `j` must satisfy 0 <= i < `self.rows` \"\n\"(%d)\" % self.rows + \"and 0 <= j < `self.cols` (%d).\" % self.cols)\n\nrows = [a for a in range(self.rows) if a != i]\ncols = [a for a in range(self.cols) if a != j]\nreturn self.extract(rows, cols)\n\n\nclass MatrixReductions(MatrixDeterminant):\n\"\"\"Provides basic matrix row/column operations.\nShould not be instantiated directly.\"\"\"\n\ndef _eval_col_op_swap(self, col1, col2):\ndef entry(i, j):\nif j == col1:\nreturn self[i, col2]\nelif j == col2:\nreturn self[i, col1]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_col_op_multiply_col_by_const(self, col, k):\ndef entry(i, j):\nif j == col:\nreturn k * self[i, j]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_col_op_add_multiple_to_other_col(self, col, k, col2):\ndef entry(i, j):\nif j == col:\nreturn self[i, j] + k * self[i, col2]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_row_op_swap(self, row1, row2):\ndef entry(i, j):\nif i == row1:\nreturn self[row2, j]\nelif i == row2:\nreturn self[row1, j]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_row_op_multiply_row_by_const(self, row, k):\ndef entry(i, j):\nif i == row:\nreturn k * self[i, j]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_row_op_add_multiple_to_other_row(self, row, k, row2):\ndef entry(i, j):\nif i == row:\nreturn self[i, j] + k * self[row2, j]\nreturn self[i, j]\nreturn self._new(self.rows, self.cols, entry)\n\ndef _eval_echelon_form(self, iszerofunc, simpfunc):\n\"\"\"Returns (mat, swaps) where `mat` is a row-equivalent matrix\nin echelon form and `swaps` is a list of row-swaps performed.\"\"\"\nreduced, pivot_cols, swaps = self._row_reduce(iszerofunc, simpfunc,\nnormalize_last=True,\nnormalize=False,\nzero_above=False)\nreturn reduced, pivot_cols, swaps\n\ndef _eval_is_echelon(self, iszerofunc):\nif self.rows <= 0 or self.cols <= 0:\nreturn True\nzeros_below = all(iszerofunc(t) for t in self[1:, 0])\nif iszerofunc(self[0, 0]):\nreturn zeros_below and self[:, 1:]._eval_is_echelon(iszerofunc)\nreturn zeros_below and self[1:, 1:]._eval_is_echelon(iszerofunc)\n\ndef _eval_rref(self, iszerofunc, simpfunc, normalize_last=True):\nreduced, pivot_cols, swaps = self._row_reduce(iszerofunc, simpfunc,\nnormalize_last, normalize=True,\nzero_above=True)\nreturn reduced, pivot_cols\n\ndef _normalize_op_args(self, op, col, k, col1, col2, error_str=\"col\"):\n\"\"\"Validate the arguments for a row/column operation.  `error_str`\ncan be one of \"row\" or \"col\" depending on the arguments being parsed.\"\"\"\nif op not in [\"n->kn\", \"n<->m\", \"n->n+km\"]:\nraise ValueError(\"Unknown {} operation '{}'. Valid col operations \"\n\"are 'n->kn', 'n<->m', 'n->n+km'\".format(error_str, op))\n\n# normalize and validate the arguments\nif op == \"n->kn\":\ncol = col if col is not None else col1\nif col is None or k is None:\nraise ValueError(\"For a {0} operation 'n->kn' you must provide the \"\n\"kwargs `{0}` and `k`\".format(error_str))\nif not 0 <= col <= self.cols:\nraise ValueError(\"This matrix doesn't have a {} '{}'\".format(error_str, col))\n\nif op == \"n<->m\":\n# we need two cols to swap. It doesn't matter\n# how they were specified, so gather them together and\n# remove `None`\ncols = set((col, k, col1, col2)).difference([None])\nif len(cols) > 2:\n# maybe the user left `k` by mistake?\ncols = set((col, col1, col2)).difference([None])\nif len(cols) != 2:\nraise ValueError(\"For a {0} operation 'n<->m' you must provide the \"\n\"kwargs `{0}1` and `{0}2`\".format(error_str))\ncol1, col2 = cols\nif not 0 <= col1 <= self.cols:\nraise ValueError(\"This matrix doesn't have a {} '{}'\".format(error_str, col1))\nif not 0 <= col2 <= self.cols:\nraise ValueError(\"This matrix doesn't have a {} '{}'\".format(error_str, col2))\n\nif op == \"n->n+km\":\ncol = col1 if col is None else col\ncol2 = col1 if col2 is None else col2\nif col is None or col2 is None or k is None:\nraise ValueError(\"For a {0} operation 'n->n+km' you must provide the \"\n\"kwargs `{0}`, `k`, and `{0}2`\".format(error_str))\nif col == col2:\nraise ValueError(\"For a {0} operation 'n->n+km' `{0}` and `{0}2` must \"\n\"be different.\".format(error_str))\nif not 0 <= col <= self.cols:\nraise ValueError(\"This matrix doesn't have a {} '{}'\".format(error_str, col))\nif not 0 <= col2 <= self.cols:\nraise ValueError(\"This matrix doesn't have a {} '{}'\".format(error_str, col2))\n\nreturn op, col, k, col1, col2\n\ndef _permute_complexity_right(self, iszerofunc):\n\"\"\"Permute columns with complicated elements as\nfar right as they can go.  Since the `sympy` row reduction\nalgorithms start on the left, having complexity right-shifted\nspeeds things up.\n\nReturns a tuple (mat, perm) where perm is a permutation\nof the columns to perform to shift the complex columns right, and mat\nis the permuted matrix.\"\"\"\n\ndef complexity(i):\n# the complexity of a column will be judged by how many\n# element's zero-ness cannot be determined\nreturn sum(1 if iszerofunc(e) is None else 0 for e in self[:, i])\ncomplex = [(complexity(i), i) for i in range(self.cols)]\nperm = [j for (i, j) in sorted(complex)]\n\nreturn (self.permute(perm, orientation='cols'), perm)\n\ndef _row_reduce(self, iszerofunc, simpfunc, normalize_last=True,\nnormalize=True, zero_above=True):\n\"\"\"Row reduce `self` and return a tuple (rref_matrix,\npivot_cols, swaps) where pivot_cols are the pivot columns\nand swaps are any row swaps that were used in the process\nof row reduction.\n\nParameters\n==========\n\niszerofunc : determines if an entry can be used as a pivot\nsimpfunc : used to simplify elements and test if they are\nzero if `iszerofunc` returns `None`\nnormalize_last : indicates where all row reduction should\nhappen in a fraction-free manner and then the rows are\nnormalized (so that the pivots are 1), or whether\nrows should be normalized along the way (like the naive\nrow reduction algorithm)\nnormalize : whether pivot rows should be normalized so that\nthe pivot value is 1\nzero_above : whether entries above the pivot should be zeroed.\nIf `zero_above=False`, an echelon matrix will be returned.\n\"\"\"\nrows, cols = self.rows, self.cols\nmat = list(self)\ndef get_col(i):\nreturn mat[i::cols]\n\ndef row_swap(i, j):\nmat[i*cols:(i + 1)*cols], mat[j*cols:(j + 1)*cols] = \\\nmat[j*cols:(j + 1)*cols], mat[i*cols:(i + 1)*cols]\n\ndef cross_cancel(a, i, b, j):\n\"\"\"Does the row op row[i] = a*row[i] - b*row[j]\"\"\"\nq = (j - i)*cols\nfor p in range(i*cols, (i + 1)*cols):\nmat[p] = a*mat[p] - b*mat[p + q]\n\npiv_row, piv_col = 0, 0\npivot_cols = []\nswaps = []\n# use a fraction free method to zero above and below each pivot\nwhile piv_col < cols and piv_row < rows:\npivot_offset, pivot_val, \\\nassumed_nonzero, newly_determined = _find_reasonable_pivot(\nget_col(piv_col)[piv_row:], iszerofunc, simpfunc)\n\n# _find_reasonable_pivot may have simplified some things\n# in the process.  Let's not let them go to waste\nfor (offset, val) in newly_determined:\noffset += piv_row\nmat[offset*cols + piv_col] = val\n\nif pivot_offset is None:\npiv_col += 1\ncontinue\n\npivot_cols.append(piv_col)\nif pivot_offset != 0:\nrow_swap(piv_row, pivot_offset + piv_row)\nswaps.append((piv_row, pivot_offset + piv_row))\n\n# if we aren't normalizing last, we normalize\n# before we zero the other rows\nif normalize_last is False:\ni, j = piv_row, piv_col\nmat[i*cols + j] = S.One\nfor p in range(i*cols + j + 1, (i + 1)*cols):\nmat[p] = mat[p] / pivot_val\n# after normalizing, the pivot value is 1\npivot_val = S.One\n\n# zero above and below the pivot\nfor row in range(rows):\n# don't zero our current row\nif row == piv_row:\ncontinue\n# don't zero above the pivot unless we're told.\nif zero_above is False and row < piv_row:\ncontinue\n# if we're already a zero, don't do anything\nval = mat[row*cols + piv_col]\nif iszerofunc(val):\ncontinue\n\ncross_cancel(pivot_val, row, val, piv_row)\npiv_row += 1\n\n# normalize each row\nif normalize_last is True and normalize is True:\nfor piv_i, piv_j in enumerate(pivot_cols):\npivot_val = mat[piv_i*cols + piv_j]\nmat[piv_i*cols + piv_j] = S.One\nfor p in range(piv_i*cols + piv_j + 1, (piv_i + 1)*cols):\nmat[p] = mat[p] / pivot_val\n\nreturn self._new(self.rows, self.cols, mat), tuple(pivot_cols), tuple(swaps)\n\ndef echelon_form(self, iszerofunc=_iszero, simplify=False, with_pivots=False):\n\"\"\"Returns a matrix row-equivalent to `self` that is\nin echelon form.  Note that echelon form of a matrix\nis *not* unique, however, properties like the row\nspace and the null space are preserved.\"\"\"\nsimpfunc = simplify if isinstance(\nsimplify, FunctionType) else _simplify\n\nmat, pivots, swaps = self._eval_echelon_form(iszerofunc, simpfunc)\nif with_pivots:\nreturn mat, pivots\nreturn mat\n\ndef elementary_col_op(self, op=\"n->kn\", col=None, k=None, col1=None, col2=None):\n\"\"\"Performs the elementary column operation `op`.\n\n`op` may be one of\n\n* \"n->kn\" (column n goes to k*n)\n* \"n<->m\" (swap column n and column m)\n* \"n->n+km\" (column n goes to column n + k*column m)\n\nParameters\n=========\n\nop : string; the elementary row operation\ncol : the column to apply the column operation\nk : the multiple to apply in the column operation\ncol1 : one column of a column swap\ncol2 : second column of a column swap or column \"m\" in the column operation\n\"n->n+km\"\n\"\"\"\n\nop, col, k, col1, col2 = self._normalize_op_args(op, col, k, col1, col2, \"col\")\n\n# now that we've validated, we're all good to dispatch\nif op == \"n->kn\":\nreturn self._eval_col_op_multiply_col_by_const(col, k)\nif op == \"n<->m\":\nreturn self._eval_col_op_swap(col1, col2)\nif op == \"n->n+km\":\nreturn self._eval_col_op_add_multiple_to_other_col(col, k, col2)\n\ndef elementary_row_op(self, op=\"n->kn\", row=None, k=None, row1=None, row2=None):\n\"\"\"Performs the elementary row operation `op`.\n\n`op` may be one of\n\n* \"n->kn\" (row n goes to k*n)\n* \"n<->m\" (swap row n and row m)\n* \"n->n+km\" (row n goes to row n + k*row m)\n\nParameters\n==========\n\nop : string; the elementary row operation\nrow : the row to apply the row operation\nk : the multiple to apply in the row operation\nrow1 : one row of a row swap\nrow2 : second row of a row swap or row \"m\" in the row operation\n\"n->n+km\"\n\"\"\"\n\nop, row, k, row1, row2 = self._normalize_op_args(op, row, k, row1, row2, \"row\")\n\n# now that we've validated, we're all good to dispatch\nif op == \"n->kn\":\nreturn self._eval_row_op_multiply_row_by_const(row, k)\nif op == \"n<->m\":\nreturn self._eval_row_op_swap(row1, row2)\nif op == \"n->n+km\":\nreturn self._eval_row_op_add_multiple_to_other_row(row, k, row2)\n\n@property\ndef is_echelon(self, iszerofunc=_iszero):\n\"\"\"Returns `True` if he matrix is in echelon form.\nThat is, all rows of zeros are at the bottom, and below\neach leading non-zero in a row are exclusively zeros.\"\"\"\n\nreturn self._eval_is_echelon(iszerofunc)\n\ndef rank(self, iszerofunc=_iszero, simplify=False):\n\"\"\"\nReturns the rank of a matrix\n\n>>> from sympy import Matrix\n>>> from sympy.abc import x\n>>> m = Matrix([[1, 2], [x, 1 - 1/x]])\n>>> m.rank()\n2\n>>> n = Matrix(3, 3, range(1, 10))\n>>> n.rank()\n2\n\"\"\"\nsimpfunc = simplify if isinstance(\nsimplify, FunctionType) else _simplify\n\n# for small matrices, we compute the rank explicitly\n# if is_zero on elements doesn't answer the question\n# for small matrices, we fall back to the full routine.\nif self.rows <= 0 or self.cols <= 0:\nreturn 0\nif self.rows <= 1 or self.cols <= 1:\nzeros = [iszerofunc(x) for x in self]\nif False in zeros:\nreturn 1\nif self.rows == 2 and self.cols == 2:\nzeros = [iszerofunc(x) for x in self]\nif not False in zeros and not None in zeros:\nreturn 0\ndet = self.det()\nif iszerofunc(det) and False in zeros:\nreturn 1\nif iszerofunc(det) is False:\nreturn 2\n\nmat, _ = self._permute_complexity_right(iszerofunc=iszerofunc)\nechelon_form, pivots, swaps = mat._eval_echelon_form(iszerofunc=iszerofunc, simpfunc=simpfunc)\nreturn len(pivots)\n\ndef rref(self, iszerofunc=_iszero, simplify=False, pivots=True, normalize_last=True):\n\"\"\"Return reduced row-echelon form of matrix and indices of pivot vars.\n\nParameters\n==========\n\niszerofunc : Function\nA function used for detecting whether an element can\nact as a pivot.  `lambda x: x.is_zero` is used by default.\nsimplify : Function\nA function used to simplify elements when looking for a pivot.\nBy default SymPy's `simplify`is used.\npivots : True or False\nIf `True`, a tuple containing the row-reduced matrix and a tuple\nof pivot columns is returned.  If `False` just the row-reduced\nmatrix is returned.\nnormalize_last : True or False\nIf `True`, no pivots are normalized to `1` until after all entries\nabove and below each pivot are zeroed.  This means the row\nreduction algorithm is fraction free until the very last step.\nIf `False`, the naive row reduction procedure is used where\neach pivot is normalized to be `1` before row operations are\nused to zero above and below the pivot.\n\nNotes\n=====\n\nThe default value of `normalize_last=True` can provide significant\nspeedup to row reduction, especially on matrices with symbols.  However,\nif you depend on the form row reduction algorithm leaves entries\nof the matrix, set `noramlize_last=False`\n\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> from sympy.abc import x\n>>> m = Matrix([[1, 2], [x, 1 - 1/x]])\n>>> m.rref()\n(Matrix([\n[1, 0],\n[0, 1]]), (0, 1))\n>>> rref_matrix, rref_pivots = m.rref()\n>>> rref_matrix\nMatrix([\n[1, 0],\n[0, 1]])\n>>> rref_pivots\n(0, 1)\n\"\"\"\nsimpfunc = simplify if isinstance(\nsimplify, FunctionType) else _simplify\n\nret, pivot_cols = self._eval_rref(iszerofunc=iszerofunc,\nsimpfunc=simpfunc,\nnormalize_last=normalize_last)\nif pivots:\nret = (ret, pivot_cols)\nreturn ret\n\n\nclass MatrixSubspaces(MatrixReductions):\n\"\"\"Provides methods relating to the fundamental subspaces\nof a matrix.  Should not be instantiated directly.\"\"\"\n\ndef columnspace(self, simplify=False):\n\"\"\"Returns a list of vectors (Matrix objects) that span columnspace of self\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> m = Matrix(3, 3, [1, 3, 0, -2, -6, 0, 3, 9, 6])\n>>> m\nMatrix([\n[ 1,  3, 0],\n[-2, -6, 0],\n[ 3,  9, 6]])\n>>> m.columnspace()\n[Matrix([\n[ 1],\n[-2],\n[ 3]]), Matrix([\n[0],\n[0],\n[6]])]\n\nSee Also\n========\n\nnullspace\nrowspace\n\"\"\"\nreduced, pivots = self.echelon_form(simplify=simplify, with_pivots=True)\n\nreturn [self.col(i) for i in pivots]\n\ndef nullspace(self, simplify=False):\n\"\"\"Returns list of vectors (Matrix objects) that span nullspace of self\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> m = Matrix(3, 3, [1, 3, 0, -2, -6, 0, 3, 9, 6])\n>>> m\nMatrix([\n[ 1,  3, 0],\n[-2, -6, 0],\n[ 3,  9, 6]])\n>>> m.nullspace()\n[Matrix([\n[-3],\n[ 1],\n[ 0]])]\n\nSee Also\n========\n\ncolumnspace\nrowspace\n\"\"\"\n\nreduced, pivots = self.rref(simplify=simplify)\n\nfree_vars = [i for i in range(self.cols) if i not in pivots]\n\nbasis = []\nfor free_var in free_vars:\n# for each free variable, we will set it to 1 and all others\n# to 0.  Then, we will use back substitution to solve the system\nvec = [S.Zero]*self.cols\nvec[free_var] = S.One\nfor piv_row, piv_col in enumerate(pivots):\nfor pos in pivots[piv_row+1:] + (free_var,):\nvec[piv_col] -= reduced[piv_row, pos]\nbasis.append(vec)\n\nreturn [self._new(self.cols, 1, b) for b in basis]\n\ndef rowspace(self, simplify=False):\n\"\"\"Returns a list of vectors that span the row space of self.\"\"\"\n\nreduced, pivots = self.echelon_form(simplify=simplify, with_pivots=True)\n\nreturn [reduced.row(i) for i in range(len(pivots))]\n\n@classmethod\ndef orthogonalize(cls, *vecs, **kwargs):\n\"\"\"Apply the Gram-Schmidt orthogonalization procedure\nto vectors supplied in `vecs`.\n\nArguments\n=========\n\nvecs : vectors to be made orthogonal\nnormalize : bool. Whether the returned vectors\nshould be renormalized to be unit vectors.\n\"\"\"\n\nnormalize = kwargs.get('normalize', False)\n\ndef project(a, b):\nreturn b * (a.dot(b) / b.dot(b))\n\ndef perp_to_subspace(vec, basis):\n\"\"\"projects vec onto the subspace given\nby the orthogonal basis `basis`\"\"\"\ncomponents = [project(vec, b) for b in basis]\nif len(basis) == 0:\nreturn vec\nreturn vec - reduce(lambda a, b: a + b, components)\n\nret = []\n# make sure we start with a non-zero vector\nwhile len(vecs) > 0 and vecs[0].is_zero:\ndel vecs[0]\n\nfor vec in vecs:\nperp = perp_to_subspace(vec, ret)\nif not perp.is_zero:\nret.append(perp)\n\nif normalize:\nret = [vec / vec.norm() for vec in ret]\n\nreturn ret\n\n\nclass MatrixEigen(MatrixSubspaces):\n\"\"\"Provides basic matrix eigenvalue/vector operations.\nShould not be instantiated directly.\"\"\"\n\n_cache_is_diagonalizable = None\n_cache_eigenvects = None\n\ndef diagonalize(self, reals_only=False, sort=False, normalize=False):\n\"\"\"\nReturn (P, D), where D is diagonal and\n\nD = P^-1 * M * P\n\nwhere M is current matrix.\n\nParameters\n==========\n\nreals_only : bool. Whether to throw an error if complex numbers are need\nto diagonalize. (Default: False)\nsort : bool. Sort the eigenvalues along the diagonal. (Default: False)\nnormalize : bool. If True, normalize the columns of P. (Default: False)\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> m = Matrix(3, 3, [1, 2, 0, 0, 3, 0, 2, -4, 2])\n>>> m\nMatrix([\n[1,  2, 0],\n[0,  3, 0],\n[2, -4, 2]])\n>>> (P, D) = m.diagonalize()\n>>> D\nMatrix([\n[1, 0, 0],\n[0, 2, 0],\n[0, 0, 3]])\n>>> P\nMatrix([\n[-1, 0, -1],\n[ 0, 0, -1],\n[ 2, 1,  2]])\n>>> P.inv() * m * P\nMatrix([\n[1, 0, 0],\n[0, 2, 0],\n[0, 0, 3]])\n\nSee Also\n========\n\nis_diagonal\nis_diagonalizable\n\"\"\"\n\nif not self.is_square:\nraise NonSquareMatrixError()\n\nif not self.is_diagonalizable(reals_only=reals_only, clear_cache=False):\nraise MatrixError(\"Matrix is not diagonalizable\")\n\neigenvecs = self._cache_eigenvects\nif eigenvecs is None:\neigenvecs = self.eigenvects(simplify=True)\n\nif sort:\neigenvecs = sorted(eigenvecs, key=default_sort_key)\n\np_cols, diag = [], []\nfor val, mult, basis in eigenvecs:\ndiag += [val] * mult\np_cols += basis\n\nif normalize:\np_cols = [v / v.norm() for v in p_cols]\n\nreturn self.hstack(*p_cols), self.diag(*diag)\n\ndef eigenvals(self, error_when_incomplete=True, **flags):\n\"\"\"Return eigenvalues using the Berkowitz agorithm to compute\nthe characteristic polynomial.\n\nParameters\n==========\n\nerror_when_incomplete : bool\nRaise an error when not all eigenvalues are computed. This is\ncaused by ``roots`` not returning a full list of eigenvalues.\n\nSince the roots routine doesn't always work well with Floats,\nthey will be replaced with Rationals before calling that\nroutine. If this is not desired, set flag ``rational`` to False.\n\"\"\"\nmat = self\nif not mat:\nreturn {}\nif flags.pop('rational', True):\nif any(v.has(Float) for v in mat):\nmat = mat.applyfunc(lambda x: nsimplify(x, rational=True))\n\nflags.pop('simplify', None)  # pop unsupported flag\neigs = roots(mat.charpoly(x=Dummy('x')), **flags)\n\n# make sure the algebraic multiplicty sums to the\n# size of the matrix\nif error_when_incomplete and sum(m for m in eigs.values()) != self.cols:\nraise MatrixError(\"Could not compute eigenvalues for {}\".format(self))\n\nreturn eigs\n\ndef eigenvects(self, error_when_incomplete=True, **flags):\n\"\"\"Return list of triples (eigenval, multiplicity, basis).\n\nThe flag ``simplify`` has two effects:\n1) if bool(simplify) is True, as_content_primitive()\nwill be used to tidy up normalization artifacts;\n2) if nullspace needs simplification to compute the\nbasis, the simplify flag will be passed on to the\nnullspace routine which will interpret it there.\n\nParameters\n==========\n\nerror_when_incomplete : bool\nRaise an error when not all eigenvalues are computed. This is\ncaused by ``roots`` not returning a full list of eigenvalues.\n\nIf the matrix contains any Floats, they will be changed to Rationals\nfor computation purposes, but the answers will be returned after being\nevaluated with evalf. If it is desired to removed small imaginary\nportions during the evalf step, pass a value for the ``chop`` flag.\n\"\"\"\nfrom sympy.matrices import eye\n\nsimplify = flags.get('simplify', True)\nif not isinstance(simplify, FunctionType):\nsimpfunc = _simplify if simplify else lambda x: x\nprimitive = flags.get('simplify', False)\nchop = flags.pop('chop', False)\n\nflags.pop('multiple', None)  # remove this if it's there\n\nmat = self\n# roots doesn't like Floats, so replace them with Rationals\nhas_floats = any(v.has(Float) for v in self)\nif has_floats:\nmat = mat.applyfunc(lambda x: nsimplify(x, rational=True))\n\ndef eigenspace(eigenval):\n\"\"\"Get a basis for the eigenspace for a particular eigenvalue\"\"\"\nm = mat - self.eye(mat.rows) * eigenval\nret = m.nullspace()\n# the nullspace for a real eigenvalue should be\n# non-trivial.  If we didn't find an eigenvector, try once\n# more a little harder\nif len(ret) == 0 and simplify:\nret = m.nullspace(simplify=True)\nif len(ret) == 0:\nraise NotImplementedError(\n\"Can't evaluate eigenvector for eigenvalue %s\" % eigenval)\nreturn ret\n\neigenvals = mat.eigenvals(rational=False,\nerror_when_incomplete=error_when_incomplete,\n**flags)\nret = [(val, mult, eigenspace(val)) for val, mult in\nsorted(eigenvals.items(), key=default_sort_key)]\nif primitive:\n# if the primitive flag is set, get rid of any common\n# integer denominators\ndef denom_clean(l):\nfrom sympy import gcd\nreturn [(v / gcd(list(v))).applyfunc(simpfunc) for v in l]\nret = [(val, mult, denom_clean(es)) for val, mult, es in ret]\nif has_floats:\n# if we had floats to start with, turn the eigenvectors to floats\nret = [(val.evalf(chop=chop), mult, [v.evalf(chop=chop) for v in es]) for val, mult, es in ret]\nreturn ret\n\ndef is_diagonalizable(self, reals_only=False, **kwargs):\n\"\"\"Returns true if a matrix is diagonalizable.\n\nParameters\n==========\n\nreals_only : bool. If reals_only=True, determine whether the matrix can be\ndiagonalized without complex numbers. (Default: False)\n\nkwargs\n======\n\nclear_cache : bool. If True, clear the result of any computations when finished.\n(Default: True)\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> m = Matrix(3, 3, [1, 2, 0, 0, 3, 0, 2, -4, 2])\n>>> m\nMatrix([\n[1,  2, 0],\n[0,  3, 0],\n[2, -4, 2]])\n>>> m.is_diagonalizable()\nTrue\n>>> m = Matrix(2, 2, [0, 1, 0, 0])\n>>> m\nMatrix([\n[0, 1],\n[0, 0]])\n>>> m.is_diagonalizable()\nFalse\n>>> m = Matrix(2, 2, [0, 1, -1, 0])\n>>> m\nMatrix([\n[ 0, 1],\n[-1, 0]])\n>>> m.is_diagonalizable()\nTrue\n>>> m.is_diagonalizable(reals_only=True)\nFalse\n\nSee Also\n========\n\nis_diagonal\ndiagonalize\n\"\"\"\n\nclear_cache = kwargs.get('clear_cache', True)\nif 'clear_subproducts' in kwargs:\nclear_cache = kwargs.get('clear_subproducts')\n\ndef cleanup():\n\"\"\"Clears any cached values if requested\"\"\"\nif clear_cache:\nself._cache_eigenvects = None\nself._cache_is_diagonalizable = None\n\nif not self.is_square:\ncleanup()\nreturn False\n\n# use the cached value if we have it\nif self._cache_is_diagonalizable is not None:\nret = self._cache_is_diagonalizable\ncleanup()\nreturn ret\n\nif all(e.is_real for e in self) and self.is_symmetric():\n# every real symmetric matrix is real diagonalizable\nself._cache_is_diagonalizable = True\ncleanup()\nreturn True\n\nself._cache_eigenvects = self.eigenvects(simplify=True)\nret = True\nfor val, mult, basis in self._cache_eigenvects:\n# if we have a complex eigenvalue\nif reals_only and not val.is_real:\nret = False\n# if the geometric multiplicity doesn't equal the algebraic\nif mult != len(basis):\nret = False\ncleanup()\nreturn ret\n\ndef jordan_form(self, calc_transform=True, **kwargs):\n\"\"\"Return `(P, J)` where `J` is a Jordan block\nmatrix and `P` is a matrix such that\n\n`self == P*J*P**-1`\n\n\nParameters\n==========\n\ncalc_transform : bool\nIf ``False``, then only `J` is returned.\nchop : bool\nAll matrices are convered to exact types when computing\neigenvalues and eigenvectors.  As a result, there may be\napproximation errors.  If ``chop==True``, these errors\nwill be truncated.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> m = Matrix([[ 6,  5, -2, -3], [-3, -1,  3,  3], [ 2,  1, -2, -3], [-1,  1,  5,  5]])\n>>> P, J = m.jordan_form()\n>>> J\nMatrix([\n[2, 1, 0, 0],\n[0, 2, 0, 0],\n[0, 0, 2, 1],\n[0, 0, 0, 2]])\n\nSee Also\n========\n\njordan_block\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError(\"Only square matrices have Jordan forms\")\n\nchop = kwargs.pop('chop', False)\nmat = self\nhas_floats = any(v.has(Float) for v in self)\n\nif has_floats:\ntry:\nmax_prec = max(term._prec for term in self._mat if isinstance(term, Float))\nexcept ValueError:\n# if no term in the matrix is explicitly a Float calling max()\n# will throw a error so setting max_prec to default value of 53\nmax_prec = 53\n# setting minimum max_dps to 15 to prevent loss of precision in\n# matrix containing non evaluated expressions\nmax_dps = max(prec_to_dps(max_prec), 15)\n\ndef restore_floats(*args):\n\"\"\"If `has_floats` is `True`, cast all `args` as\nmatrices of floats.\"\"\"\nif has_floats:\nargs = [m.evalf(prec=max_dps, chop=chop) for m in args]\nif len(args) == 1:\nreturn args[0]\nreturn args\n\n# cache calculations for some speedup\nmat_cache = {}\ndef eig_mat(val, pow):\n\"\"\"Cache computations of (self - val*I)**pow for quick\nretrieval\"\"\"\nif (val, pow) in mat_cache:\nreturn mat_cache[(val, pow)]\nif (val, pow - 1) in mat_cache:\nmat_cache[(val, pow)] = mat_cache[(val, pow - 1)] * mat_cache[(val, 1)]\nelse:\nmat_cache[(val, pow)] = (mat - val*self.eye(self.rows))**pow\nreturn mat_cache[(val, pow)]\n\n# helper functions\ndef nullity_chain(val):\n\"\"\"Calculate the sequence  [0, nullity(E), nullity(E**2), ...]\nuntil it is constant where `E = self - val*I`\"\"\"\n# mat.rank() is faster than computing the null space,\n# so use the rank-nullity theorem\ncols = self.cols\nret = [0]\nnullity = cols - eig_mat(val, 1).rank()\ni = 2\nwhile nullity != ret[-1]:\nret.append(nullity)\nnullity = cols - eig_mat(val, i).rank()\ni += 1\nreturn ret\n\ndef blocks_from_nullity_chain(d):\n\"\"\"Return a list of the size of each Jordan block.\nIf d_n is the nullity of E**n, then the number\nof Jordan blocks of size n is\n\n2*d_n - d_(n-1) - d_(n+1)\"\"\"\n# d[0] is always the number of columns, so skip past it\nmid = [2*d[n] - d[n - 1] - d[n + 1] for n in range(1, len(d) - 1)]\n# d is assumed to plateau with \"d[ len(d) ] == d[-1]\", so\n# 2*d_n - d_(n-1) - d_(n+1) == d_n - d_(n-1)\nend = [d[-1] - d[-2]] if len(d) > 1 else [d[0]]\nreturn mid + end\n\ndef pick_vec(small_basis, big_basis):\n\"\"\"Picks a vector from big_basis that isn't in\nthe subspace spanned by small_basis\"\"\"\nif len(small_basis) == 0:\nreturn big_basis[0]\nfor v in big_basis:\n_, pivots = self.hstack(*(small_basis + [v])).echelon_form(with_pivots=True)\nif pivots[-1] == len(small_basis):\nreturn v\n\n# roots doesn't like Floats, so replace them with Rationals\nif has_floats:\nmat = mat.applyfunc(lambda x: nsimplify(x, rational=True))\n\n# first calculate the jordan block structure\neigs = mat.eigenvals()\n\n# make sure that we found all the roots by counting\n# the algebraic multiplicity\nif sum(m for m in eigs.values()) != mat.cols:\nraise MatrixError(\"Could not compute eigenvalues for {}\".format(mat))\n\n# most matrices have distinct eigenvalues\n# and so are diagonalizable.  In this case, don't\n# do extra work!\nif len(eigs.keys()) == mat.cols:\nblocks = list(sorted(eigs.keys(), key=default_sort_key))\njordan_mat = mat.diag(*blocks)\nif not calc_transform:\nreturn restore_floats(jordan_mat)\njordan_basis = [eig_mat(eig, 1).nullspace()[0] for eig in blocks]\nbasis_mat = mat.hstack(*jordan_basis)\nreturn restore_floats(basis_mat, jordan_mat)\n\nblock_structure = []\nfor eig in sorted(eigs.keys(), key=default_sort_key):\nchain = nullity_chain(eig)\nblock_sizes = blocks_from_nullity_chain(chain)\n# if block_sizes == [a, b, c, ...], then the number of\n# Jordan blocks of size 1 is a, of size 2 is b, etc.\n# create an array that has (eig, block_size) with one\n# entry for each block\nsize_nums = [(i+1, num) for i, num in enumerate(block_sizes)]\n# we expect larger Jordan blocks to come earlier\nsize_nums.reverse()\n\nblock_structure.extend(\n(eig, size) for size, num in size_nums for _ in range(num))\nblocks = (mat.jordan_block(size=size, eigenvalue=eig) for eig, size in block_structure)\njordan_mat = mat.diag(*blocks)\n\nif not calc_transform:\nreturn restore_floats(jordan_mat)\n\n# For each generalized eigenspace, calculate a basis.\n# We start by looking for a vector in null( (A - eig*I)**n )\n# which isn't in null( (A - eig*I)**(n-1) ) where n is\n# the size of the Jordan block\n#\n# Ideally we'd just loop through block_structure and\n# compute each generalized eigenspace.  However, this\n# causes a lot of unneeded computation.  Instead, we\n# go through the eigenvalues separately, since we know\n# their generalized eigenspaces must have bases that\n# are linearly independent.\njordan_basis = []\n\nfor eig in sorted(eigs.keys(), key=default_sort_key):\neig_basis = []\nfor block_eig, size in block_structure:\nif block_eig != eig:\ncontinue\nnull_big = (eig_mat(eig, size)).nullspace()\nnull_small = (eig_mat(eig, size - 1)).nullspace()\n# we want to pick something that is in the big basis\n# and not the small, but also something that is independent\n# of any other generalized eigenvectors from a different\n# generalized eigenspace sharing the same eigenvalue.\nvec = pick_vec(null_small + eig_basis, null_big)\nnew_vecs = [(eig_mat(eig, i))*vec for i in range(size)]\neig_basis.extend(new_vecs)\njordan_basis.extend(reversed(new_vecs))\n\nbasis_mat = mat.hstack(*jordan_basis)\n\nreturn restore_floats(basis_mat, jordan_mat)\n\ndef left_eigenvects(self, **flags):\n\"\"\"Returns left eigenvectors and eigenvalues.\n\nThis function returns the list of triples (eigenval, multiplicity,\nbasis) for the left eigenvectors. Options are the same as for\neigenvects(), i.e. the ``**flags`` arguments gets passed directly to\neigenvects().\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> M = Matrix([[0, 1, 1], [1, 0, 0], [1, 1, 1]])\n>>> M.eigenvects()\n[(-1, 1, [Matrix([\n[-1],\n[ 1],\n[ 0]])]), (0, 1, [Matrix([\n[ 0],\n[-1],\n[ 1]])]), (2, 1, [Matrix([\n[2/3],\n[1/3],\n[  1]])])]\n>>> M.left_eigenvects()\n[(-1, 1, [Matrix([[-2, 1, 1]])]), (0, 1, [Matrix([[-1, -1, 1]])]), (2,\n1, [Matrix([[1, 1, 1]])])]\n\n\"\"\"\neigs = self.transpose().eigenvects(**flags)\n\nreturn [(val, mult, [l.transpose() for l in basis]) for val, mult, basis in eigs]\n\ndef singular_values(self):\n\"\"\"Compute the singular values of a Matrix\n\nExamples\n========\n\n>>> from sympy import Matrix, Symbol\n>>> x = Symbol('x', real=True)\n>>> A = Matrix([[0, 1, 0], [0, x, 0], [-1, 0, 0]])\n>>> A.singular_values()\n[sqrt(x**2 + 1), 1, 0]\n\nSee Also\n========\n\ncondition_number\n\"\"\"\nmat = self\n# Compute eigenvalues of A.H A\nvalmultpairs = (mat.H * mat).eigenvals()\n\n# Expands result from eigenvals into a simple list\nvals = []\nfor k, v in valmultpairs.items():\nvals += [sqrt(k)] * v  # dangerous! same k in several spots!\n# sort them in descending order\nvals.sort(reverse=True, key=default_sort_key)\n\nreturn vals\n\n\n\nclass MatrixCalculus(MatrixCommon):\n\"\"\"Provides calculus-related matrix operations.\"\"\"\n\ndef diff(self, *args):\n\"\"\"Calculate the derivative of each element in the matrix.\n``args`` will be passed to the ``integrate`` function.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> from sympy.abc import x, y\n>>> M = Matrix([[x, y], [1, 0]])\n>>> M.diff(x)\nMatrix([\n[1, 0],\n[0, 0]])\n\nSee Also\n========\n\nintegrate\nlimit\n\"\"\"\nfrom sympy import Derivative\nreturn Derivative(self, *args, evaluate=True)\n\ndef _eval_derivative(self, arg):\nreturn self.applyfunc(lambda x: x.diff(arg))\n\ndef integrate(self, *args):\n\"\"\"Integrate each element of the matrix.  ``args`` will\nbe passed to the ``integrate`` function.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> from sympy.abc import x, y\n>>> M = Matrix([[x, y], [1, 0]])\n>>> M.integrate((x, ))\nMatrix([\n[x**2/2, x*y],\n[     x,   0]])\n>>> M.integrate((x, 0, 2))\nMatrix([\n[2, 2*y],\n[2,   0]])\n\nSee Also\n========\n\nlimit\ndiff\n\"\"\"\nreturn self.applyfunc(lambda x: x.integrate(*args))\n\ndef jacobian(self, X):\n\"\"\"Calculates the Jacobian matrix (derivative of a vector-valued function).\n\nParameters\n==========\n\nself : vector of expressions representing functions f_i(x_1, ..., x_n).\nX : set of x_i's in order, it can be a list or a Matrix\n\nBoth self and X can be a row or a column matrix in any order\n(i.e., jacobian() should always work).\n\nExamples\n========\n\n>>> from sympy import sin, cos, Matrix\n>>> from sympy.abc import rho, phi\n>>> X = Matrix([rho*cos(phi), rho*sin(phi), rho**2])\n>>> Y = Matrix([rho, phi])\n>>> X.jacobian(Y)\nMatrix([\n[cos(phi), -rho*sin(phi)],\n[sin(phi),  rho*cos(phi)],\n[   2*rho,             0]])\n>>> X = Matrix([rho*cos(phi), rho*sin(phi)])\n>>> X.jacobian(Y)\nMatrix([\n[cos(phi), -rho*sin(phi)],\n[sin(phi),  rho*cos(phi)]])\n\nSee Also\n========\n\nhessian\nwronskian\n\"\"\"\nif not isinstance(X, MatrixBase):\nX = self._new(X)\n# Both X and self can be a row or a column matrix, so we need to make\n# sure all valid combinations work, but everything else fails:\nif self.shape[0] == 1:\nm = self.shape[1]\nelif self.shape[1] == 1:\nm = self.shape[0]\nelse:\nraise TypeError(\"self must be a row or a column matrix\")\nif X.shape[0] == 1:\nn = X.shape[1]\nelif X.shape[1] == 1:\nn = X.shape[0]\nelse:\nraise TypeError(\"X must be a row or a column matrix\")\n\n# m is the number of functions and n is the number of variables\n# computing the Jacobian is now easy:\nreturn self._new(m, n, lambda j, i: self[j].diff(X[i]))\n\ndef limit(self, *args):\n\"\"\"Calculate the limit of each element in the matrix.\n``args`` will be passed to the ``limit`` function.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> from sympy.abc import x, y\n>>> M = Matrix([[x, y], [1, 0]])\n>>> M.limit(x, 2)\nMatrix([\n[2, y],\n[1, 0]])\n\nSee Also\n========\n\nintegrate\ndiff\n\"\"\"\nreturn self.applyfunc(lambda x: x.limit(*args))\n\n\n# https://github.com/sympy/sympy/pull/12854\nclass MatrixDeprecated(MatrixCommon):\n\"\"\"A class to house deprecated matrix methods.\"\"\"\n\ndef berkowitz_charpoly(self, x=Dummy('lambda'), simplify=_simplify):\nreturn self.charpoly(x=x)\n\ndef berkowitz_det(self):\n\"\"\"Computes determinant using Berkowitz method.\n\nSee Also\n========\n\ndet\nberkowitz\n\"\"\"\nreturn self.det(method='berkowitz')\n\ndef berkowitz_eigenvals(self, **flags):\n\"\"\"Computes eigenvalues of a Matrix using Berkowitz method.\n\nSee Also\n========\n\nberkowitz\n\"\"\"\nreturn self.eigenvals(**flags)\n\ndef berkowitz_minors(self):\n\"\"\"Computes principal minors using Berkowitz method.\n\nSee Also\n========\n\nberkowitz\n\"\"\"\nsign, minors = S.One, []\n\nfor poly in self.berkowitz():\nminors.append(sign * poly[-1])\nsign = -sign\n\nreturn tuple(minors)\n\ndef berkowitz(self):\nfrom sympy.matrices import zeros\nberk = ((1,),)\nif not self:\nreturn berk\n\nif not self.is_square:\nraise NonSquareMatrixError()\n\nA, N = self, self.rows\ntransforms = [0] * (N - 1)\n\nfor n in range(N, 1, -1):\nT, k = zeros(n + 1, n), n - 1\n\nR, C = -A[k, :k], A[:k, k]\nA, a = A[:k, :k], -A[k, k]\n\nitems = [C]\n\nfor i in range(0, n - 2):\nitems.append(A * items[i])\n\nfor i, B in enumerate(items):\nitems[i] = (R * B)[0, 0]\n\nitems = [S.One, a] + items\n\nfor i in range(n):\nT[i:, i] = items[:n - i + 1]\n\ntransforms[k - 1] = T\n\npolys = [self._new([S.One, -A[0, 0]])]\n\nfor i, T in enumerate(transforms):\npolys.append(T * polys[i])\n\nreturn berk + tuple(map(tuple, polys))\n\ndef cofactorMatrix(self, method=\"berkowitz\"):\nreturn self.cofactor_matrix(method=method)\n\ndef det_bareis(self):\nreturn self.det(method='bareiss')\n\ndef det_bareiss(self):\n\"\"\"Compute matrix determinant using Bareiss' fraction-free\nalgorithm which is an extension of the well known Gaussian\nelimination method. This approach is best suited for dense\nsymbolic matrices and will result in a determinant with\nminimal number of fractions. It means that less term\nrewriting is needed on resulting formulae.\n\nTODO: Implement algorithm for sparse matrices (SFF),\nhttp://www.eecis.udel.edu/~saunders/papers/sffge/it5.ps.\n\nSee Also\n========\n\ndet\nberkowitz_det\n\"\"\"\nreturn self.det(method='bareiss')\n\ndef det_LU_decomposition(self):\n\"\"\"Compute matrix determinant using LU decomposition\n\n\nNote that this method fails if the LU decomposition itself\nfails. In particular, if the matrix has no inverse this method\nwill fail.\n\nTODO: Implement algorithm for sparse matrices (SFF),\nhttp://www.eecis.udel.edu/~saunders/papers/sffge/it5.ps.\n\nSee Also\n========\n\n\ndet\ndet_bareiss\nberkowitz_det\n\"\"\"\nreturn self.det(method='lu')\n\ndef jordan_cell(self, eigenval, n):\nreturn self.jordan_block(size=n, eigenvalue=eigenval)\n\ndef jordan_cells(self, calc_transformation=True):\nP, J = self.jordan_form()\nreturn P, J.get_diag_blocks()\n\ndef minorEntry(self, i, j, method=\"berkowitz\"):\nreturn self.minor(i, j, method=method)\n\ndef minorMatrix(self, i, j):\nreturn self.minor_submatrix(i, j)\n\ndef permuteBkwd(self, perm):\n\"\"\"Permute the rows of the matrix with the given permutation in reverse.\"\"\"\nreturn self.permute_rows(perm, direction='backward')\n\ndef permuteFwd(self, perm):\n\"\"\"Permute the rows of the matrix with the given permutation.\"\"\"\nreturn self.permute_rows(perm, direction='forward')\n\n\nclass MatrixBase(MatrixDeprecated,\nMatrixCalculus,\nMatrixEigen,\nMatrixCommon):\n\"\"\"Base class for matrix objects.\"\"\"\n# Added just for numpy compatibility\n__array_priority__ = 11\n\nis_Matrix = True\n_class_priority = 3\n_sympify = staticmethod(sympify)\n\n__hash__ = None  # Mutable\n\ndef __array__(self):\nfrom .dense import matrix2numpy\nreturn matrix2numpy(self)\n\ndef __getattr__(self, attr):\nif attr in ('diff', 'integrate', 'limit'):\ndef doit(*args):\nitem_doit = lambda item: getattr(item, attr)(*args)\nreturn self.applyfunc(item_doit)\n\nreturn doit\nelse:\nraise AttributeError(\n\"%s has no attribute %s.\" % (self.__class__.__name__, attr))\n\ndef __len__(self):\n\"\"\"Return the number of elements of self.\n\nImplemented mainly so bool(Matrix()) == False.\n\"\"\"\nreturn self.rows * self.cols\n\ndef __mathml__(self):\nmml = \"\"\nfor i in range(self.rows):\nmml += \"<matrixrow>\"\nfor j in range(self.cols):\nmml += self[i, j].__mathml__()\nmml += \"</matrixrow>\"\nreturn \"<matrix>\" + mml + \"</matrix>\"\n\n# needed for python 2 compatibility\ndef __ne__(self, other):\nreturn not self == other\n\ndef _matrix_pow_by_jordan_blocks(self, num):\nfrom sympy.matrices import diag, MutableMatrix\nfrom sympy import binomial\n\ndef jordan_cell_power(jc, n):\nN = jc.shape[0]\nl = jc[0, 0]\nif l == 0 and (n < N - 1) != False:\nraise ValueError(\"Matrix det == 0; not invertible\")\nelif l == 0 and N > 1 and n % 1 != 0:\nraise ValueError(\"Non-integer power cannot be evaluated\")\nfor i in range(N):\nfor j in range(N-i):\nbn = binomial(n, i)\nif isinstance(bn, binomial):\nbn = bn._eval_expand_func()\njc[j, i+j] = l**(n-i)*bn\n\nP, J = self.jordan_form()\njordan_cells = J.get_diag_blocks()\n# Make sure jordan_cells matrices are mutable:\njordan_cells = [MutableMatrix(j) for j in jordan_cells]\nfor j in jordan_cells:\njordan_cell_power(j, num)\nreturn self._new(P*diag(*jordan_cells)*P.inv())\n\ndef __repr__(self):\nreturn sstr(self)\n\ndef __str__(self):\nif self.rows == 0 or self.cols == 0:\nreturn 'Matrix(%s, %s, [])' % (self.rows, self.cols)\nreturn \"Matrix(%s)\" % str(self.tolist())\n\ndef _diagonalize_clear_subproducts(self):\ndel self._is_symbolic\ndel self._is_symmetric\ndel self._eigenvects\n\ndef _format_str(self, printer=None):\nif not printer:\nfrom sympy.printing.str import StrPrinter\nprinter = StrPrinter()\n# Handle zero dimensions:\nif self.rows == 0 or self.cols == 0:\nreturn 'Matrix(%s, %s, [])' % (self.rows, self.cols)\nif self.rows == 1:\nreturn \"Matrix([%s])\" % self.table(printer, rowsep=',\\n')\nreturn \"Matrix([\\n%s])\" % self.table(printer, rowsep=',\\n')\n\n@classmethod\ndef _handle_creation_inputs(cls, *args, **kwargs):\n\"\"\"Return the number of rows, cols and flat matrix elements.\n\nExamples\n========\n\n>>> from sympy import Matrix, I\n\nMatrix can be constructed as follows:\n\n* from a nested list of iterables\n\n>>> Matrix( ((1, 2+I), (3, 4)) )\nMatrix([\n[1, 2 + I],\n[3,     4]])\n\n* from un-nested iterable (interpreted as a column)\n\n>>> Matrix( [1, 2] )\nMatrix([\n[1],\n[2]])\n\n* from un-nested iterable with dimensions\n\n>>> Matrix(1, 2, [1, 2] )\nMatrix([[1, 2]])\n\n* from no arguments (a 0 x 0 matrix)\n\n>>> Matrix()\nMatrix(0, 0, [])\n\n* from a rule\n\n>>> Matrix(2, 2, lambda i, j: i/(j + 1) )\nMatrix([\n[0,   0],\n[1, 1/2]])\n\n\"\"\"\nfrom sympy.matrices.sparse import SparseMatrix\n\nflat_list = None\n\nif len(args) == 1:\n# Matrix(SparseMatrix(...))\nif isinstance(args[0], SparseMatrix):\nreturn args[0].rows, args[0].cols, flatten(args[0].tolist())\n\n# Matrix(Matrix(...))\nelif isinstance(args[0], MatrixBase):\nreturn args[0].rows, args[0].cols, args[0]._mat\n\n# Matrix(MatrixSymbol('X', 2, 2))\nelif isinstance(args[0], Basic) and args[0].is_Matrix:\nreturn args[0].rows, args[0].cols, args[0].as_explicit()._mat\n\n# Matrix(numpy.ones((2, 2)))\nelif hasattr(args[0], \"__array__\"):\n# NumPy array or matrix or some other object that implements\n# __array__. So let's first use this method to get a\n# numpy.array() and then make a python list out of it.\narr = args[0].__array__()\nif len(arr.shape) == 2:\nrows, cols = arr.shape[0], arr.shape[1]\nflat_list = [cls._sympify(i) for i in arr.ravel()]\nreturn rows, cols, flat_list\nelif len(arr.shape) == 1:\nrows, cols = arr.shape[0], 1\nflat_list = [S.Zero] * rows\nfor i in range(len(arr)):\nflat_list[i] = cls._sympify(arr[i])\nreturn rows, cols, flat_list\nelse:\nraise NotImplementedError(\n\"SymPy supports just 1D and 2D matrices\")\n\n# Matrix([1, 2, 3]) or Matrix([[1, 2], [3, 4]])\nelif is_sequence(args[0]) \\\nand not isinstance(args[0], DeferredVector):\nin_mat = []\nncol = set()\nfor row in args[0]:\nif isinstance(row, MatrixBase):\nin_mat.extend(row.tolist())\nif row.cols or row.rows:  # only pay attention if it's not 0x0\nncol.add(row.cols)\nelse:\nin_mat.append(row)\ntry:\nncol.add(len(row))\nexcept TypeError:\nncol.add(1)\nif len(ncol) > 1:\nraise ValueError(\"Got rows of variable lengths: %s\" %\nsorted(list(ncol)))\ncols = ncol.pop() if ncol else 0\nrows = len(in_mat) if cols else 0\nif rows:\nif not is_sequence(in_mat[0]):\ncols = 1\nflat_list = [cls._sympify(i) for i in in_mat]\nreturn rows, cols, flat_list\nflat_list = []\nfor j in range(rows):\nfor i in range(cols):\nflat_list.append(cls._sympify(in_mat[j][i]))\n\nelif len(args) == 3:\nrows = as_int(args[0])\ncols = as_int(args[1])\n\nif rows < 0 or cols < 0:\nraise ValueError(\"Cannot create a {} x {} matrix. \"\n\"Both dimensions must be positive\".format(rows, cols))\n\n# Matrix(2, 2, lambda i, j: i+j)\nif len(args) == 3 and isinstance(args[2], collections.Callable):\nop = args[2]\nflat_list = []\nfor i in range(rows):\nflat_list.extend(\n[cls._sympify(op(cls._sympify(i), cls._sympify(j)))\nfor j in range(cols)])\n\n# Matrix(2, 2, [1, 2, 3, 4])\nelif len(args) == 3 and is_sequence(args[2]):\nflat_list = args[2]\nif len(flat_list) != rows * cols:\nraise ValueError(\n'List length should be equal to rows*columns')\nflat_list = [cls._sympify(i) for i in flat_list]\n\n\n# Matrix()\nelif len(args) == 0:\n# Empty Matrix\nrows = cols = 0\nflat_list = []\n\nif flat_list is None:\nraise TypeError(\"Data type not understood\")\n\nreturn rows, cols, flat_list\n\ndef _setitem(self, key, value):\n\"\"\"Helper to set value at location given by key.\n\nExamples\n========\n\n>>> from sympy import Matrix, I, zeros, ones\n>>> m = Matrix(((1, 2+I), (3, 4)))\n>>> m\nMatrix([\n[1, 2 + I],\n[3,     4]])\n>>> m[1, 0] = 9\n>>> m\nMatrix([\n[1, 2 + I],\n[9,     4]])\n>>> m[1, 0] = [[0, 1]]\n\nTo replace row r you assign to position r*m where m\nis the number of columns:\n\n>>> M = zeros(4)\n>>> m = M.cols\n>>> M[3*m] = ones(1, m)*2; M\nMatrix([\n[0, 0, 0, 0],\n[0, 0, 0, 0],\n[0, 0, 0, 0],\n[2, 2, 2, 2]])\n\nAnd to replace column c you can assign to position c:\n\n>>> M[2] = ones(m, 1)*4; M\nMatrix([\n[0, 0, 4, 0],\n[0, 0, 4, 0],\n[0, 0, 4, 0],\n[2, 2, 4, 2]])\n\"\"\"\nfrom .dense import Matrix\n\nis_slice = isinstance(key, slice)\ni, j = key = self.key2ij(key)\nis_mat = isinstance(value, MatrixBase)\nif type(i) is slice or type(j) is slice:\nif is_mat:\nself.copyin_matrix(key, value)\nreturn\nif not isinstance(value, Expr) and is_sequence(value):\nself.copyin_list(key, value)\nreturn\nraise ValueError('unexpected value: %s' % value)\nelse:\nif (not is_mat and\nnot isinstance(value, Basic) and is_sequence(value)):\nvalue = Matrix(value)\nis_mat = True\nif is_mat:\nif is_slice:\nkey = (slice(*divmod(i, self.cols)),\nslice(*divmod(j, self.cols)))\nelse:\nkey = (slice(i, i + value.rows),\nslice(j, j + value.cols))\nself.copyin_matrix(key, value)\nelse:\nreturn i, j, self._sympify(value)\nreturn\n\ndef add(self, b):\n\"\"\"Return self + b \"\"\"\nreturn self + b\n\ndef cholesky_solve(self, rhs):\n\"\"\"Solves Ax = B using Cholesky decomposition,\nfor a general square non-singular matrix.\nFor a non-square matrix with rows > cols,\nthe least squares solution is returned.\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ndiagonal_solve\nLDLsolve\nLUsolve\nQRsolve\npinv_solve\n\"\"\"\nif self.is_symmetric():\nL = self._cholesky()\nelif self.rows >= self.cols:\nL = (self.T * self)._cholesky()\nrhs = self.T * rhs\nelse:\nraise NotImplementedError('Under-determined System. '\n'Try M.gauss_jordan_solve(rhs)')\nY = L._lower_triangular_solve(rhs)\nreturn (L.T)._upper_triangular_solve(Y)\n\ndef cholesky(self):\n\"\"\"Returns the Cholesky decomposition L of a matrix A\nsuch that L * L.T = A\n\nA must be a square, symmetric, positive-definite\nand non-singular matrix.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix\n>>> A = Matrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n>>> A.cholesky()\nMatrix([\n[ 5, 0, 0],\n[ 3, 3, 0],\n[-1, 1, 3]])\n>>> A.cholesky() * A.cholesky().T\nMatrix([\n[25, 15, -5],\n[15, 18,  0],\n[-5,  0, 11]])\n\nSee Also\n========\n\nLDLdecomposition\nLUdecomposition\nQRdecomposition\n\"\"\"\n\nif not self.is_square:\nraise NonSquareMatrixError(\"Matrix must be square.\")\nif not self.is_symmetric():\nraise ValueError(\"Matrix must be symmetric.\")\nreturn self._cholesky()\n\ndef condition_number(self):\n\"\"\"Returns the condition number of a matrix.\n\nThis is the maximum singular value divided by the minimum singular value\n\nExamples\n========\n\n>>> from sympy import Matrix, S\n>>> A = Matrix([[1, 0, 0], [0, 10, 0], [0, 0, S.One/10]])\n>>> A.condition_number()\n100\n\nSee Also\n========\n\nsingular_values\n\"\"\"\nif not self:\nreturn S.Zero\nsingularvalues = self.singular_values()\nreturn Max(*singularvalues) / Min(*singularvalues)\n\ndef copy(self):\n\"\"\"\nReturns the copy of a matrix.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> A = Matrix(2, 2, [1, 2, 3, 4])\n>>> A.copy()\nMatrix([\n[1, 2],\n[3, 4]])\n\n\"\"\"\nreturn self._new(self.rows, self.cols, self._mat)\n\ndef cross(self, b):\nr\"\"\"\nReturn the cross product of ``self`` and ``b`` relaxing the condition\nof compatible dimensions: if each has 3 elements, a matrix of the\nsame type and shape as ``self`` will be returned. If ``b`` has the same\nshape as ``self`` then common identities for the cross product (like\n`a \\times b = - b \\times a`) will hold.\n\nParameters\n==========\nb : 3x1 or 1x3 Matrix\n\nSee Also\n========\n\ndot\nmultiply\nmultiply_elementwise\n\"\"\"\nif not is_sequence(b):\nraise TypeError(\n\"`b` must be an ordered iterable or Matrix, not %s.\" %\ntype(b))\nif not (self.rows * self.cols == b.rows * b.cols == 3):\nraise ShapeError(\"Dimensions incorrect for cross product: %s x %s\" %\n((self.rows, self.cols), (b.rows, b.cols)))\nelse:\nreturn self._new(self.rows, self.cols, (\n(self[1] * b[2] - self[2] * b[1]),\n(self[2] * b[0] - self[0] * b[2]),\n(self[0] * b[1] - self[1] * b[0])))\n\n@property\ndef D(self):\n\"\"\"Return Dirac conjugate (if self.rows == 4).\n\nExamples\n========\n\n>>> from sympy import Matrix, I, eye\n>>> m = Matrix((0, 1 + I, 2, 3))\n>>> m.D\nMatrix([[0, 1 - I, -2, -3]])\n>>> m = (eye(4) + I*eye(4))\n>>> m[0, 3] = 2\n>>> m.D\nMatrix([\n[1 - I,     0,      0,      0],\n[    0, 1 - I,      0,      0],\n[    0,     0, -1 + I,      0],\n[    2,     0,      0, -1 + I]])\n\nIf the matrix does not have 4 rows an AttributeError will be raised\nbecause this property is only defined for matrices with 4 rows.\n\n>>> Matrix(eye(2)).D\nTraceback (most recent call last):\n...\nAttributeError: Matrix has no attribute D.\n\nSee Also\n========\n\nconjugate: By-element conjugation\nH: Hermite conjugation\n\"\"\"\nfrom sympy.physics.matrices import mgamma\nif self.rows != 4:\n# In Python 3.2, properties can only return an AttributeError\n# so we can't raise a ShapeError -- see commit which added the\n# first line of this inline comment. Also, there is no need\n# for a message since MatrixBase will raise the AttributeError\nraise AttributeError\nreturn self.H * mgamma(0)\n\ndef diagonal_solve(self, rhs):\n\"\"\"Solves Ax = B efficiently, where A is a diagonal Matrix,\nwith non-zero diagonal entries.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix, eye\n>>> A = eye(2)*2\n>>> B = Matrix([[1, 2], [3, 4]])\n>>> A.diagonal_solve(B) == B/2\nTrue\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\nLDLsolve\nLUsolve\nQRsolve\npinv_solve\n\"\"\"\nif not self.is_diagonal:\nraise TypeError(\"Matrix should be diagonal\")\nif rhs.rows != self.rows:\nraise TypeError(\"Size mis-match\")\nreturn self._diagonal_solve(rhs)\n\ndef dot(self, b):\n\"\"\"Return the dot product of Matrix self and b relaxing the condition\nof compatible dimensions: if either the number of rows or columns are\nthe same as the length of b then the dot product is returned. If self\nis a row or column vector, a scalar is returned. Otherwise, a list\nof results is returned (and in that case the number of columns in self\nmust match the length of b).\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> v = [1, 1, 1]\n>>> M.row(0).dot(v)\n6\n>>> M.col(0).dot(v)\n12\n>>> M.dot(v)\n[6, 15, 24]\n\nSee Also\n========\n\ncross\nmultiply\nmultiply_elementwise\n\"\"\"\nfrom .dense import Matrix\n\nif not isinstance(b, MatrixBase):\nif is_sequence(b):\nif len(b) != self.cols and len(b) != self.rows:\nraise ShapeError(\n\"Dimensions incorrect for dot product: %s, %s\" % (\nself.shape, len(b)))\nreturn self.dot(Matrix(b))\nelse:\nraise TypeError(\n\"`b` must be an ordered iterable or Matrix, not %s.\" %\ntype(b))\n\nmat = self\nif mat.cols == b.rows:\nif b.cols != 1:\nmat = mat.T\nb = b.T\nprod = flatten((mat * b).tolist())\nif len(prod) == 1:\nreturn prod[0]\nreturn prod\nif mat.cols == b.cols:\nreturn mat.dot(b.T)\nelif mat.rows == b.rows:\nreturn mat.T.dot(b)\nelse:\nraise ShapeError(\"Dimensions incorrect for dot product: %s, %s\" % (\nself.shape, b.shape))\n\ndef dual(self):\n\"\"\"Returns the dual of a matrix, which is:\n\n`(1/2)*levicivita(i, j, k, l)*M(k, l)` summed over indices `k` and `l`\n\nSince the levicivita method is anti_symmetric for any pairwise\nexchange of indices, the dual of a symmetric matrix is the zero\nmatrix. Strictly speaking the dual defined here assumes that the\n'matrix' `M` is a contravariant anti_symmetric second rank tensor,\nso that the dual is a covariant second rank tensor.\n\n\"\"\"\nfrom sympy import LeviCivita\nfrom sympy.matrices import zeros\n\nM, n = self[:, :], self.rows\nwork = zeros(n)\nif self.is_symmetric():\nreturn work\n\nfor i in range(1, n):\nfor j in range(1, n):\nacum = 0\nfor k in range(1, n):\nacum += LeviCivita(i, j, 0, k) * M[0, k]\nwork[i, j] = acum\nwork[j, i] = -acum\n\nfor l in range(1, n):\nacum = 0\nfor a in range(1, n):\nfor b in range(1, n):\nacum += LeviCivita(0, l, a, b) * M[a, b]\nacum /= 2\nwork[0, l] = -acum\nwork[l, 0] = acum\n\nreturn work\n\ndef exp(self):\n\"\"\"Return the exponentiation of a square matrix.\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError(\n\"Exponentiation is valid only for square matrices\")\ntry:\nP, J = self.jordan_form()\ncells = J.get_diag_blocks()\nexcept MatrixError:\nraise NotImplementedError(\n\"Exponentiation is implemented only for matrices for which the Jordan normal form can be computed\")\n\ndef _jblock_exponential(b):\n# This function computes the matrix exponential for one single Jordan block\nnr = b.rows\nl = b[0, 0]\nif nr == 1:\nres = exp(l)\nelse:\nfrom sympy import eye\n# extract the diagonal part\nd = b[0, 0] * eye(nr)\n# and the nilpotent part\nn = b - d\n# compute its exponential\nnex = eye(nr)\nfor i in range(1, nr):\nnex = nex + n ** i / factorial(i)\n# combine the two parts\nres = exp(b[0, 0]) * nex\nreturn (res)\n\nblocks = list(map(_jblock_exponential, cells))\nfrom sympy.matrices import diag\neJ = diag(*blocks)\n# n = self.rows\nret = P * eJ * P.inv()\nreturn type(self)(ret)\n\ndef gauss_jordan_solve(self, b, freevar=False):\n\"\"\"\nSolves Ax = b using Gauss Jordan elimination.\n\nThere may be zero, one, or infinite solutions.  If one solution\nexists, it will be returned. If infinite solutions exist, it will\nbe returned parametrically. If no solutions exist, It will throw\nValueError.\n\nParameters\n==========\n\nb : Matrix\nThe right hand side of the equation to be solved for.  Must have\nthe same number of rows as matrix A.\n\nfreevar : List\nIf the system is underdetermined (e.g. A has more columns than\nrows), infinite solutions are possible, in terms of arbitrary\nvalues of free variables. Then the index of the free variables\nin the solutions (column Matrix) will be returned by freevar, if\nthe flag `freevar` is set to `True`.\n\nReturns\n=======\n\nx : Matrix\nThe matrix that will satisfy Ax = B.  Will have as many rows as\nmatrix A has columns, and as many columns as matrix B.\n\nparams : Matrix\nIf the system is underdetermined (e.g. A has more columns than\nrows), infinite solutions are possible, in terms of arbitrary\nparameters. These arbitrary parameters are returned as params\nMatrix.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> A = Matrix([[1, 2, 1, 1], [1, 2, 2, -1], [2, 4, 0, 6]])\n>>> b = Matrix([7, 12, 4])\n>>> sol, params = A.gauss_jordan_solve(b)\n>>> sol\nMatrix([\n[-2*tau0 - 3*tau1 + 2],\n[                 tau0],\n[           2*tau1 + 5],\n[                 tau1]])\n>>> params\nMatrix([\n[tau0],\n[tau1]])\n\n>>> A = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 10]])\n>>> b = Matrix([3, 6, 9])\n>>> sol, params = A.gauss_jordan_solve(b)\n>>> sol\nMatrix([\n[-1],\n[ 2],\n[ 0]])\n>>> params\nMatrix(0, 1, [])\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nLUsolve\nQRsolve\npinv\n\nReferences\n==========\n\n.. [1] http://en.wikipedia.org/wiki/Gaussian_elimination\n\n\"\"\"\nfrom sympy.matrices import Matrix, zeros\n\naug = self.hstack(self.copy(), b.copy())\nrow, col = aug[:, :-1].shape\n\n# solve by reduced row echelon form\nA, pivots = aug.rref(simplify=True)\nA, v = A[:, :-1], A[:, -1]\npivots = list(filter(lambda p: p < col, pivots))\nrank = len(pivots)\n\n# Bring to block form\npermutation = Matrix(range(col)).T\nA = A.vstack(A, permutation)\n\nfor i, c in enumerate(pivots):\nA.col_swap(i, c)\n\nA, permutation = A[:-1, :], A[-1, :]\n\n# check for existence of solutions\n# rank of aug Matrix should be equal to rank of coefficient matrix\nif not v[rank:, 0].is_zero:\nraise ValueError(\"Linear system has no solution\")\n\n# Get index of free symbols (free parameters)\nfree_var_index = permutation[\nlen(pivots):]  # non-pivots columns are free variables\n\n# Free parameters\n# what are current unnumbered free symbol names?\nname = _uniquely_named_symbol('tau', aug,\ncompare=lambda i: str(i).rstrip('1234567890')).name\ngen = numbered_symbols(name)\ntau = Matrix([next(gen) for k in range(col - rank)]).reshape(col - rank, 1)\n\n# Full parametric solution\nV = A[:rank, rank:]\nvt = v[:rank, 0]\nfree_sol = tau.vstack(vt - V * tau, tau)\n\n# Undo permutation\nsol = zeros(col, 1)\nfor k, v in enumerate(free_sol):\nsol[permutation[k], 0] = v\n\nif freevar:\nreturn sol, tau, free_var_index\nelse:\nreturn sol, tau\n\ndef inv_mod(self, m):\nr\"\"\"\nReturns the inverse of the matrix `K` (mod `m`), if it exists.\n\nMethod to find the matrix inverse of `K` (mod `m`) implemented in this function:\n\n* Compute `\\mathrm{adj}(K) = \\mathrm{cof}(K)^t`, the adjoint matrix of `K`.\n\n* Compute `r = 1/\\mathrm{det}(K) \\pmod m`.\n\n* `K^{-1} = r\\cdot \\mathrm{adj}(K) \\pmod m`.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> A = Matrix(2, 2, [1, 2, 3, 4])\n>>> A.inv_mod(5)\nMatrix([\n[3, 1],\n[4, 2]])\n>>> A.inv_mod(3)\nMatrix([\n[1, 1],\n[0, 1]])\n\n\"\"\"\nfrom sympy.ntheory import totient\nif not self.is_square:\nraise NonSquareMatrixError()\nN = self.cols\nphi = totient(m)\ndet_K = self.det()\nif gcd(det_K, m) != 1:\nraise ValueError('Matrix is not invertible (mod %d)' % m)\ndet_inv = pow(int(det_K), int(phi - 1), int(m))\nK_adj = self.adjugate()\nK_inv = self.__class__(N, N,\n[det_inv * K_adj[i, j] % m for i in range(N) for\nj in range(N)])\nreturn K_inv\n\ndef inverse_ADJ(self, iszerofunc=_iszero):\n\"\"\"Calculates the inverse using the adjugate matrix and a determinant.\n\nSee Also\n========\n\ninv\ninverse_LU\ninverse_GE\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError(\"A Matrix must be square to invert.\")\n\nd = self.det(method='berkowitz')\nzero = d.equals(0)\nif zero is None:\n# if equals() can't decide, will rref be able to?\nok = self.rref(simplify=True)[0]\nzero = any(iszerofunc(ok[j, j]) for j in range(ok.rows))\nif zero:\nraise ValueError(\"Matrix det == 0; not invertible.\")\n\nreturn self.adjugate() / d\n\ndef inverse_GE(self, iszerofunc=_iszero):\n\"\"\"Calculates the inverse using Gaussian elimination.\n\nSee Also\n========\n\ninv\ninverse_LU\ninverse_ADJ\n\"\"\"\nfrom .dense import Matrix\nif not self.is_square:\nraise NonSquareMatrixError(\"A Matrix must be square to invert.\")\n\nbig = Matrix.hstack(self.as_mutable(), Matrix.eye(self.rows))\nred = big.rref(iszerofunc=iszerofunc, simplify=True)[0]\nif any(iszerofunc(red[j, j]) for j in range(red.rows)):\nraise ValueError(\"Matrix det == 0; not invertible.\")\n\nreturn self._new(red[:, big.rows:])\n\ndef inverse_LU(self, iszerofunc=_iszero):\n\"\"\"Calculates the inverse using LU decomposition.\n\nSee Also\n========\n\ninv\ninverse_GE\ninverse_ADJ\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError()\n\nok = self.rref(simplify=True)[0]\nif any(iszerofunc(ok[j, j]) for j in range(ok.rows)):\nraise ValueError(\"Matrix det == 0; not invertible.\")\n\nreturn self.LUsolve(self.eye(self.rows), iszerofunc=_iszero)\n\ndef inv(self, method=None, **kwargs):\n\"\"\"\nReturn the inverse of a matrix.\n\nCASE 1: If the matrix is a dense matrix.\n\nReturn the matrix inverse using the method indicated (default\nis Gauss elimination).\n\nParameters\n==========\n\nmethod : ('GE', 'LU', or 'ADJ')\n\nNotes\n=====\n\nAccording to the ``method`` keyword, it calls the appropriate method:\n\nGE .... inverse_GE(); default\nLU .... inverse_LU()\nADJ ... inverse_ADJ()\n\nSee Also\n========\n\ninverse_LU\ninverse_GE\ninverse_ADJ\n\nRaises\n------\nValueError\nIf the determinant of the matrix is zero.\n\nCASE 2: If the matrix is a sparse matrix.\n\nReturn the matrix inverse using Cholesky or LDL (default).\n\nkwargs\n======\n\nmethod : ('CH', 'LDL')\n\nNotes\n=====\n\nAccording to the ``method`` keyword, it calls the appropriate method:\n\nLDL ... inverse_LDL(); default\nCH .... inverse_CH()\n\nRaises\n------\nValueError\nIf the determinant of the matrix is zero.\n\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError()\nif method is not None:\nkwargs['method'] = method\nreturn self._eval_inverse(**kwargs)\n\ndef is_nilpotent(self):\n\"\"\"Checks if a matrix is nilpotent.\n\nA matrix B is nilpotent if for some integer k, B**k is\na zero matrix.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> a = Matrix([[0, 0, 0], [1, 0, 0], [1, 1, 0]])\n>>> a.is_nilpotent()\nTrue\n\n>>> a = Matrix([[1, 0, 1], [1, 0, 0], [1, 1, 0]])\n>>> a.is_nilpotent()\nFalse\n\"\"\"\nif not self:\nreturn True\nif not self.is_square:\nraise NonSquareMatrixError(\n\"Nilpotency is valid only for square matrices\")\nx = _uniquely_named_symbol('x', self)\np = self.charpoly(x)\nif p.args[0] == x ** self.rows:\nreturn True\nreturn False\n\ndef key2bounds(self, keys):\n\"\"\"Converts a key with potentially mixed types of keys (integer and slice)\ninto a tuple of ranges and raises an error if any index is out of self's\nrange.\n\nSee Also\n========\n\nkey2ij\n\"\"\"\n\nislice, jslice = [isinstance(k, slice) for k in keys]\nif islice:\nif not self.rows:\nrlo = rhi = 0\nelse:\nrlo, rhi = keys[0].indices(self.rows)[:2]\nelse:\nrlo = a2idx(keys[0], self.rows)\nrhi = rlo + 1\nif jslice:\nif not self.cols:\nclo = chi = 0\nelse:\nclo, chi = keys[1].indices(self.cols)[:2]\nelse:\nclo = a2idx(keys[1], self.cols)\nchi = clo + 1\nreturn rlo, rhi, clo, chi\n\ndef key2ij(self, key):\n\"\"\"Converts key into canonical form, converting integers or indexable\nitems into valid integers for self's range or returning slices\nunchanged.\n\nSee Also\n========\n\nkey2bounds\n\"\"\"\nif is_sequence(key):\nif not len(key) == 2:\nraise TypeError('key must be a sequence of length 2')\nreturn [a2idx(i, n) if not isinstance(i, slice) else i\nfor i, n in zip(key, self.shape)]\nelif isinstance(key, slice):\nreturn key.indices(len(self))[:2]\nelse:\nreturn divmod(a2idx(key, len(self)), self.cols)\n\ndef LDLdecomposition(self):\n\"\"\"Returns the LDL Decomposition (L, D) of matrix A,\nsuch that L * D * L.T == A\nThis method eliminates the use of square root.\nFurther this ensures that all the diagonal entries of L are 1.\nA must be a square, symmetric, positive-definite\nand non-singular matrix.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix, eye\n>>> A = Matrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n>>> L, D = A.LDLdecomposition()\n>>> L\nMatrix([\n[   1,   0, 0],\n[ 3/5,   1, 0],\n[-1/5, 1/3, 1]])\n>>> D\nMatrix([\n[25, 0, 0],\n[ 0, 9, 0],\n[ 0, 0, 9]])\n>>> L * D * L.T * A.inv() == eye(A.rows)\nTrue\n\nSee Also\n========\n\ncholesky\nLUdecomposition\nQRdecomposition\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError(\"Matrix must be square.\")\nif not self.is_symmetric():\nraise ValueError(\"Matrix must be symmetric.\")\nreturn self._LDLdecomposition()\n\ndef LDLsolve(self, rhs):\n\"\"\"Solves Ax = B using LDL decomposition,\nfor a general square and non-singular matrix.\n\nFor a non-square matrix with rows > cols,\nthe least squares solution is returned.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix, eye\n>>> A = eye(2)*2\n>>> B = Matrix([[1, 2], [3, 4]])\n>>> A.LDLsolve(B) == B/2\nTrue\n\nSee Also\n========\n\nLDLdecomposition\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLUsolve\nQRsolve\npinv_solve\n\"\"\"\nif self.is_symmetric():\nL, D = self.LDLdecomposition()\nelif self.rows >= self.cols:\nL, D = (self.T * self).LDLdecomposition()\nrhs = self.T * rhs\nelse:\nraise NotImplementedError('Under-determined System. '\n'Try M.gauss_jordan_solve(rhs)')\nY = L._lower_triangular_solve(rhs)\nZ = D._diagonal_solve(Y)\nreturn (L.T)._upper_triangular_solve(Z)\n\ndef lower_triangular_solve(self, rhs):\n\"\"\"Solves Ax = B, where A is a lower triangular matrix.\n\nSee Also\n========\n\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nLUsolve\nQRsolve\npinv_solve\n\"\"\"\n\nif not self.is_square:\nraise NonSquareMatrixError(\"Matrix must be square.\")\nif rhs.rows != self.rows:\nraise ShapeError(\"Matrices size mismatch.\")\nif not self.is_lower:\nraise ValueError(\"Matrix must be lower triangular.\")\nreturn self._lower_triangular_solve(rhs)\n\ndef LUdecomposition(self,\niszerofunc=_iszero,\nsimpfunc=None,\nrankcheck=False):\n\"\"\"Returns (L, U, perm) where L is a lower triangular matrix with unit\ndiagonal, U is an upper triangular matrix, and perm is a list of row\nswap index pairs. If A is the original matrix, then\nA = (L*U).permuteBkwd(perm), and the row permutation matrix P such\nthat P*A = L*U can be computed by P=eye(A.row).permuteFwd(perm).\n\nSee documentation for LUCombined for details about the keyword argument\nrankcheck, iszerofunc, and simpfunc.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> a = Matrix([[4, 3], [6, 3]])\n>>> L, U, _ = a.LUdecomposition()\n>>> L\nMatrix([\n[  1, 0],\n[3/2, 1]])\n>>> U\nMatrix([\n[4,    3],\n[0, -3/2]])\n\nSee Also\n========\n\ncholesky\nLDLdecomposition\nQRdecomposition\nLUdecomposition_Simple\nLUdecompositionFF\nLUsolve\n\"\"\"\n\ncombined, p = self.LUdecomposition_Simple(iszerofunc=iszerofunc,\nsimpfunc=simpfunc,\nrankcheck=rankcheck)\n\n# L is lower triangular self.rows x self.rows\n# U is upper triangular self.rows x self.cols\n# L has unit diagonal. For each column in combined, the subcolumn\n# below the diagonal of combined is shared by L.\n# If L has more columns than combined, then the remaining subcolumns\n# below the diagonal of L are zero.\n# The upper triangular portion of L and combined are equal.\ndef entry_L(i, j):\nif i < j:\n# Super diagonal entry\nreturn S.Zero\nelif i == j:\nreturn S.One\nelif j < combined.cols:\nreturn combined[i, j]\n# Subdiagonal entry of L with no corresponding\n# entry in combined\nreturn S.Zero\n\ndef entry_U(i, j):\nreturn S.Zero if i > j else combined[i, j]\n\nL = self._new(combined.rows, combined.rows, entry_L)\nU = self._new(combined.rows, combined.cols, entry_U)\n\nreturn L, U, p\n\n\ndef LUdecomposition_Simple(self,\niszerofunc=_iszero,\nsimpfunc=None,\nrankcheck=False):\n\"\"\"Compute an lu decomposition of m x n matrix A, where P*A = L*U\n\n* L is m x m lower triangular with unit diagonal\n* U is m x n upper triangular\n* P is an m x m permutation matrix\n\nReturns an m x n matrix lu, and an m element list perm where each\nelement of perm is a pair of row exchange indices.\n\nThe factors L and U are stored in lu as follows:\nThe subdiagonal elements of L are stored in the subdiagonal elements\nof lu, that is lu[i, j] = L[i, j] whenever i > j.\nThe elements on the diagonal of L are all 1, and are not explicitly\nstored.\nU is stored in the upper triangular portion of lu, that is\nlu[i ,j] = U[i, j] whenever i <= j.\nThe output matrix can be visualized as:\n\nMatrix([\n[u, u, u, u],\n[l, u, u, u],\n[l, l, u, u],\n[l, l, l, u]])\n\nwhere l represents a subdiagonal entry of the L factor, and u\nrepresents an entry from the upper triangular entry of the U\nfactor.\n\nperm is a list row swap index pairs such that if A is the original\nmatrix, then A = (L*U).permuteBkwd(perm), and the row permutation\nmatrix P such that ``P*A = L*U`` can be computed by\n``P=eye(A.row).permuteFwd(perm)``.\n\nThe keyword argument rankcheck determines if this function raises a\nValueError when passed a matrix whose rank is strictly less than\nmin(num rows, num cols). The default behavior is to decompose a rank\ndeficient matrix. Pass rankcheck=True to raise a\nValueError instead. (This mimics the previous behavior of this function).\n\nThe keyword arguments iszerofunc and simpfunc are used by the pivot\nsearch algorithm.\niszerofunc is a callable that returns a boolean indicating if its\ninput is zero, or None if it cannot make the determination.\nsimpfunc is a callable that simplifies its input.\nThe default is simpfunc=None, which indicate that the pivot search\nalgorithm should not attempt to simplify any candidate pivots.\nIf simpfunc fails to simplify its input, then it must return its input\ninstead of a copy.\n\nWhen a matrix contains symbolic entries, the pivot search algorithm\ndiffers from the case where every entry can be categorized as zero or\nnonzero.\nThe algorithm searches column by column through the submatrix whose\ntop left entry coincides with the pivot position.\nIf it exists, the pivot is the first entry in the current search\ncolumn that iszerofunc guarantees is nonzero.\nIf no such candidate exists, then each candidate pivot is simplified\nif simpfunc is not None.\nThe search is repeated, with the difference that a candidate may be\nthe pivot if ``iszerofunc()`` cannot guarantee that it is nonzero.\nIn the second search the pivot is the first candidate that\niszerofunc can guarantee is nonzero.\nIf no such candidate exists, then the pivot is the first candidate\nfor which iszerofunc returns None.\nIf no such candidate exists, then the search is repeated in the next\ncolumn to the right.\nThe pivot search algorithm differs from the one in `rref()`, which\nrelies on ``_find_reasonable_pivot()``.\nFuture versions of ``LUdecomposition_simple()`` may use\n``_find_reasonable_pivot()``.\n\nSee Also\n========\n\nLUdecomposition\nLUdecompositionFF\nLUsolve\n\"\"\"\n\nif rankcheck:\n# https://github.com/sympy/sympy/issues/9796\npass\n\nif self.rows == 0 or self.cols == 0:\n# Define LU decomposition of a matrix with no entries as a matrix\n# of the same dimensions with all zero entries.\nreturn self.zeros(self.rows, self.cols), []\n\nlu = self.as_mutable()\nrow_swaps = []\n\npivot_col = 0\nfor pivot_row in range(0, lu.rows - 1):\n# Search for pivot. Prefer entry that iszeropivot determines\n# is nonzero, over entry that iszeropivot cannot guarantee\n# is  zero.\n# XXX `_find_reasonable_pivot` uses slow zero testing. Blocked by bug #10279\n# Future versions of LUdecomposition_simple can pass iszerofunc and simpfunc\n# to _find_reasonable_pivot().\n# In pass 3 of _find_reasonable_pivot(), the predicate in `if x.equals(S.Zero):`\n# calls sympy.simplify(), and not the simplification function passed in via\n# the keyword argument simpfunc.\n\niszeropivot = True\nwhile pivot_col != self.cols and iszeropivot:\nsub_col = (lu[r, pivot_col] for r in range(pivot_row, self.rows))\npivot_row_offset, pivot_value, is_assumed_non_zero, ind_simplified_pairs =\\\n_find_reasonable_pivot_naive(sub_col, iszerofunc, simpfunc)\niszeropivot = pivot_value is None\nif iszeropivot:\n# All candidate pivots in this column are zero.\n# Proceed to next column.\npivot_col += 1\n\nif rankcheck and pivot_col != pivot_row:\n# All entries including and below the pivot position are\n# zero, which indicates that the rank of the matrix is\n# strictly less than min(num rows, num cols)\n# Mimic behavior of previous implementation, by throwing a\n# ValueError.\nraise ValueError(\"Rank of matrix is strictly less than\"\n\" number of rows or columns.\"\n\" Pass keyword argument\"\n\" rankcheck=False to compute\"\n\" the LU decomposition of this matrix.\")\n\ncandidate_pivot_row = None if pivot_row_offset is None else pivot_row + pivot_row_offset\n\nif candidate_pivot_row is None and iszeropivot:\n# If candidate_pivot_row is None and iszeropivot is True\n# after pivot search has completed, then the submatrix\n# below and to the right of (pivot_row, pivot_col) is\n# all zeros, indicating that Gaussian elimination is\n# complete.\nreturn lu, row_swaps\n\n# Update entries simplified during pivot search.\nfor offset, val in ind_simplified_pairs:\nlu[pivot_row + offset, pivot_col] = val\n\nif pivot_row != candidate_pivot_row:\n# Row swap book keeping:\n# Record which rows were swapped.\n# Update stored portion of L factor by multiplying L on the\n# left and right with the current permutation.\n# Swap rows of U.\nrow_swaps.append([pivot_row, candidate_pivot_row])\n\n# Update L.\nlu[pivot_row, 0:pivot_row], lu[candidate_pivot_row, 0:pivot_row] = \\\nlu[candidate_pivot_row, 0:pivot_row], lu[pivot_row, 0:pivot_row]\n\n# Swap pivot row of U with candidate pivot row.\nlu[pivot_row, pivot_col:lu.cols], lu[candidate_pivot_row, pivot_col:lu.cols] = \\\nlu[candidate_pivot_row, pivot_col:lu.cols], lu[pivot_row, pivot_col:lu.cols]\n\n# Introduce zeros below the pivot by adding a multiple of the\n# pivot row to a row under it, and store the result in the\n# row under it.\n# Only entries in the target row whose index is greater than\n# start_col may be nonzero.\nstart_col = pivot_col + 1\nfor row in range(pivot_row + 1, lu.rows):\n# Store factors of L in the subcolumn below\n# (pivot_row, pivot_row).\nlu[row, pivot_row] =\\\nlu[row, pivot_col]/lu[pivot_row, pivot_col]\n\n# Form the linear combination of the pivot row and the current\n# row below the pivot row that zeros the entries below the pivot.\n# Employing slicing instead of a loop here raises\n# NotImplementedError: Cannot add Zero to MutableSparseMatrix\n# in sympy/matrices/tests/test_sparse.py.\n# c = pivot_row + 1 if pivot_row == pivot_col else pivot_col\nfor c in range(start_col, lu.cols):\nlu[row, c] = lu[row, c] - lu[row, pivot_row]*lu[pivot_row, c]\n\nif pivot_row != pivot_col:\n# matrix rank < min(num rows, num cols),\n# so factors of L are not stored directly below the pivot.\n# These entries are zero by construction, so don't bother\n# computing them.\nfor row in range(pivot_row + 1, lu.rows):\nlu[row, pivot_col] = S.Zero\n\npivot_col += 1\nif pivot_col == lu.cols:\n# All candidate pivots are zero implies that Gaussian\n# elimination is complete.\nreturn lu, row_swaps\n\nreturn lu, row_swaps\n\ndef LUdecompositionFF(self):\n\"\"\"Compute a fraction-free LU decomposition.\n\nReturns 4 matrices P, L, D, U such that PA = L D**-1 U.\nIf the elements of the matrix belong to some integral domain I, then all\nelements of L, D and U are guaranteed to belong to I.\n\n**Reference**\n- W. Zhou & D.J. Jeffrey, \"Fraction-free matrix factors: new forms\nfor LU and QR factors\". Frontiers in Computer Science in China,\nVol 2, no. 1, pp. 67-80, 2008.\n\nSee Also\n========\n\nLUdecomposition\nLUdecomposition_Simple\nLUsolve\n\"\"\"\nfrom sympy.matrices import SparseMatrix\nzeros = SparseMatrix.zeros\neye = SparseMatrix.eye\n\nn, m = self.rows, self.cols\nU, L, P = self.as_mutable(), eye(n), eye(n)\nDD = zeros(n, n)\noldpivot = 1\n\nfor k in range(n - 1):\nif U[k, k] == 0:\nfor kpivot in range(k + 1, n):\nif U[kpivot, k]:\nbreak\nelse:\nraise ValueError(\"Matrix is not full rank\")\nU[k, k:], U[kpivot, k:] = U[kpivot, k:], U[k, k:]\nL[k, :k], L[kpivot, :k] = L[kpivot, :k], L[k, :k]\nP[k, :], P[kpivot, :] = P[kpivot, :], P[k, :]\nL[k, k] = Ukk = U[k, k]\nDD[k, k] = oldpivot * Ukk\nfor i in range(k + 1, n):\nL[i, k] = Uik = U[i, k]\nfor j in range(k + 1, m):\nU[i, j] = (Ukk * U[i, j] - U[k, j] * Uik) / oldpivot\nU[i, k] = 0\noldpivot = Ukk\nDD[n - 1, n - 1] = oldpivot\nreturn P, L, DD, U\n\ndef LUsolve(self, rhs, iszerofunc=_iszero):\n\"\"\"Solve the linear system Ax = rhs for x where A = self.\n\nThis is for symbolic matrices, for real or complex ones use\nmpmath.lu_solve or mpmath.qr_solve.\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nQRsolve\npinv_solve\nLUdecomposition\n\"\"\"\nif rhs.rows != self.rows:\nraise ShapeError(\n\"`self` and `rhs` must have the same number of rows.\")\n\nA, perm = self.LUdecomposition_Simple(iszerofunc=_iszero)\nn = self.rows\nb = rhs.permute_rows(perm).as_mutable()\n# forward substitution, all diag entries are scaled to 1\nfor i in range(n):\nfor j in range(i):\nscale = A[i, j]\nb.zip_row_op(i, j, lambda x, y: x - y * scale)\n# backward substitution\nfor i in range(n - 1, -1, -1):\nfor j in range(i + 1, n):\nscale = A[i, j]\nb.zip_row_op(i, j, lambda x, y: x - y * scale)\nscale = A[i, i]\nb.row_op(i, lambda x, _: x / scale)\nreturn rhs.__class__(b)\n\ndef multiply(self, b):\n\"\"\"Returns self*b\n\nSee Also\n========\n\ndot\ncross\nmultiply_elementwise\n\"\"\"\nreturn self * b\n\ndef normalized(self):\n\"\"\"Return the normalized version of ``self``.\n\nSee Also\n========\n\nnorm\n\"\"\"\nif self.rows != 1 and self.cols != 1:\nraise ShapeError(\"A Matrix must be a vector to normalize.\")\nnorm = self.norm()\nout = self.applyfunc(lambda i: i / norm)\nreturn out\n\ndef norm(self, ord=None):\n\"\"\"Return the Norm of a Matrix or Vector.\nIn the simplest case this is the geometric size of the vector\nOther norms can be specified by the ord parameter\n\n\n=====  ============================  ==========================\nord    norm for matrices             norm for vectors\n=====  ============================  ==========================\nNone   Frobenius norm                2-norm\n'fro'  Frobenius norm                - does not exist\ninf    maximum row sum               max(abs(x))\n-inf   --                            min(abs(x))\n1      maximum column sum            as below\n-1     --                            as below\n2      2-norm (largest sing. value)  as below\n-2     smallest singular value       as below\nother  - does not exist              sum(abs(x)**ord)**(1./ord)\n=====  ============================  ==========================\n\nExamples\n========\n\n>>> from sympy import Matrix, Symbol, trigsimp, cos, sin, oo\n>>> x = Symbol('x', real=True)\n>>> v = Matrix([cos(x), sin(x)])\n>>> trigsimp( v.norm() )\n1\n>>> v.norm(10)\n(sin(x)**10 + cos(x)**10)**(1/10)\n>>> A = Matrix([[1, 1], [1, 1]])\n>>> A.norm(1) # maximum sum of absolute values of A is 2\n2\n>>> A.norm(2) # Spectral norm (max of |Ax|/|x| under 2-vector-norm)\n2\n>>> A.norm(-2) # Inverse spectral norm (smallest singular value)\n0\n>>> A.norm() # Frobenius Norm\n2\n>>> A.norm(oo) # Infinity Norm\n2\n>>> Matrix([1, -2]).norm(oo)\n2\n>>> Matrix([-1, 2]).norm(-oo)\n1\n\nSee Also\n========\n\nnormalized\n\"\"\"\n# Row or Column Vector Norms\nvals = list(self.values()) or [0]\nif self.rows == 1 or self.cols == 1:\nif ord == 2 or ord is None:  # Common case sqrt(<x, x>)\nreturn sqrt(Add(*(abs(i) ** 2 for i in vals)))\n\nelif ord == 1:  # sum(abs(x))\nreturn Add(*(abs(i) for i in vals))\n\nelif ord == S.Infinity:  # max(abs(x))\nreturn Max(*[abs(i) for i in vals])\n\nelif ord == S.NegativeInfinity:  # min(abs(x))\nreturn Min(*[abs(i) for i in vals])\n\n# Otherwise generalize the 2-norm, Sum(x_i**ord)**(1/ord)\n# Note that while useful this is not mathematically a norm\ntry:\nreturn Pow(Add(*(abs(i) ** ord for i in vals)), S(1) / ord)\nexcept (NotImplementedError, TypeError):\nraise ValueError(\"Expected order to be Number, Symbol, oo\")\n\n# Matrix Norms\nelse:\nif ord == 1:  # Maximum column sum\nm = self.applyfunc(abs)\nreturn Max(*[sum(m.col(i)) for i in range(m.cols)])\n\nelif ord == 2:  # Spectral Norm\n# Maximum singular value\nreturn Max(*self.singular_values())\n\nelif ord == -2:\n# Minimum singular value\nreturn Min(*self.singular_values())\n\nelif ord == S.Infinity:   # Infinity Norm - Maximum row sum\nm = self.applyfunc(abs)\nreturn Max(*[sum(m.row(i)) for i in range(m.rows)])\n\nelif (ord is None or isinstance(ord,\nstring_types) and ord.lower() in\n['f', 'fro', 'frobenius', 'vector']):\n# Reshape as vector and send back to norm function\nreturn self.vec().norm(ord=2)\n\nelse:\nraise NotImplementedError(\"Matrix Norms under development\")\n\ndef pinv_solve(self, B, arbitrary_matrix=None):\n\"\"\"Solve Ax = B using the Moore-Penrose pseudoinverse.\n\nThere may be zero, one, or infinite solutions.  If one solution\nexists, it will be returned.  If infinite solutions exist, one will\nbe returned based on the value of arbitrary_matrix.  If no solutions\nexist, the least-squares solution is returned.\n\nParameters\n==========\n\nB : Matrix\nThe right hand side of the equation to be solved for.  Must have\nthe same number of rows as matrix A.\narbitrary_matrix : Matrix\nIf the system is underdetermined (e.g. A has more columns than\nrows), infinite solutions are possible, in terms of an arbitrary\nmatrix.  This parameter may be set to a specific matrix to use\nfor that purpose; if so, it must be the same shape as x, with as\nmany rows as matrix A has columns, and as many columns as matrix\nB.  If left as None, an appropriate matrix containing dummy\nsymbols in the form of ``wn_m`` will be used, with n and m being\nrow and column position of each symbol.\n\nReturns\n=======\n\nx : Matrix\nThe matrix that will satisfy Ax = B.  Will have as many rows as\nmatrix A has columns, and as many columns as matrix B.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> A = Matrix([[1, 2, 3], [4, 5, 6]])\n>>> B = Matrix([7, 8])\n>>> A.pinv_solve(B)\nMatrix([\n[ _w0_0/6 - _w1_0/3 + _w2_0/6 - 55/18],\n[-_w0_0/3 + 2*_w1_0/3 - _w2_0/3 + 1/9],\n[ _w0_0/6 - _w1_0/3 + _w2_0/6 + 59/18]])\n>>> A.pinv_solve(B, arbitrary_matrix=Matrix([0, 0, 0]))\nMatrix([\n[-55/18],\n[   1/9],\n[ 59/18]])\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nLUsolve\nQRsolve\npinv\n\nNotes\n=====\n\nThis may return either exact solutions or least squares solutions.\nTo determine which, check ``A * A.pinv() * B == B``.  It will be\nTrue if exact solutions exist, and False if only a least-squares\nsolution exists.  Be aware that the left hand side of that equation\nmay need to be simplified to correctly compare to the right hand\nside.\n\nReferences\n==========\n\n.. [1] https://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse#Obtaining_all_solutions_of_a_linear_system\n\n\"\"\"\nfrom sympy.matrices import eye\nA = self\nA_pinv = self.pinv()\nif arbitrary_matrix is None:\nrows, cols = A.cols, B.cols\nw = symbols('w:{0}_:{1}'.format(rows, cols), cls=Dummy)\narbitrary_matrix = self.__class__(cols, rows, w).T\nreturn A_pinv * B + (eye(A.cols) - A_pinv * A) * arbitrary_matrix\n\ndef pinv(self):\n\"\"\"Calculate the Moore-Penrose pseudoinverse of the matrix.\n\nThe Moore-Penrose pseudoinverse exists and is unique for any matrix.\nIf the matrix is invertible, the pseudoinverse is the same as the\ninverse.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> Matrix([[1, 2, 3], [4, 5, 6]]).pinv()\nMatrix([\n[-17/18,  4/9],\n[  -1/9,  1/9],\n[ 13/18, -2/9]])\n\nSee Also\n========\n\ninv\npinv_solve\n\nReferences\n==========\n\n.. [1] https://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse\n\n\"\"\"\nA = self\nAH = self.H\n# Trivial case: pseudoinverse of all-zero matrix is its transpose.\nif A.is_zero:\nreturn AH\ntry:\nif self.rows >= self.cols:\nreturn (AH * A).inv() * AH\nelse:\nreturn AH * (A * AH).inv()\nexcept ValueError:\n# Matrix is not full rank, so A*AH cannot be inverted.\nraise NotImplementedError('Rank-deficient matrices are not yet '\n'supported.')\n\ndef print_nonzero(self, symb=\"X\"):\n\"\"\"Shows location of non-zero entries for fast shape lookup.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix, eye\n>>> m = Matrix(2, 3, lambda i, j: i*3+j)\n>>> m\nMatrix([\n[0, 1, 2],\n[3, 4, 5]])\n>>> m.print_nonzero()\n[ XX]\n[XXX]\n>>> m = eye(4)\n>>> m.print_nonzero(\"x\")\n[x   ]\n[ x  ]\n[  x ]\n[   x]\n\n\"\"\"\ns = []\nfor i in range(self.rows):\nline = []\nfor j in range(self.cols):\nif self[i, j] == 0:\nline.append(\" \")\nelse:\nline.append(str(symb))\ns.append(\"[%s]\" % ''.join(line))\nprint('\\n'.join(s))\n\ndef project(self, v):\n\"\"\"Return the projection of ``self`` onto the line containing ``v``.\n\nExamples\n========\n\n>>> from sympy import Matrix, S, sqrt\n>>> V = Matrix([sqrt(3)/2, S.Half])\n>>> x = Matrix([[1, 0]])\n>>> V.project(x)\nMatrix([[sqrt(3)/2, 0]])\n>>> V.project(-x)\nMatrix([[sqrt(3)/2, 0]])\n\"\"\"\nreturn v * (self.dot(v) / v.dot(v))\n\ndef QRdecomposition(self):\n\"\"\"Return Q, R where A = Q*R, Q is orthogonal and R is upper triangular.\n\nExamples\n========\n\nThis is the example from wikipedia:\n\n>>> from sympy import Matrix\n>>> A = Matrix([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> Q, R = A.QRdecomposition()\n>>> Q\nMatrix([\n[ 6/7, -69/175, -58/175],\n[ 3/7, 158/175,   6/175],\n[-2/7,    6/35,  -33/35]])\n>>> R\nMatrix([\n[14,  21, -14],\n[ 0, 175, -70],\n[ 0,   0,  35]])\n>>> A == Q*R\nTrue\n\nQR factorization of an identity matrix:\n\n>>> A = Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n>>> Q, R = A.QRdecomposition()\n>>> Q\nMatrix([\n[1, 0, 0],\n[0, 1, 0],\n[0, 0, 1]])\n>>> R\nMatrix([\n[1, 0, 0],\n[0, 1, 0],\n[0, 0, 1]])\n\nSee Also\n========\n\ncholesky\nLDLdecomposition\nLUdecomposition\nQRsolve\n\"\"\"\ncls = self.__class__\nmat = self.as_mutable()\n\nif not mat.rows >= mat.cols:\nraise MatrixError(\n\"The number of rows must be greater than columns\")\nn = mat.rows\nm = mat.cols\nrank = n\nrow_reduced = mat.rref()[0]\nfor i in range(row_reduced.rows):\nif row_reduced.row(i).norm() == 0:\nrank -= 1\nif not rank == mat.cols:\nraise MatrixError(\"The rank of the matrix must match the columns\")\nQ, R = mat.zeros(n, m), mat.zeros(m)\nfor j in range(m):  # for each column vector\ntmp = mat[:, j]  # take original v\nfor i in range(j):\n# subtract the project of mat on new vector\ntmp -= Q[:, i] * mat[:, j].dot(Q[:, i])\ntmp.expand()\n# normalize it\nR[j, j] = tmp.norm()\nQ[:, j] = tmp / R[j, j]\nif Q[:, j].norm() != 1:\nraise NotImplementedError(\n\"Could not normalize the vector %d.\" % j)\nfor i in range(j):\nR[i, j] = Q[:, i].dot(mat[:, j])\nreturn cls(Q), cls(R)\n\ndef QRsolve(self, b):\n\"\"\"Solve the linear system 'Ax = b'.\n\n'self' is the matrix 'A', the method argument is the vector\n'b'.  The method returns the solution vector 'x'.  If 'b' is a\nmatrix, the system is solved for each column of 'b' and the\nreturn value is a matrix of the same shape as 'b'.\n\nThis method is slower (approximately by a factor of 2) but\nmore stable for floating-point arithmetic than the LUsolve method.\nHowever, LUsolve usually uses an exact arithmetic, so you don't need\nto use QRsolve.\n\nThis is mainly for educational purposes and symbolic matrices, for real\n(or complex) matrices use mpmath.qr_solve.\n\nSee Also\n========\n\nlower_triangular_solve\nupper_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nLUsolve\npinv_solve\nQRdecomposition\n\"\"\"\n\nQ, R = self.as_mutable().QRdecomposition()\ny = Q.T * b\n\n# back substitution to solve R*x = y:\n# We build up the result \"backwards\" in the vector 'x' and reverse it\n# only in the end.\nx = []\nn = R.rows\nfor j in range(n - 1, -1, -1):\ntmp = y[j, :]\nfor k in range(j + 1, n):\ntmp -= R[j, k] * x[n - 1 - k]\nx.append(tmp / R[j, j])\nreturn self._new([row._mat for row in reversed(x)])\n\ndef solve_least_squares(self, rhs, method='CH'):\n\"\"\"Return the least-square fit to the data.\n\nBy default the cholesky_solve routine is used (method='CH'); other\nmethods of matrix inversion can be used. To find out which are\navailable, see the docstring of the .inv() method.\n\nExamples\n========\n\n>>> from sympy.matrices import Matrix, ones\n>>> A = Matrix([1, 2, 3])\n>>> B = Matrix([2, 3, 4])\n>>> S = Matrix(A.row_join(B))\n>>> S\nMatrix([\n[1, 2],\n[2, 3],\n[3, 4]])\n\nIf each line of S represent coefficients of Ax + By\nand x and y are [2, 3] then S*xy is:\n\n>>> r = S*Matrix([2, 3]); r\nMatrix([\n[ 8],\n[13],\n[18]])\n\nBut let's add 1 to the middle value and then solve for the\nleast-squares value of xy:\n\n>>> xy = S.solve_least_squares(Matrix([8, 14, 18])); xy\nMatrix([\n[ 5/3],\n[10/3]])\n\nThe error is given by S*xy - r:\n\n>>> S*xy - r\nMatrix([\n[1/3],\n[1/3],\n[1/3]])\n>>> _.norm().n(2)\n0.58\n\nIf a different xy is used, the norm will be higher:\n\n>>> xy += ones(2, 1)/10\n>>> (S*xy - r).norm().n(2)\n1.5\n\n\"\"\"\nif method == 'CH':\nreturn self.cholesky_solve(rhs)\nt = self.T\nreturn (t * self).inv(method=method) * t * rhs\n\ndef solve(self, rhs, method='GE'):\n\"\"\"Return solution to self*soln = rhs using given inversion method.\n\nFor a list of possible inversion methods, see the .inv() docstring.\n\"\"\"\n\nif not self.is_square:\nif self.rows < self.cols:\nraise ValueError('Under-determined system. '\n'Try M.gauss_jordan_solve(rhs)')\nelif self.rows > self.cols:\nraise ValueError('For over-determined system, M, having '\n'more rows than columns, try M.solve_least_squares(rhs).')\nelse:\nreturn self.inv(method=method) * rhs\n\ndef table(self, printer, rowstart='[', rowend=']', rowsep='\\n',\ncolsep=', ', align='right'):\nr\"\"\"\nString form of Matrix as a table.\n\n``printer`` is the printer to use for on the elements (generally\nsomething like StrPrinter())\n\n``rowstart`` is the string used to start each row (by default '[').\n\n``rowend`` is the string used to end each row (by default ']').\n\n``rowsep`` is the string used to separate rows (by default a newline).\n\n``colsep`` is the string used to separate columns (by default ', ').\n\n``align`` defines how the elements are aligned. Must be one of 'left',\n'right', or 'center'.  You can also use '<', '>', and '^' to mean the\nsame thing, respectively.\n\nThis is used by the string printer for Matrix.\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> from sympy.printing.str import StrPrinter\n>>> M = Matrix([[1, 2], [-33, 4]])\n>>> printer = StrPrinter()\n>>> M.table(printer)\n'[  1, 2]\\n[-33, 4]'\n>>> print(M.table(printer))\n[  1, 2]\n[-33, 4]\n>>> print(M.table(printer, rowsep=',\\n'))\n[  1, 2],\n[-33, 4]\n>>> print('[%s]' % M.table(printer, rowsep=',\\n'))\n[[  1, 2],\n[-33, 4]]\n>>> print(M.table(printer, colsep=' '))\n[  1 2]\n[-33 4]\n>>> print(M.table(printer, align='center'))\n[ 1 , 2]\n[-33, 4]\n>>> print(M.table(printer, rowstart='{', rowend='}'))\n{  1, 2}\n{-33, 4}\n\"\"\"\n# Handle zero dimensions:\nif self.rows == 0 or self.cols == 0:\nreturn '[]'\n# Build table of string representations of the elements\nres = []\n# Track per-column max lengths for pretty alignment\nmaxlen = [0] * self.cols\nfor i in range(self.rows):\nres.append([])\nfor j in range(self.cols):\ns = printer._print(self[i, j])\nres[-1].append(s)\nmaxlen[j] = max(len(s), maxlen[j])\n# Patch strings together\nalign = {\n'left': 'ljust',\n'right': 'rjust',\n'center': 'center',\n'<': 'ljust',\n'>': 'rjust',\n'^': 'center',\n}[align]\nfor i, row in enumerate(res):\nfor j, elem in enumerate(row):\nrow[j] = getattr(elem, align)(maxlen[j])\nres[i] = rowstart + colsep.join(row) + rowend\nreturn rowsep.join(res)\n\ndef upper_triangular_solve(self, rhs):\n\"\"\"Solves Ax = B, where A is an upper triangular matrix.\n\nSee Also\n========\n\nlower_triangular_solve\ngauss_jordan_solve\ncholesky_solve\ndiagonal_solve\nLDLsolve\nLUsolve\nQRsolve\npinv_solve\n\"\"\"\nif not self.is_square:\nraise NonSquareMatrixError(\"Matrix must be square.\")\nif rhs.rows != self.rows:\nraise TypeError(\"Matrix size mismatch.\")\nif not self.is_upper:\nraise TypeError(\"Matrix is not upper triangular.\")\nreturn self._upper_triangular_solve(rhs)\n\ndef vech(self, diagonal=True, check_symmetry=True):\n\"\"\"Return the unique elements of a symmetric Matrix as a one column matrix\nby stacking the elements in the lower triangle.\n\nArguments:\ndiagonal -- include the diagonal cells of self or not\ncheck_symmetry -- checks symmetry of self but not completely reliably\n\nExamples\n========\n\n>>> from sympy import Matrix\n>>> m=Matrix([[1, 2], [2, 3]])\n>>> m\nMatrix([\n[1, 2],\n[2, 3]])\n>>> m.vech()\nMatrix([\n[1],\n[2],\n[3]])\n>>> m.vech(diagonal=False)\nMatrix([[2]])\n\nSee Also\n========\n\nvec\n\"\"\"\nfrom sympy.matrices import zeros\n\nc = self.cols\nif c != self.rows:\nraise ShapeError(\"Matrix must be square\")\nif check_symmetry:\nself.simplify()\nif self != self.transpose():\nraise ValueError(\n\"Matrix appears to be asymmetric; consider check_symmetry=False\")\ncount = 0\nif diagonal:\nv = zeros(c * (c + 1) // 2, 1)\nfor j in range(c):\nfor i in range(j, c):\nv[count] = self[i, j]\ncount += 1\nelse:\nv = zeros(c * (c - 1) // 2, 1)\nfor j in range(c):\nfor i in range(j + 1, c):\nv[count] = self[i, j]\ncount += 1\nreturn v\n\n\ndef classof(A, B):\n\"\"\"\nGet the type of the result when combining matrices of different types.\n\nCurrently the strategy is that immutability is contagious.\n\nExamples\n========\n\n>>> from sympy import Matrix, ImmutableMatrix\n>>> from sympy.matrices.matrices import classof\n>>> M = Matrix([[1, 2], [3, 4]]) # a Mutable Matrix\n>>> IM = ImmutableMatrix([[1, 2], [3, 4]])\n>>> classof(M, IM)\n<class 'sympy.matrices.immutable.ImmutableDenseMatrix'>\n\"\"\"\ntry:\nif A._class_priority > B._class_priority:\nreturn A.__class__\nelse:\nreturn B.__class__\nexcept AttributeError:\npass\ntry:\nimport numpy\nif isinstance(A, numpy.ndarray):\nreturn B.__class__\nif isinstance(B, numpy.ndarray):\nreturn A.__class__\nexcept (AttributeError, ImportError):\npass\nraise TypeError(\"Incompatible classes %s, %s\" % (A.__class__, B.__class__))\n\n\ndef a2idx(j, n=None):\n\"\"\"Return integer after making positive and validating against n.\"\"\"\nif type(j) is not int:\ntry:\nj = j.__index__()\nexcept AttributeError:\nraise IndexError(\"Invalid index a[%r]\" % (j,))\nif n is not None:\nif j < 0:\nj += n\nif not (j >= 0 and j < n):\nraise IndexError(\"Index out of range: a[%s]\" % j)\nreturn int(j)\n\n\ndef _find_reasonable_pivot(col, iszerofunc=_iszero, simpfunc=_simplify):\n\"\"\" Find the lowest index of an item in `col` that is\nsuitable for a pivot.  If `col` consists only of\nFloats, the pivot with the largest norm is returned.\nOtherwise, the first element where `iszerofunc` returns\nFalse is used.  If `iszerofunc` doesn't return false,\nitems are simplified and retested until a suitable\npivot is found.\n\nReturns a 4-tuple\n(pivot_offset, pivot_val, assumed_nonzero, newly_determined)\nwhere pivot_offset is the index of the pivot, pivot_val is\nthe (possibly simplified) value of the pivot, assumed_nonzero\nis True if an assumption that the pivot was non-zero\nwas made without being proved, and newly_determined are\nelements that were simplified during the process of pivot\nfinding.\"\"\"\n\nnewly_determined = []\ncol = list(col)\n# a column that contains a mix of floats and integers\n# but at least one float is considered a numerical\n# column, and so we do partial pivoting\nif all(isinstance(x, (Float, Integer)) for x in col) and any(\nisinstance(x, Float) for x in col):\ncol_abs = [abs(x) for x in col]\nmax_value = max(col_abs)\nif iszerofunc(max_value):\n# just because iszerofunc returned True, doesn't\n# mean the value is numerically zero.  Make sure\n# to replace all entries with numerical zeros\nif max_value != 0:\nnewly_determined = [(i, 0) for i, x in enumerate(col) if x != 0]\nreturn (None, None, False, newly_determined)\nindex = col_abs.index(max_value)\nreturn (index, col[index], False, newly_determined)\n\n# PASS 1 (iszerofunc directly)\npossible_zeros = []\nfor i, x in enumerate(col):\nis_zero = iszerofunc(x)\n# is someone wrote a custom iszerofunc, it may return\n# BooleanFalse or BooleanTrue instead of True or False,\n# so use == for comparison instead of `is`\nif is_zero == False:\n# we found something that is definitely not zero\nreturn (i, x, False, newly_determined)\npossible_zeros.append(is_zero)\n\n# by this point, we've found no certain non-zeros\nif all(possible_zeros):\n# if everything is definitely zero, we have\n# no pivot\nreturn (None, None, False, newly_determined)\n\n# PASS 2 (iszerofunc after simplify)\n# we haven't found any for-sure non-zeros, so\n# go through the elements iszerofunc couldn't\n# make a determination about and opportunistically\n# simplify to see if we find something\nfor i, x in enumerate(col):\nif possible_zeros[i] is not None:\ncontinue\nsimped = simpfunc(x)\nis_zero = iszerofunc(simped)\nif is_zero == True or is_zero == False:\nnewly_determined.append((i, simped))\nif is_zero == False:\nreturn (i, simped, False, newly_determined)\npossible_zeros[i] = is_zero\n\n# after simplifying, some things that were recognized\n# as zeros might be zeros\nif all(possible_zeros):\n# if everything is definitely zero, we have\n# no pivot\nreturn (None, None, False, newly_determined)\n\n# PASS 3 (.equals(0))\n# some expressions fail to simplify to zero, but\n# `.equals(0)` evaluates to True.  As a last-ditch\n# attempt, apply `.equals` to these expressions\nfor i, x in enumerate(col):\nif possible_zeros[i] is not None:\ncontinue\nif x.equals(S.Zero):\n# `.iszero` may return False with\n# an implicit assumption (e.g., `x.equals(0)`\n# when `x` is a symbol), so only treat it\n# as proved when `.equals(0)` returns True\npossible_zeros[i] = True\nnewly_determined.append((i, S.Zero))\n\nif all(possible_zeros):\nreturn (None, None, False, newly_determined)\n\n# at this point there is nothing that could definitely\n# be a pivot.  To maintain compatibility with existing\n# behavior, we'll assume that an illdetermined thing is\n# non-zero.  We should probably raise a warning in this case\ni = possible_zeros.index(None)\nreturn (i, col[i], True, newly_determined)\n\ndef _find_reasonable_pivot_naive(col, iszerofunc=_iszero, simpfunc=None):\n\"\"\"\nHelper that computes the pivot value and location from a\nsequence of contiguous matrix column elements. As a side effect\nof the pivot search, this function may simplify some of the elements\nof the input column. A list of these simplified entries and their\nindices are also returned.\nThis function mimics the behavior of _find_reasonable_pivot(),\nbut does less work trying to determine if an indeterminate candidate\npivot simplifies to zero. This more naive approach can be much faster,\nwith the trade-off that it may erroneously return a pivot that is zero.\n\n`col` is a sequence of contiguous column entries to be searched for\na suitable pivot.\n`iszerofunc` is a callable that returns a Boolean that indicates\nif its input is zero, or None if no such determination can be made.\n`simpfunc` is a callable that simplifies its input. It must return\nits input if it does not simplify its input. Passing in\n`simpfunc=None` indicates that the pivot search should not attempt\nto simplify any candidate pivots.\n\nReturns a 4-tuple:\n(pivot_offset, pivot_val, assumed_nonzero, newly_determined)\n`pivot_offset` is the sequence index of the pivot.\n`pivot_val` is the value of the pivot.\npivot_val and col[pivot_index] are equivalent, but will be different\nwhen col[pivot_index] was simplified during the pivot search.\n`assumed_nonzero` is a boolean indicating if the pivot cannot be\nguaranteed to be zero. If assumed_nonzero is true, then the pivot\nmay or may not be non-zero. If assumed_nonzero is false, then\nthe pivot is non-zero.\n`newly_determined` is a list of index-value pairs of pivot candidates\nthat were simplified during the pivot search.\n\"\"\"\n\n# indeterminates holds the index-value pairs of each pivot candidate\n# that is neither zero or non-zero, as determined by iszerofunc().\n# If iszerofunc() indicates that a candidate pivot is guaranteed\n# non-zero, or that every candidate pivot is zero then the contents\n# of indeterminates are unused.\n# Otherwise, the only viable candidate pivots are symbolic.\n# In this case, indeterminates will have at least one entry,\n# and all but the first entry are ignored when simpfunc is None.\nindeterminates = []\nfor i, col_val in enumerate(col):\ncol_val_is_zero = iszerofunc(col_val)\nif col_val_is_zero == False:\n# This pivot candidate is non-zero.\nreturn i, col_val, False, []\nelif col_val_is_zero is None:\n# The candidate pivot's comparison with zero\n# is indeterminate.\nindeterminates.append((i, col_val))\n\nif len(indeterminates) == 0:\n# All candidate pivots are guaranteed to be zero, i.e. there is\n# no pivot.\nreturn None, None, False, []\n\nif simpfunc is None:\n# Caller did not pass in a simplification function that might\n# determine if an indeterminate pivot candidate is guaranteed\n# to be nonzero, so assume the first indeterminate candidate\n# is non-zero.\nreturn indeterminates[0][0], indeterminates[0][1], True, []\n\n# newly_determined holds index-value pairs of candidate pivots\n# that were simplified during the search for a non-zero pivot.\nnewly_determined = []\nfor i, col_val in indeterminates:\ntmp_col_val = simpfunc(col_val)\nif id(col_val) != id(tmp_col_val):\n# simpfunc() simplified this candidate pivot.\nnewly_determined.append((i, tmp_col_val))\nif iszerofunc(tmp_col_val) == False:\n# Candidate pivot simplified to a guaranteed non-zero value.\nreturn i, tmp_col_val, False, newly_determined\n\nreturn indeterminates[0][0], indeterminates[0][1], True, newly_determined\n"
    },
    "ground_truth_patch": {
        "sympy/matrices/matrices.py": " from sympy.core.add import Add\n from sympy.core.basic import Basic, Atom\n from sympy.core.expr import Expr\nfrom sympy.core.function import expand_mul\n from sympy.core.power import Pow\n from sympy.core.symbol import (Symbol, Dummy, symbols,\n     _uniquely_named_symbol)\n \n from sympy.utilities.iterables import flatten, numbered_symbols\n from sympy.core.decorators import call_highest_priority\nfrom sympy.core.compatibility import (is_sequence, default_sort_key, range,\nNotIterable)\n \n \n from types import FunctionType\n         return None\n \n \ndef _is_zero_after_expand_mul(x):\n\"\"\"Tests by expand_mul only, suitable for polynomials and rational\nfunctions.\"\"\"\nreturn expand_mul(x) == 0\n\n\n class DeferredVector(Symbol, NotIterable):\n     \"\"\"A vector whose components are deferred (e.g. for use with lambdify)\n \n         http://www.eecis.udel.edu/~saunders/papers/sffge/it5.ps.\n         \"\"\"\n \n         # Recursively implemented Bareiss' algorithm as per Deanna Richelle Leggett's\n         # thesis http://www.math.usm.edu/perry/Research/Thesis_DRL.pdf\n         def bareiss(mat, cumm=1):\n                 return mat[0, 0]\n \n             # find a pivot and extract the remaining matrix\n# With the default iszerofunc, _find_reasonable_pivot slows down\n# the computation by the factor of 2.5 in one test.\n# Relevant issues: #10279 and #13877.\npivot_pos, pivot_val, _, _ = _find_reasonable_pivot(mat[:, 0],\niszerofunc=_is_zero_after_expand_mul)\n             if pivot_pos == None:\n                 return S.Zero\n \n",
        "sympy/utilities/randtest.py": " from sympy.core.compatibility import is_sequence, as_int\n \n \ndef random_complex_number(a=2, b=-1, c=3, d=1, rational=False, tolerance=None):\n     \"\"\"\n     Return a random complex number.\n \n     To reduce chance of hitting branch cuts or anything, we guarantee\n     b <= Im z <= d, a <= Re z <= c\n\nWhen rational is True, a rational approximation to a random number\nis obtained within specified tolerance, if any.\n     \"\"\"\n     A, B = uniform(a, c), uniform(b, d)\n     if not rational:\n         return A + I*B\nreturn (nsimplify(A, rational=True, tolerance=tolerance) +\nI*nsimplify(B, rational=True, tolerance=tolerance))\n \n \n def verify_numerically(f, g, z=None, tol=1.0e-6, a=2, b=-1, c=3, d=1):\n"
    },
    "response": "{\"is_clone\": true, \"classification\": \"type-3\", \"reasoning\": \"These code samples are type-3 clones because they share a similar structure and logic, but contain modifications such as changes in function implementation, added comments, and variations in how certain elements are handled or defined.\"}"
}