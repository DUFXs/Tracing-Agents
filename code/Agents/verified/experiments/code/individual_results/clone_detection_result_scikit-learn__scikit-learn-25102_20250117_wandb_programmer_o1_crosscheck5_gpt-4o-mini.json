{
    "instance_id": "scikit-learn__scikit-learn-25102",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/utils/_set_output.py": "index 335773c6a..662c03d58 100644\n     *,\n     columns,\n     index=None,\ndtypes=None,\n ):\n     \"\"\"Create a Pandas DataFrame.\n \n         Data to be wrapped as pandas dataframe.\n \n     columns : callable, ndarray, or None\nThe column names or a callable that returns the column names.\n \n     index : array-like, default=None\n         Index for data.\n \ndtypes : dict or None, default=None\nIf not None, a mapping from column name to original dtype, used to\nrecast columns in the newly created DataFrame (best effort).\n     \"\"\"\nfrom scipy.sparse import issparse\n     if issparse(data_to_wrap):\n         raise ValueError(\"Pandas output does not support sparse data.\")\n \n             data_to_wrap.index = index\n         return data_to_wrap\n \ndf = pd.DataFrame(data_to_wrap, index=index, columns=columns)\n\nif dtypes is not None:\nfor col, original_dtype in dtypes.items():\nif col in df.columns:\ntry:\ndf[col] = df[col].astype(original_dtype)\nexcept Exception:\n# If casting is invalid, we skip\npass\n\nreturn df\n\n \n \n def _get_output_config(method, estimator=None):\n         If the output config is \"default\" or the estimator is not configured\n         for wrapping return `data_to_wrap` unchanged.\n         If the output config is \"pandas\", return `data_to_wrap` as a pandas\nDataFrame, attempting to preserve original dtypes if available.\n     \"\"\"\n     output_config = _get_output_config(method, estimator)\n \n         return data_to_wrap\n \n     # dense_config == \"pandas\"\n# We'll preserve dtypes if the original input is a dataframe\ndtypes = None\nif hasattr(original_input, \"dtypes\"):\ndtypes = original_input.dtypes.to_dict()\n\n     return _wrap_in_pandas_container(\n         data_to_wrap=data_to_wrap,\n         index=getattr(original_input, \"index\", None),\n         columns=estimator.get_feature_names_out,\ndtypes=dtypes,\n     )\n \n \n\n def _wrap_method_output(f, method):\n     \"\"\"Wrapper used by `_SetOutputMixin` to automatically wrap methods.\"\"\"\n \n",
        "test_preserve_dtypes.py": "new file mode 100644\nindex 000000000..45ede4b47\nimport sys\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest, chi2\n\ndef main():\nprint(\"Step 1: Loading iris data as DataFrame with float16 and category columns.\")\nX, y = load_iris(return_X_y=True, as_frame=True)\nX = X.astype({\"petal width (cm)\": np.float16,\n\"petal length (cm)\": np.float16})\nX[\"cat\"] = y.astype(\"category\")\n\nprint(\"Step 2: Using SelectKBest(chi2, k=2). We want to preserve float16 and category.\")\nselector = SelectKBest(chi2, k=2)\n\n# We'll attempt to enable 'pandas' output (the user wants to preserve dtypes).\n# We have not yet implemented any special dtype-preserving logic, so on HEAD it won't work.\n# Once we fix the code, it should preserve float16 and 'category'.\nselector.set_output(transform=\"pandas\")\nX_out = selector.fit_transform(X, y)\n\n# The user sees float64 for both columns. We'll check if 'petal length (cm)' is float16\n# and the 'cat' column is category. If it's not, we fail.\ndtypes_out = X_out.dtypes\nprint(\"Step 3: dtypes of the output DataFrame are:\")\nprint(dtypes_out)\n\n# The user wants petal length to remain float16, cat to remain category\ndesired_dtypes = {\n\"petal length (cm)\": np.dtype(\"float16\"),\n\"cat\": pd.CategoricalDtype()\n}\n\nokay = True\nfor col, desired in desired_dtypes.items():\nif col not in dtypes_out:\nprint(f\"FAIL: Column {col} not found in output.\")\nokay = False\nelse:\n# Check if category is preserved\nif isinstance(desired, pd.CategoricalDtype):\nif not pd.api.types.is_categorical_dtype(X_out[col].dtype):\nprint(f\"FAIL: Column {col} is not category, got {X_out[col].dtype}.\")\nokay = False\nelse:\nprint(f\"PASS: Column {col} is category.\")\nelse:\nif X_out[col].dtype != desired:\nprint(f\"FAIL: Column {col} dtype is {X_out[col].dtype}, expected {desired}.\")\nokay = False\nelse:\nprint(f\"PASS: Column {col} is float16.\")\n\nif okay:\nprint(\"All tests passed, so apparently dtypes are preserved.\")\nsys.exit(0)\nelse:\nprint(\"Dtype preservation test failed.\")\nsys.exit(1)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/base.py": "         y=\"no_validation\",\n         reset=True,\n         validate_separately=False,\ncast_to_ndarray=True,\n         **check_params,\n     ):\n         \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n             `estimator=self` is automatically added to these dicts to generate\n             more informative error message in case of invalid input data.\n \ncast_to_ndarray : bool, default=True\nCast `X` and `y` to ndarray with checks in `check_params`. If\n`False`, `X` and `y` are unchanged and only `feature_names` and\n`n_features_in_` are checked.\n\n         **check_params : kwargs\n             Parameters passed to :func:`sklearn.utils.check_array` or\n             :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n         if no_val_X and no_val_y:\n             raise ValueError(\"Validation should be done on X, y or both.\")\n         elif not no_val_X and no_val_y:\nif cast_to_ndarray:\nX = check_array(X, input_name=\"X\", **check_params)\n             out = X\n         elif no_val_X and not no_val_y:\nif cast_to_ndarray:\ny = _check_y(y, **check_params) if cast_to_ndarray else y\n             out = y\n         else:\nif validate_separately and cast_to_ndarray:\n                 # We need this because some estimators validate X and y\n                 # separately, and in general, separately calling check_array()\n                 # on X and y isn't equivalent to just calling check_X_y()\n",
        "sklearn/feature_selection/_base.py": " from ..cross_decomposition._pls import _PLS\n from ..utils import (\n     check_array,\n     safe_sqr,\n )\n from ..utils._tags import _safe_tags\nfrom ..utils import _safe_indexing\nfrom ..utils._set_output import _get_output_config\n from ..utils.validation import _check_feature_names_in, check_is_fitted\n \n \n         X_r : array of shape [n_samples, n_selected_features]\n             The input samples with only the selected features.\n         \"\"\"\n# Preserve X when X is a dataframe and the output is configured to\n# be pandas.\noutput_config_dense = _get_output_config(\"transform\", estimator=self)[\"dense\"]\npreserve_X = hasattr(X, \"iloc\") and output_config_dense == \"pandas\"\n\n         # note: we use _safe_tags instead of _get_tags because this is a\n         # public Mixin.\n         X = self._validate_data(\n             dtype=None,\n             accept_sparse=\"csr\",\n             force_all_finite=not _safe_tags(self, key=\"allow_nan\"),\ncast_to_ndarray=not preserve_X,\n             reset=False,\n         )\n         return self._transform(X)\n                 \" too noisy or the selection test too strict.\",\n                 UserWarning,\n             )\nif hasattr(X, \"iloc\"):\nreturn X.iloc[:, :0]\n             return np.empty(0, dtype=X.dtype).reshape((X.shape[0], 0))\nreturn _safe_indexing(X, mask, axis=1)\n \n     def inverse_transform(self, X):\n         \"\"\"Reverse the transformation operation.\n"
    },
    "response": "{\"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they serve different purposes and contain distinct functionalities. Code sample 1 focuses on wrapping data into a pandas DataFrame and preserving data types, while code sample 2 is about validating inputs and handling transformations with feature selection. The concepts, structures, and implementations differ significantly, making them not clones.\"}"
}