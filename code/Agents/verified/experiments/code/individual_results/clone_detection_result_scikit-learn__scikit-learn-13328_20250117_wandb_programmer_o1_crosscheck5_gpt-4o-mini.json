{
    "instance_id": "scikit-learn__scikit-learn-13328",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/linear_model/huber.py": "index 65c686400..2e7bf2015 100644\n \n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit the model according to the given training data.\n\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             Training vector, where n_samples in the number of samples and\n             n_features is the number of features.\n\n         y : array-like, shape (n_samples,)\n             Target vector relative to X.\n\n         sample_weight : array-like, shape (n_samples,)\n             Weight given to each sample.\n\n         Returns\n         -------\n         self : object\n         \"\"\"\n         X, y = check_X_y(\nX, y, copy=False, accept_sparse=['csr'], y_numeric=True, dtype=np.float64\n)\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\n         else:\n             sample_weight = np.ones_like(y)\n\n         if self.epsilon < 1.0:\n             raise ValueError(\n                 \"epsilon should be greater than or equal to 1.0, got %f\"\n                 % self.epsilon)\n\n         if self.warm_start and hasattr(self, 'coef_'):\n             parameters = np.concatenate(\n                 (self.coef_, [self.intercept_, self.scale_]))\n             # Make sure to initialize the scale parameter to a strictly\n             # positive value:\n             parameters[-1] = 1\n\n         # Sigma or the scale factor should be non-negative.\n         # Setting it to be zero might cause undefined bounds hence we set it\n         # to a value close to zero.\n         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n         bounds[-1][0] = np.finfo(np.float64).eps * 10\n\n         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n             _huber_loss_and_gradient, parameters,\n             args=(X, y, self.epsilon, self.alpha, sample_weight),\n         else:\n             self.intercept_ = 0.0\n         self.coef_ = parameters[:X.shape[1]]\n\n         residual = np.abs(\n             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n         self.outliers_ = residual > self.scale_ * self.epsilon\n         return self\n\n",
        "test_huber_bool_repro.py": "new file mode 100644\nindex 000000000..159016841\nimport sys\nimport numpy as np\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.datasets import make_regression\n\ndef main():\nprint(\"Running test_huber_bool_repro...\")\nprint(\"Generating data...\")\nX, y, coef = make_regression(n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\nX_bool = X > 0\n\nprint(\"Fitting HuberRegressor on boolean data. Expecting a failure on HEAD if bug is present.\")\ntry:\nhuber = HuberRegressor().fit(X_bool, y)\nexcept TypeError as e:\nprint(\"Test script caught TypeError, bug is present:\", str(e))\nsys.exit(1)\nexcept Exception as e:\nprint(\"Test script caught an unexpected error:\", str(e))\nsys.exit(2)\nelse:\nprint(\"No error occurred! The bug is fixed or not present.\")\nsys.exit(0)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/linear_model/huber.py": "         self : object\n         \"\"\"\n         X, y = check_X_y(\nX, y, copy=False, accept_sparse=['csr'], y_numeric=True,\ndtype=[np.float64, np.float32])\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\n"
    },
    "response": "{\"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they are both parts of different files with unrelated functionality. The first sample involves fitting a model with various parameters and handling weights and errors, while the second sample focuses on checking the consistency of the training data with a specific type setting. They do not share structural similarity that would categorize them as clones.\"}"
}