{
    "instance_id": "scikit-learn__scikit-learn-25973",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/feature_selection/_sequential.py": "index e983c55de..a9a9c2d79 100644\n \n     def fit(self, X, y=None):\n         \"\"\"Learn the features to select from X.\n\n         Parameters\n         ----------\n         X : array-like of shape (n_samples, n_features)\nTraining vectors, where n_samples is the number of samples and\nn_features is the number of predictors.\n\n         y : array-like of shape (n_samples,), default=None\n             Target values. This parameter may be ignored for\n             unsupervised learning.\n\n         Returns\n         -------\n         self : object\n             Returns the instance itself.\n         \"\"\"\n         self._validate_params()\n\n# convert cv generator to a list if necessary\nif hasattr(self.cv, '__iter__') and not hasattr(self.cv, '__len__'):\nself.cv = list(self.cv)\n\n         # FIXME: to be removed in 1.3\n         if self.n_features_to_select in (\"warn\", None):\n             # for backwards compatibility\nimport warnings\n             warnings.warn(\n                 \"Leaving `n_features_to_select` to \"\n                 \"None is deprecated in 1.0 and will become 'auto' \"\n                 \"when creating an instance.\",\n                 FutureWarning,\n             )\n\n         tags = self._get_tags()\n         X = self._validate_data(\n             X,\naccept_sparse='csc',\n             ensure_min_features=2,\nforce_all_finite=not tags.get('allow_nan', True),\n         )\n         n_features = X.shape[1]\n\n         # FIXME: to be fixed in 1.3\n         error_msg = (\n             \"n_features_to_select must be either 'auto', 'warn', \"\n                 self.n_features_to_select_ = n_features - 1\n             else:\n                 self.n_features_to_select_ = n_features // 2\nelif isinstance(self.n_features_to_select, int):\n             if not 0 < self.n_features_to_select < n_features:\n                 raise ValueError(error_msg)\n             self.n_features_to_select_ = self.n_features_to_select\nelse:\nfrom numbers import Real\nif isinstance(self.n_features_to_select, Real):\nself.n_features_to_select_ = int(n_features * self.n_features_to_select)\n\n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n\nfrom sklearn.base import clone\n         cloned_estimator = clone(self.estimator)\n\n         # the current mask corresponds to the set of features:\n         # - that we have already *selected* if we do forward selection\n         # - that we have already *excluded* if we do backward selection\nimport numpy as np\n         current_mask = np.zeros(shape=n_features, dtype=bool)\n         n_iterations = (\n             self.n_features_to_select_\n             if self.n_features_to_select == \"auto\" or self.direction == \"forward\"\n             else n_features - self.n_features_to_select_\n         )\n\nold_score = -float('inf')\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n\n             old_score = new_score\n             current_mask[new_feature_idx] = True\n\n         if self.direction == \"backward\":\n             current_mask = ~current_mask\n\n         self.support_ = current_mask\n         self.n_features_to_select_ = self.support_.sum()\n\n         return self\n\n \n     def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n         # Return the best new feature and its score to add to the current_mask,\n",
        "test_sequential_cv_iter.py": "new file mode 100644\nindex 000000000..9fc04e960\nimport sys\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ndef main():\nprint(\"Reproducing bug with SequentialFeatureSelector + custom cv splits\")\n\n# Prepare data\nX, y = make_classification()\ngroups = np.zeros_like(y, dtype=int)\ngroups[y.size // 2:] = 1\n\n# Construct an iterable of splits\ncv = LeaveOneGroupOut()\nsplits = cv.split(X, y, groups=groups)\n\nclf = KNeighborsClassifier(n_neighbors=5)\nseq = SequentialFeatureSelector(clf, n_features_to_select=5, scoring='accuracy', cv=splits)\n\n# Attempt to fit\ntry:\nseq.fit(X, y)\nprint(\"It worked! No IndexError encountered.\")\nsys.exit(0)\nexcept Exception as e:\nprint(\"Encountered exception:\", str(e))\nsys.exit(1)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/feature_selection/_sequential.py": " import warnings\n \n from ._base import SelectorMixin\nfrom ..base import BaseEstimator, MetaEstimatorMixin, clone, is_classifier\n from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n from ..utils._param_validation import RealNotInt\n from ..utils._tags import _safe_tags\n from ..utils.validation import check_is_fitted\nfrom ..model_selection import cross_val_score, check_cv\n from ..metrics import get_scorer_names\n \n \n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n \ncv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n\n         cloned_estimator = clone(self.estimator)\n \n         # the current mask corresponds to the set of features:\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\ncloned_estimator, X, y, cv, current_mask\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n \n         return self\n \ndef _get_best_new_feature_score(self, estimator, X, y, cv, current_mask):\n         # Return the best new feature and its score to add to the current_mask,\n         # i.e. return the best new feature and its score to add (resp. remove)\n         # when doing forward selection (resp. backward selection).\n                 estimator,\n                 X_new,\n                 y,\ncv=cv,\n                 scoring=self.scoring,\n                 n_jobs=self.n_jobs,\n             ).mean()\n"
    },
    "response": "{\"is_clone\": true, \"classification\": \"type-2\", \"reasoning\": \"These code samples are type-2 clones because they share similar logic and structural designs in terms of feature selection, but they differ in identifier names, details in function parameters, and the overall method of implementation. The essence of the functions and their operations remains the same, with variations in variable names and some changes in the arrangement of certain lines and logic.\"}"
}