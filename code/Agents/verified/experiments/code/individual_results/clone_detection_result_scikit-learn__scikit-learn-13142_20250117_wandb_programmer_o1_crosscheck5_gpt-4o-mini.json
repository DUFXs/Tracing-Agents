{
    "instance_id": "scikit-learn__scikit-learn-13142",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/mixture/base.py": "index bd34333c0..dd36bdb3d 100644\n \n     def fit_predict(self, X, y=None):\n         \"\"\"Estimate model parameters using X and predict the labels for X.\n\n         The method fits the model n_init times and sets the parameters with\n         which the model has the largest likelihood or lower bound. Within each\n         trial, the method iterates between E-step and M-step for `max_iter`\n         times until the change of likelihood or lower bound is less than\n         `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n         predicts the most probable label for the input data points.\n\n         .. versionadded:: 0.20\n\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row\n             corresponds to a single data point.\n\n         Returns\n         -------\n         labels : array, shape (n_samples,)\n         \"\"\"\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n\n         # if we enable warm_start, we will have a unique initialisation\ndo_init = not (self.warm_start and hasattr(self, 'converged_'))\n         n_init = self.n_init if do_init else 1\n\nmax_lower_bound = -float('inf')\n         self.converged_ = False\n\n         random_state = check_random_state(self.random_state)\n\n         n_samples, _ = X.shape\n         for init in range(n_init):\n             self._print_verbose_msg_init_beg(init)\n\n             if do_init:\n                 self._initialize_parameters(X, random_state)\n\nlower_bound = -float('inf') if do_init else self.lower_bound_\n\n             for n_iter in range(1, self.max_iter + 1):\n                 prev_lower_bound = lower_bound\n\n                 log_prob_norm, log_resp = self._e_step(X)\n                 self._m_step(X, log_resp)\n                 lower_bound = self._compute_lower_bound(\n                     log_resp, log_prob_norm)\n\n                 change = lower_bound - prev_lower_bound\n                 self._print_verbose_msg_iter_end(n_iter, change)\n\n                 if abs(change) < self.tol:\n                     self.converged_ = True\n                     break\n\n             self._print_verbose_msg_init_end(lower_bound)\n\n             if lower_bound > max_lower_bound:\n                 max_lower_bound = lower_bound\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n\n         if not self.converged_:\nwarnings.warn(\n'Initialization %d did not converge. '\n'Try different init parameters, '\n'or increase max_iter, tol '\n'or check for degenerate data.'\n% (init + 1), ConvergenceWarning)\n\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n\n# Always do a final e-step to guarantee that the labels returned by\n# fit_predict(X) are always consistent with fit(X).predict(X).\n_, log_resp = self._e_step(X)\n\n         return log_resp.argmax(axis=1)\n\n \n     def _e_step(self, X):\n         \"\"\"E step.\n",
        "test_n_init_gaussian_mixture.py": "new file mode 100644\nindex 000000000..67f18531b\nimport sys\nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\nfrom numpy.testing import assert_array_equal\n\ndef main():\nprint(\"Reproduction test for n_init discrepancy in GaussianMixture...\")\nrng = np.random.RandomState(42)\nX = rng.randn(1000, 5)\n\nprint(\"Case 1: n_init=1\")\ngm1 = GaussianMixture(n_components=5, n_init=1, random_state=42)\nc1_fitpred = gm1.fit_predict(X)\nc1_pred = gm1.predict(X)\nprint(\"Labels from fit_predict:\", c1_fitpred)\nprint(\"Labels from predict:    \", c1_pred)\ntry:\nassert_array_equal(c1_fitpred, c1_pred)\nprint(\"PASS: n_init=1 was consistent.\")\nexcept AssertionError as e:\nprint(\"FAIL: n_init=1 was inconsistent.\")\nprint(e)\nsys.exit(1)\n\nprint()\nprint(\"Case 2: n_init=5\")\ngm2 = GaussianMixture(n_components=5, n_init=5, random_state=42)\nc2_fitpred = gm2.fit_predict(X)\nc2_pred = gm2.predict(X)\nprint(\"Labels from fit_predict:\", c2_fitpred)\nprint(\"Labels from predict:    \", c2_pred)\ntry:\nassert_array_equal(c2_fitpred, c2_pred)\nprint(\"PASS: n_init=5 was consistent.\")\nexcept AssertionError as e:\nprint(\"FAIL: n_init=5 was inconsistent.\")\nprint(e)\nprint(\"Test script exiting with error.\")\nsys.exit(1)\n\nprint()\nprint(\"Both n_init=1 and n_init=5 gave consistent fit_predict/predict labels. Test script passing.\")\nsys.exit(0)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/mixture/base.py": "                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n# Always do a final e-step to guarantee that the labels returned by\n# fit_predict(X) are always consistent with fit(X).predict(X)\n# for any value of max_iter and tol (and any random_state).\n_, log_resp = self._e_step(X)\n\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n"
    },
    "response": "{\"is_clone\": true, \"classification\": \"type-1\", \"reasoning\": \"These code samples are type-1 clones because they are identical except for differences in whitespace and layout. The code logic and functionality are unchanged, only the formatting and indentation have slight variations.\"}"
}