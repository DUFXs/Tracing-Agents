{
    "instance_id": "scikit-learn__scikit-learn-9288",
    "agent": "20241221_codestory_midwit_claude-3-5-sonnet_swe-search",
    "agent_patch": {
        "reproduce_error.py": "new file mode 100644\nindex 000000000..d685c1c84\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# For reproducibility\nnp.random.seed(42)\n\n# Generate some data\nX, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)\n\n# Run KMeans with various n_jobs values\nfor n_jobs in range(1, 5):\nkmeans = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)\nkmeans.fit(X)\nprint(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')\n\\ No newline at end of file\n",
        "sklearn/cluster/_k_means.pyx": "index 8a66f2506..2c62d72fb 100644\n     cdef int i, j, c\n     cdef np.ndarray[floating, ndim=2] centers\n     cdef np.ndarray[floating, ndim=1] weight_in_cluster\ncdef np.ndarray[floating, ndim=2] weighted_sums\n \n     dtype = np.float32 if floating is float else np.float64\n     centers = np.zeros((n_clusters, n_features), dtype=dtype)\n     weight_in_cluster = np.zeros((n_clusters,), dtype=dtype)\nweighted_sums = np.zeros((n_clusters, n_features), dtype=dtype)\n \n# First compute cluster weights\n     for i in range(n_samples):\n         c = labels[i]\n         weight_in_cluster[c] += sample_weight[i]\n\n     empty_clusters = np.where(weight_in_cluster == 0)[0]\n     # maybe also relocate small clusters?\n \n         for i, cluster_id in enumerate(empty_clusters):\n             # XXX two relocated clusters could be close to each other\n             far_index = far_from_centers[i]\n# Initialize empty cluster with its furthest point\nfor j in range(n_features):\nweighted_sums[cluster_id, j] = X[far_index, j] * sample_weight[far_index]\n             weight_in_cluster[cluster_id] = sample_weight[far_index]\n \n# Compute weighted sums for non-empty clusters\n     for i in range(n_samples):\nc = labels[i]\nif weight_in_cluster[c] > 0:  # Only process non-empty clusters\nfor j in range(n_features):\nweighted_sums[c, j] += X[i, j] * sample_weight[i]\n\n# Normalize to get centers\nfor c in range(n_clusters):\nif weight_in_cluster[c] > 0:\nfor j in range(n_features):\ncenters[c, j] = weighted_sums[c, j] / weight_in_cluster[c]\n \n     return centers\n \n",
        "sklearn/cluster/k_means_.py": "index b7fbdf7da..ae5c54382 100644\n     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n# Generate seeds for both sequential and parallel cases\nseeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\nfor it, seed in enumerate(seeds):\n# run a k-means once with consistent seed\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\nrandom_state=seed)  # Use same seed sequence as parallel case\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\n                                    verbose=verbose, tol=tol,\n                                    precompute_distances=precompute_distances,\n                                    x_squared_norms=x_squared_norms,\nrandom_state=seed)  # Use pre-generated seeds\n             for seed in seeds)\n         # Get results with the lowest inertia\n         labels, inertia, centers, n_iters = zip(*results)\n",
        "test_kmeans_extended.py": "new file mode 100644\nindex 000000000..8bd89c232\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# Test with different random states and data sizes\nfor random_state in [2, 42, 123]:\nfor n_samples in [1000, 10000]:\nprint(f\"\\nTesting with random_state={random_state}, n_samples={n_samples}\")\n# Generate some data\nX, y = make_blobs(n_samples=n_samples, centers=10, n_features=2, random_state=random_state)\n\n# Run KMeans with various n_jobs values\nfor n_jobs in range(1, 5):\nkmeans = KMeans(n_clusters=10, random_state=random_state, n_jobs=n_jobs)\nkmeans.fit(X)\nprint(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')\n\\ No newline at end of file\n"
    },
    "ground_truth_patch": {
        "sklearn/cluster/k_means_.py": "     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n\nseeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\nfor seed in seeds:\n             # run a k-means once\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\nrandom_state=seed)\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\n"
    },
    "response": "{\"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they do not share any significant syntactical structure or logic. They are from different files and contain entirely distinct logic related to KMeans clustering. Code sample 1 is a script for running KMeans with varying `n_jobs` and testing different random states, while code sample 2 is part of a nested implementation detail for managing parallel executions within the KMeans function. Therefore, they are substantially different in their intent and execution.\"}"
}