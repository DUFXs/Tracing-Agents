{
    "instance_id": "scikit-learn__scikit-learn-13779",
    "agent": "20241221_codestory_midwit_claude-3-5-sonnet_swe-search",
    "agent_patch": {
        "edge_cases_test.py": "new file mode 100644\nindex 000000000..3943453d8\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nimport numpy as np\n\n# Load data\nX, y = load_iris(return_X_y=True)\nsample_weight = np.ones(y.shape)\n\n# Test case 1: Mix of None and non-None estimators with sample weights\nvoter = VotingClassifier(\nestimators=[('lr', None),\n('rf', RandomForestClassifier())]\n)\nprint(\"Test 1: Mix of None and non-None estimators\")\nvoter.fit(X, y, sample_weight=sample_weight)\nvoter.predict(X)\nprint(\"Passed!\")\n\n# Test case 2: Mix of None and non-None estimators with weights parameter\nvoter = VotingClassifier(\nestimators=[('lr', None),\n('rf', RandomForestClassifier())],\nweights=[1, 2]\n)\nprint(\"\\nTest 2: Mix of None and non-None estimators with weights\")\nvoter.fit(X, y, sample_weight=sample_weight)\nvoter.predict(X)\nprint(\"Passed!\")\n\n# Test case 3: Try to set all estimators to None (should raise ValueError)\nvoter = VotingClassifier(\nestimators=[('lr', LogisticRegression()),\n('rf', RandomForestClassifier())]\n)\ntry:\nvoter.set_params(lr=None, rf=None)\nvoter.fit(X, y, sample_weight=sample_weight)\nraise Exception(\"Should have raised ValueError for all None estimators\")\nexcept ValueError as e:\nif \"All estimators are None\" in str(e):\nprint(\"\\nTest 3: All None estimators correctly raises ValueError\")\nprint(\"Passed!\")\nelse:\nraise\n\\ No newline at end of file\n",
        "reproduce_error.py": "new file mode 100644\nindex 000000000..880a8563f\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nimport numpy as np\n\nX, y = load_iris(return_X_y=True)\nvoter = VotingClassifier(\nestimators=[('lr', LogisticRegression()),\n('rf', RandomForestClassifier())]\n)\nvoter.fit(X, y, sample_weight=np.ones(y.shape))\nvoter.set_params(lr=None)\nvoter.fit(X, y, sample_weight=np.ones(y.shape))\n\\ No newline at end of file\n",
        "sklearn/ensemble/voting.py": "index 7afa7180c..13accdcec 100644\n \n         if sample_weight is not None:\n             for name, step in self.estimators:\nif step is not None and not has_fit_parameter(step, 'sample_weight'):\n                     raise ValueError('Underlying estimator \\'%s\\' does not'\n                                      ' support sample weights.' % name)\n \n"
    },
    "ground_truth_patch": {
        "sklearn/ensemble/voting.py": " \n         if sample_weight is not None:\n             for name, step in self.estimators:\nif step is None:\ncontinue\n                 if not has_fit_parameter(step, 'sample_weight'):\n                     raise ValueError('Underlying estimator \\'%s\\' does not'\n                                      ' support sample weights.' % name)\n"
    },
    "response": "{\"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they belong to different contexts and serve different purposes. Code sample 1 contains a set of three test cases utilizing the VotingClassifier to evaluate scenarios involving None values as estimators, while code sample 2 presents a portion of the implementation of the VotingClassifier that deals with sample weights. The first code sample has a testing structure and multiple estimators, whereas the second sample is part of the VotingClassifier's source code, which handles error checking. Therefore, they are not clones of each other.\"}"
}