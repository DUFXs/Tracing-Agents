{
    "instance_id": "scikit-learn__scikit-learn-13328",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/linear_model/huber.py": "index 65c686400..2e7bf2015 100644\n \n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit the model according to the given training data.\n\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             Training vector, where n_samples in the number of samples and\n             n_features is the number of features.\n\n         y : array-like, shape (n_samples,)\n             Target vector relative to X.\n\n         sample_weight : array-like, shape (n_samples,)\n             Weight given to each sample.\n\n         Returns\n         -------\n         self : object\n         \"\"\"\n         X, y = check_X_y(\nX, y, copy=False, accept_sparse=['csr'], y_numeric=True, dtype=np.float64\n)\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\n         else:\n             sample_weight = np.ones_like(y)\n\n         if self.epsilon < 1.0:\n             raise ValueError(\n                 \"epsilon should be greater than or equal to 1.0, got %f\"\n                 % self.epsilon)\n\n         if self.warm_start and hasattr(self, 'coef_'):\n             parameters = np.concatenate(\n                 (self.coef_, [self.intercept_, self.scale_]))\n             # Make sure to initialize the scale parameter to a strictly\n             # positive value:\n             parameters[-1] = 1\n\n         # Sigma or the scale factor should be non-negative.\n         # Setting it to be zero might cause undefined bounds hence we set it\n         # to a value close to zero.\n         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n         bounds[-1][0] = np.finfo(np.float64).eps * 10\n\n         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n             _huber_loss_and_gradient, parameters,\n             args=(X, y, self.epsilon, self.alpha, sample_weight),\n         else:\n             self.intercept_ = 0.0\n         self.coef_ = parameters[:X.shape[1]]\n\n         residual = np.abs(\n             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n         self.outliers_ = residual > self.scale_ * self.epsilon\n         return self\n\n",
        "test_huber_bool_repro.py": "new file mode 100644\nindex 000000000..159016841\nimport sys\nimport numpy as np\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.datasets import make_regression\n\ndef main():\nprint(\"Running test_huber_bool_repro...\")\nprint(\"Generating data...\")\nX, y, coef = make_regression(n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\nX_bool = X > 0\n\nprint(\"Fitting HuberRegressor on boolean data. Expecting a failure on HEAD if bug is present.\")\ntry:\nhuber = HuberRegressor().fit(X_bool, y)\nexcept TypeError as e:\nprint(\"Test script caught TypeError, bug is present:\", str(e))\nsys.exit(1)\nexcept Exception as e:\nprint(\"Test script caught an unexpected error:\", str(e))\nsys.exit(2)\nelse:\nprint(\"No error occurred! The bug is fixed or not present.\")\nsys.exit(0)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/linear_model/huber.py": "         self : object\n         \"\"\"\n         X, y = check_X_y(\nX, y, copy=False, accept_sparse=['csr'], y_numeric=True,\ndtype=[np.float64, np.float32])\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 implements the `fit` method of a Huber regression model, which involves several operations like checking the input data, raising an error if conditions are not met, and optimizing parameters using a convex optimization method. It handles sample weights, checks the consistency of data lengths, and prepares data bounds for optimization. The method ultimately returns the model object after fitting. Meanwhile, the code in code sample 2 extracts only the documentation string and the beginning part of the `fit` method and contains a modified call to `check_X_y` with an additional dtype argument. This is a partial implementation from code sample 1 with differences in implementation details and specifications of the data types.\", \"is_clone\": true, \"classification\": \"type-2\", \"reasoning\": \"These code samples are type-2 clones because they are structurally similar but differ in identifier names and literal values (in the dtype argument for the `check_X_y` function), as well as whitespace and layout.\"}"
}