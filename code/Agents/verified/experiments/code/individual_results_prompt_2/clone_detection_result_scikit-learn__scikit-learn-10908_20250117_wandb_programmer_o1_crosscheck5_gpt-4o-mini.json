{
    "instance_id": "scikit-learn__scikit-learn-10908",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/feature_extraction/text.py": "index 9b1ebd632..bb3621c03 100644\n                         raise ValueError(msg)\n             if not vocabulary:\n                 raise ValueError(\"empty vocabulary passed to fit\")\n             self.vocabulary_ = dict(vocabulary)\n         else:\npass\n\n \n     def _check_vocabulary(self):\n         \"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\n         self.min_df = min_df\n         if max_df < 0 or min_df < 0:\n             raise ValueError(\"negative value for max_df or min_df\")\n\nimport numbers\n         self.max_features = max_features\n         if max_features is not None:\n             if (not isinstance(max_features, numbers.Integral) or\n                 raise ValueError(\n                     \"max_features=%r, neither a positive integer nor None\"\n                     % max_features)\n\n         self.ngram_range = ngram_range\n         self.vocabulary = vocabulary\n         self.binary = binary\n         self.dtype = dtype\n\n# If a user vocabulary is given, decide whether to validate immediately.\n# Dict or set => validate now (so that transform works without fitting).\n# List/tuple/other => skip immediate validation (so duplicates cause error at fit).\nif vocabulary is not None:\nfrom collections import Mapping\nif isinstance(vocabulary, Mapping) or isinstance(vocabulary, set):\nself._validate_vocabulary()\nself.fixed_vocabulary_ = True\nelse:\nself.fixed_vocabulary_ = False\n\n\n\n\n\n\n\n\n\n \n     def _sort_features(self, X, vocabulary):\n         \"\"\"Sort features by name\n     def _count_vocab(self, raw_documents, fixed_vocab):\n         \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n         \"\"\"\n# If we're using a fixed vocabulary but haven't validated it yet\n# (e.g. user passed a list/tuple), do so now. This also sets self.vocabulary_.\nif fixed_vocab and not hasattr(self, 'vocabulary_'):\nself._validate_vocabulary()\n\n         if fixed_vocab:\n             vocabulary = self.vocabulary_\n         else:\n             # Add a new value when a new vocabulary item is seen\nfrom collections import defaultdict\n             vocabulary = defaultdict()\n             vocabulary.default_factory = vocabulary.__len__\n\n         analyze = self.build_analyzer()\n         j_indices = []\n         indptr = []\n\n         values = _make_int_array()\n         indptr.append(0)\n         for doc in raw_documents:\n                 except KeyError:\n                     # Ignore out-of-vocabulary items for fixed_vocab=True\n                     continue\n\n             j_indices.extend(feature_counter.keys())\n             values.extend(feature_counter.values())\n             indptr.append(len(j_indices))\n\n         if not fixed_vocab:\n             # disable defaultdict behaviour\n             vocabulary = dict(vocabulary)\n             if not vocabulary:\nraise ValueError(\"empty vocabulary; perhaps the documents only contain stop words\")\n\nimport scipy.sparse as sp\nimport numpy as np\nfrom ..utils.fixes import sp_version\n         if indptr[-1] > 2147483648:  # = 2**31 - 1\n             if sp_version >= (0, 14):\n                 indices_dtype = np.int64\n                                   ' which is unsupported with scipy {}. '\n                                   'Please upgrade to scipy >=0.14')\n                                  .format(indptr[-1], '.'.join(sp_version)))\n         else:\n             indices_dtype = np.int32\n\n         j_indices = np.asarray(j_indices, dtype=indices_dtype)\n         indptr = np.asarray(indptr, dtype=indices_dtype)\n         values = np.frombuffer(values, dtype=np.intc)\n\n         X = sp.csr_matrix((values, j_indices, indptr),\n                           shape=(len(indptr) - 1, len(vocabulary)),\n                           dtype=self.dtype)\n         X.sort_indices()\n         return vocabulary, X\n\n \n     def fit(self, raw_documents, y=None):\nif isinstance(raw_documents, six.string_types):\nraise ValueError(\"Iterable over raw text documents expected, string object received.\")\nself._validate_params()  # e.g. check ngram_range\n         self.fit_transform(raw_documents)\n         return self\n\n \n     def fit_transform(self, raw_documents, y=None):\n         if isinstance(raw_documents, six.string_types):\nraise ValueError(\"Iterable over raw text documents expected, string object received.\")\n         self._validate_params()\n\n# If user provided vocabulary but not yet validated, do so now\nif self.vocabulary is not None and not self.fixed_vocabulary_:\nself._validate_vocabulary()\n\n# If it's now marked fixed, skip building new tokens\nif self.fixed_vocabulary_:\n_, X = self._count_vocab(raw_documents, fixed_vocab=True)\nif self.binary:\nX.data.fill(1)\nreturn X\n\n# Otherwise, learn a fresh vocabulary\n         max_df = self.max_df\n         min_df = self.min_df\n         max_features = self.max_features\n\nvocabulary, X = self._count_vocab(raw_documents, fixed_vocab=False)\n\n         if self.binary:\n             X.data.fill(1)\n\nX = self._sort_features(X, vocabulary)\n\nimport numbers\nn_doc = X.shape[0]\nmax_doc_count = (max_df if isinstance(max_df, numbers.Integral)\nelse max_df * n_doc)\nmin_doc_count = (min_df if isinstance(min_df, numbers.Integral)\nelse min_df * n_doc)\nif max_doc_count < min_doc_count:\nraise ValueError(\"max_df corresponds to < documents than min_df\")\n\nX, self.stop_words_ = self._limit_features(X, vocabulary,\nmax_doc_count,\nmin_doc_count,\nmax_features)\nself.vocabulary_ = vocabulary\n         return X\n\n\n\n\n \n     def transform(self, raw_documents):\n         if isinstance(raw_documents, six.string_types):\nraise ValueError(\"Iterable over raw text documents expected, string object received.\")\nself._validate_params()\n\n# Validate user vocabulary if not done yet\nif self.vocabulary is not None and not self.fixed_vocabulary_:\n             self._validate_vocabulary()\n\nself._check_vocabulary()  # ensures vocabulary_ is fitted\n\n         _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n         if self.binary:\n             X.data.fill(1)\n\n         return X\n\n\n\n \n     def inverse_transform(self, X):\n         \"\"\"Return terms per document with nonzero entries in X.\n                 for i in range(n_samples)]\n \n     def get_feature_names(self):\n# If user-provided vocabulary (list/tuple) was never validated, do it now:\nif not hasattr(self, 'vocabulary_') and self.vocabulary is not None:\nfrom collections import Mapping, Sequence\nif not isinstance(self.vocabulary, str):\nif (isinstance(self.vocabulary, Mapping)\nor isinstance(self.vocabulary, set)\nor isinstance(self.vocabulary, Sequence)):\nself._validate_vocabulary()\nself.fixed_vocabulary_ = True\n# Now check\n         self._check_vocabulary()\n         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n                                      key=itemgetter(1))]\n\n \n \n def _make_int_array():\n",
        "test_cv_user_vocab_get_feature_names.py": "new file mode 100644\nindex 000000000..32921d25c\nimport sys\nimport traceback\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef main():\nprint(\"Test: Reproduce user-provided vocabulary get_feature_names bug.\")\nvocabulary = [\"and\",\"document\",\"first\",\"is\",\"one\",\"second\",\"the\",\"third\",\"this\"]\nprint(\"Step 1: Instantiate CountVectorizer with vocabulary =\", vocabulary)\ncv = CountVectorizer(vocabulary=vocabulary)\nprint(\"Step 2: Call get_feature_names() before fit() or transform()\")\ntry:\nfeats = cv.get_feature_names()\nprint(\"Success! get_feature_names did not raise an error.\")\nprint(\"Features returned =\", feats)\nprint(\"Test would pass if this was the desired behavior, so exit 0 to indicate bug is fixed.\")\nsys.exit(0)\nexcept Exception as e:\nprint(\"Exception raised:\", e)\nprint(\"Traceback:\", traceback.format_exc())\nprint(\"Test fails, which is the current known bug. Exiting with code 1.\")\nsys.exit(1)\n\nif __name__ == '__main__':\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/feature_extraction/text.py": " \n     def get_feature_names(self):\n         \"\"\"Array mapping from feature integer indices to feature name\"\"\"\nif not hasattr(self, 'vocabulary_'):\nself._validate_vocabulary()\n\n         self._check_vocabulary()\n \n         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 provides a comprehensive implementation of methods related to feature extraction and vocabulary management in text processing, including error handling for empty vocabularies and validation. The implementation includes functions to fit documents into a vocabulary, transform raw documents, and manage the vocabulary effectively. In contrast, the code in code sample 2 is a succinct test script designed to verify the behavior of the `get_feature_names()` function of a `CountVectorizer` instantiated with a predefined vocabulary, checking whether it raises exceptions when called prior to fitting the model.\", \"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because the first code sample is an implementation of classes and methods in a feature extraction context, while the second code sample is focused on testing specific functionality of those methods and does not share substantial structural similarities or functionality with the first sample.\"}"
}