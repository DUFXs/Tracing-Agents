{
    "instance_id": "sphinx-doc__sphinx-7440",
    "agent": "20241221_codestory_midwit_claude-3-5-sonnet_swe-search",
    "agent_patch": {
        "reproduce_error.py": "new file mode 100644\nindex 000000000..e5b7cfa23\nimport os\nimport sys\nfrom sphinx.application import Sphinx\nfrom sphinx.util.console import nocolor\n\n# Disable colors in Sphinx output\nnocolor()\n\n# Create a temporary test directory\ntest_dir = os.path.join(os.path.dirname(__file__), 'test_build')\nos.makedirs(test_dir, exist_ok=True)\n\n# Create source directory\nsrc_dir = os.path.join(test_dir, 'source')\nos.makedirs(src_dir, exist_ok=True)\n\n# Create test RST file with duplicate terms\ntest_rst = '''\nGlossary\n========\n\n.. glossary::\n\nmysql\nA case-sensitive database system (lowercase)\n\nMySQL\nA case-sensitive database system (uppercase)\n'''\n\n# Write the test file\nwith open(os.path.join(src_dir, 'index.rst'), 'w') as f:\nf.write(test_rst)\n\n# Create conf.py\nconf_py = '''\nproject = 'Test'\ncopyright = '2023'\nauthor = 'Test'\nextensions = []\n'''\n\nwith open(os.path.join(src_dir, 'conf.py'), 'w') as f:\nf.write(conf_py)\n\n# Build the documentation\napp = Sphinx(\nsrc_dir,                # source directory\nsrc_dir,                # configuration directory\nos.path.join(test_dir, 'build'),  # output directory\nos.path.join(test_dir, 'doctree'),  # doctree directory\n'html',                 # builder name\n{},                     # config overrides\nsys.stderr,            # status output\nsys.stderr,            # warning output\nTrue,                  # fresh env\n)\n\napp.build()\n\\ No newline at end of file\n",
        "sphinx/domains/std.py": "index d820cfe5c..19270a11f 100644\n         term['ids'].append(node_id)\n \n     std = cast(StandardDomain, env.get_domain('std'))\n# Store terms with their original case to allow case-sensitive terms\nstd.note_object('term', termtext, node_id, location=term)\n \n     # add an index entry too\n     indexnode = addnodes.index()\n         'envvar':  EnvVarXRefRole(),\n         # links to tokens in grammar productions\n         'token':   TokenXRefRole(),\n# links to terms in glossary (case-sensitive)\n'term':    XRefRole(lowercase=False, innernodeclass=nodes.inline,\n                             warn_dangling=True),\n         # links to headings or arbitrary labels\n         'ref':     XRefRole(lowercase=True, innernodeclass=nodes.inline,\n"
    },
    "ground_truth_patch": {
        "sphinx/domains/std.py": "         term['ids'].append(node_id)\n \n     std = cast(StandardDomain, env.get_domain('std'))\nstd.note_object('term', termtext, node_id, location=term)\n \n     # add an index entry too\n     indexnode = addnodes.index()\n         # links to tokens in grammar productions\n         'token':   TokenXRefRole(),\n         # links to terms in glossary\n'term':    XRefRole(innernodeclass=nodes.inline,\n                             warn_dangling=True),\n         # links to headings or arbitrary labels\n         'ref':     XRefRole(lowercase=True, innernodeclass=nodes.inline,\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 creates a temporary directory structure for building documentation with Sphinx, defines a glossary in an RST format, and builds the documentation. It involves creating a test directory, source directory, and configuration file before invoking the Sphinx build process. On the other hand, the code in code sample 2 is a snippet from a Sphinx domain module that handles the storage of terms in a glossary. It appears to deal with appending term IDs, fetching domain references, and setting up cross-references. The two code samples serve different purposes and functionalities, with code sample 1 focused on building documentation and code sample 2 focused on processing glossary terms. Therefore, they are not clones. \", \"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they belong to different functionalities within the Sphinx framework, with code sample 1 handling documentation generation and code sample 2 dealing with glossary term processing.\"}"
}