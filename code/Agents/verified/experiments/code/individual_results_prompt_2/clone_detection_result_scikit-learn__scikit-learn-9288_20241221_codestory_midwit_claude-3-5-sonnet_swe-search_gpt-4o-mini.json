{
    "instance_id": "scikit-learn__scikit-learn-9288",
    "agent": "20241221_codestory_midwit_claude-3-5-sonnet_swe-search",
    "agent_patch": {
        "reproduce_error.py": "new file mode 100644\nindex 000000000..d685c1c84\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# For reproducibility\nnp.random.seed(42)\n\n# Generate some data\nX, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)\n\n# Run KMeans with various n_jobs values\nfor n_jobs in range(1, 5):\nkmeans = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)\nkmeans.fit(X)\nprint(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')\n\\ No newline at end of file\n",
        "sklearn/cluster/_k_means.pyx": "index 8a66f2506..2c62d72fb 100644\n     cdef int i, j, c\n     cdef np.ndarray[floating, ndim=2] centers\n     cdef np.ndarray[floating, ndim=1] weight_in_cluster\ncdef np.ndarray[floating, ndim=2] weighted_sums\n \n     dtype = np.float32 if floating is float else np.float64\n     centers = np.zeros((n_clusters, n_features), dtype=dtype)\n     weight_in_cluster = np.zeros((n_clusters,), dtype=dtype)\nweighted_sums = np.zeros((n_clusters, n_features), dtype=dtype)\n \n# First compute cluster weights\n     for i in range(n_samples):\n         c = labels[i]\n         weight_in_cluster[c] += sample_weight[i]\n\n     empty_clusters = np.where(weight_in_cluster == 0)[0]\n     # maybe also relocate small clusters?\n \n         for i, cluster_id in enumerate(empty_clusters):\n             # XXX two relocated clusters could be close to each other\n             far_index = far_from_centers[i]\n# Initialize empty cluster with its furthest point\nfor j in range(n_features):\nweighted_sums[cluster_id, j] = X[far_index, j] * sample_weight[far_index]\n             weight_in_cluster[cluster_id] = sample_weight[far_index]\n \n# Compute weighted sums for non-empty clusters\n     for i in range(n_samples):\nc = labels[i]\nif weight_in_cluster[c] > 0:  # Only process non-empty clusters\nfor j in range(n_features):\nweighted_sums[c, j] += X[i, j] * sample_weight[i]\n\n# Normalize to get centers\nfor c in range(n_clusters):\nif weight_in_cluster[c] > 0:\nfor j in range(n_features):\ncenters[c, j] = weighted_sums[c, j] / weight_in_cluster[c]\n \n     return centers\n \n",
        "sklearn/cluster/k_means_.py": "index b7fbdf7da..ae5c54382 100644\n     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n# Generate seeds for both sequential and parallel cases\nseeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\nfor it, seed in enumerate(seeds):\n# run a k-means once with consistent seed\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\nrandom_state=seed)  # Use same seed sequence as parallel case\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\n                                    verbose=verbose, tol=tol,\n                                    precompute_distances=precompute_distances,\n                                    x_squared_norms=x_squared_norms,\nrandom_state=seed)  # Use pre-generated seeds\n             for seed in seeds)\n         # Get results with the lowest inertia\n         labels, inertia, centers, n_iters = zip(*results)\n",
        "test_kmeans_extended.py": "new file mode 100644\nindex 000000000..8bd89c232\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n# Test with different random states and data sizes\nfor random_state in [2, 42, 123]:\nfor n_samples in [1000, 10000]:\nprint(f\"\\nTesting with random_state={random_state}, n_samples={n_samples}\")\n# Generate some data\nX, y = make_blobs(n_samples=n_samples, centers=10, n_features=2, random_state=random_state)\n\n# Run KMeans with various n_jobs values\nfor n_jobs in range(1, 5):\nkmeans = KMeans(n_clusters=10, random_state=random_state, n_jobs=n_jobs)\nkmeans.fit(X)\nprint(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')\n\\ No newline at end of file\n"
    },
    "ground_truth_patch": {
        "sklearn/cluster/k_means_.py": "     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n\nseeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\nfor seed in seeds:\n             # run a k-means once\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\nrandom_state=seed)\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 implements a KMeans clustering algorithm using synthetic data generated with 'make_blobs'. It sets a random seed for reproducibility and runs the KMeans algorithm across different values for the number of jobs (n_jobs), printing the inertia for each configuration. The code sample creates data with a specified number of samples (10,000), clustering these into 10 groups. \\n\\nOn the other hand, code sample 2 is part of a module that checks for the algorithm type and generates seeds for KMeans clustering. It runs the clustering algorithm with pre-generated random seeds to optimize the process across various configurations, including a parallel execution path for better performance. However, it does not perform the clustering itself but supports the execution of multiple runs of the KMeans algorithm for given parameters. \\n\\nThe first sample focuses on testing the KMeans clustering itself with a variety of configurations, while the second sample provides underlying support functionalities for executing clustering with specific optimizations.\", \"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they serve different purposes within the KMeans clustering framework and do not share computational logic or structure. Sample 1 tests the algorithm, while sample 2 manages the setup for running the algorithm.\"}"
}