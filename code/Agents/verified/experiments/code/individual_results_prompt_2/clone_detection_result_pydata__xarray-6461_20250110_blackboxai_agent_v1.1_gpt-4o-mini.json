{
    "instance_id": "pydata__xarray-6461",
    "agent": "20250110_blackboxai_agent_v1.1",
    "agent_patch": {
        ".gitignore": "index 686c7efa..12e8c960 100644\n Icon*\n \n .ipynb_checkpoints\n.aider/\n",
        "xarray/core/computation.py": "index 7676d8e5..e0917633 100644\n     if keep_attrs is True:\n         # keep the attributes of x, the second parameter, by default to\n         # be consistent with the `where` method of `DataArray` and `Dataset`\nkeep_attrs = lambda attrs, context: attrs[1] if len(attrs) > 1 and attrs[1] is not None else {}\n \n     # alignment for three arguments is complicated, so don't support it yet\n     return apply_ufunc(\n",
        "xarray/core/computation.py.bak": "new file mode 100644\nindex 00000000..7676d8e5\n\"\"\"\nFunctions for applying functions that act on arrays to xarray's labeled data.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nimport itertools\nimport operator\nimport warnings\nfrom collections import Counter\nfrom typing import (\nTYPE_CHECKING,\nAbstractSet,\nAny,\nCallable,\nHashable,\nIterable,\nMapping,\nSequence,\n)\n\nimport numpy as np\n\nfrom . import dtypes, duck_array_ops, utils\nfrom .alignment import align, deep_align\nfrom .indexes import Index, filter_indexes_from_coords\nfrom .merge import merge_attrs, merge_coordinates_without_align\nfrom .options import OPTIONS, _get_keep_attrs\nfrom .pycompat import is_duck_dask_array\nfrom .utils import is_dict_like\nfrom .variable import Variable\n\nif TYPE_CHECKING:\nfrom .coordinates import Coordinates\nfrom .dataarray import DataArray\nfrom .dataset import Dataset\nfrom .types import T_Xarray\n\n_NO_FILL_VALUE = utils.ReprObject(\"<no-fill-value>\")\n_DEFAULT_NAME = utils.ReprObject(\"<default-name>\")\n_JOINS_WITHOUT_FILL_VALUES = frozenset({\"inner\", \"exact\"})\n\n\ndef _first_of_type(args, kind):\n\"\"\"Return either first object of type 'kind' or raise if not found.\"\"\"\nfor arg in args:\nif isinstance(arg, kind):\nreturn arg\nraise ValueError(\"This should be unreachable.\")\n\n\ndef _all_of_type(args, kind):\n\"\"\"Return all objects of type 'kind'\"\"\"\nreturn [arg for arg in args if isinstance(arg, kind)]\n\n\nclass _UFuncSignature:\n\"\"\"Core dimensions signature for a given function.\n\nBased on the signature provided by generalized ufuncs in NumPy.\n\nAttributes\n----------\ninput_core_dims : tuple[tuple]\nCore dimension names on each input variable.\noutput_core_dims : tuple[tuple]\nCore dimension names on each output variable.\n\"\"\"\n\n__slots__ = (\n\"input_core_dims\",\n\"output_core_dims\",\n\"_all_input_core_dims\",\n\"_all_output_core_dims\",\n\"_all_core_dims\",\n)\n\ndef __init__(self, input_core_dims, output_core_dims=((),)):\nself.input_core_dims = tuple(tuple(a) for a in input_core_dims)\nself.output_core_dims = tuple(tuple(a) for a in output_core_dims)\nself._all_input_core_dims = None\nself._all_output_core_dims = None\nself._all_core_dims = None\n\n@property\ndef all_input_core_dims(self):\nif self._all_input_core_dims is None:\nself._all_input_core_dims = frozenset(\ndim for dims in self.input_core_dims for dim in dims\n)\nreturn self._all_input_core_dims\n\n@property\ndef all_output_core_dims(self):\nif self._all_output_core_dims is None:\nself._all_output_core_dims = frozenset(\ndim for dims in self.output_core_dims for dim in dims\n)\nreturn self._all_output_core_dims\n\n@property\ndef all_core_dims(self):\nif self._all_core_dims is None:\nself._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\nreturn self._all_core_dims\n\n@property\ndef dims_map(self):\nreturn {\ncore_dim: f\"dim{n}\" for n, core_dim in enumerate(sorted(self.all_core_dims))\n}\n\n@property\ndef num_inputs(self):\nreturn len(self.input_core_dims)\n\n@property\ndef num_outputs(self):\nreturn len(self.output_core_dims)\n\ndef __eq__(self, other):\ntry:\nreturn (\nself.input_core_dims == other.input_core_dims\nand self.output_core_dims == other.output_core_dims\n)\nexcept AttributeError:\nreturn False\n\ndef __ne__(self, other):\nreturn not self == other\n\ndef __repr__(self):\nreturn \"{}({!r}, {!r})\".format(\ntype(self).__name__,\nlist(self.input_core_dims),\nlist(self.output_core_dims),\n)\n\ndef __str__(self):\nlhs = \",\".join(\"({})\".format(\",\".join(dims)) for dims in self.input_core_dims)\nrhs = \",\".join(\"({})\".format(\",\".join(dims)) for dims in self.output_core_dims)\nreturn f\"{lhs}->{rhs}\"\n\ndef to_gufunc_string(self, exclude_dims=frozenset()):\n\"\"\"Create an equivalent signature string for a NumPy gufunc.\n\nUnlike __str__, handles dimensions that don't map to Python\nidentifiers.\n\nAlso creates unique names for input_core_dims contained in exclude_dims.\n\"\"\"\ninput_core_dims = [\n[self.dims_map[dim] for dim in core_dims]\nfor core_dims in self.input_core_dims\n]\noutput_core_dims = [\n[self.dims_map[dim] for dim in core_dims]\nfor core_dims in self.output_core_dims\n]\n\n# enumerate input_core_dims contained in exclude_dims to make them unique\nif exclude_dims:\n\nexclude_dims = [self.dims_map[dim] for dim in exclude_dims]\n\ncounter = Counter()\n\ndef _enumerate(dim):\nif dim in exclude_dims:\nn = counter[dim]\ncounter.update([dim])\ndim = f\"{dim}_{n}\"\nreturn dim\n\ninput_core_dims = [\n[_enumerate(dim) for dim in arg] for arg in input_core_dims\n]\n\nalt_signature = type(self)(input_core_dims, output_core_dims)\nreturn str(alt_signature)\n\n\ndef result_name(objects: list) -> Any:\n# use the same naming heuristics as pandas:\n# https://github.com/blaze/blaze/issues/458#issuecomment-51936356\nnames = {getattr(obj, \"name\", _DEFAULT_NAME) for obj in objects}\nnames.discard(_DEFAULT_NAME)\nif len(names) == 1:\n(name,) = names\nelse:\nname = None\nreturn name\n\n\ndef _get_coords_list(args) -> list[Coordinates]:\ncoords_list = []\nfor arg in args:\ntry:\ncoords = arg.coords\nexcept AttributeError:\npass  # skip this argument\nelse:\ncoords_list.append(coords)\nreturn coords_list\n\n\ndef build_output_coords_and_indexes(\nargs: list,\nsignature: _UFuncSignature,\nexclude_dims: AbstractSet = frozenset(),\ncombine_attrs: str = \"override\",\n) -> tuple[list[dict[Any, Variable]], list[dict[Any, Index]]]:\n\"\"\"Build output coordinates and indexes for an operation.\n\nParameters\n----------\nargs : list\nList of raw operation arguments. Any valid types for xarray operations\nare OK, e.g., scalars, Variable, DataArray, Dataset.\nsignature : _UfuncSignature\nCore dimensions signature for the operation.\nexclude_dims : set, optional\nDimensions excluded from the operation. Coordinates along these\ndimensions are dropped.\n\nReturns\n-------\nDictionaries of Variable and Index objects with merged coordinates.\n\"\"\"\ncoords_list = _get_coords_list(args)\n\nif len(coords_list) == 1 and not exclude_dims:\n# we can skip the expensive merge\n(unpacked_coords,) = coords_list\nmerged_vars = dict(unpacked_coords.variables)\nmerged_indexes = dict(unpacked_coords.xindexes)\nelse:\nmerged_vars, merged_indexes = merge_coordinates_without_align(\ncoords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs\n)\n\noutput_coords = []\noutput_indexes = []\nfor output_dims in signature.output_core_dims:\ndropped_dims = signature.all_input_core_dims - set(output_dims)\nif dropped_dims:\nfiltered_coords = {\nk: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)\n}\nfiltered_indexes = filter_indexes_from_coords(\nmerged_indexes, set(filtered_coords)\n)\nelse:\nfiltered_coords = merged_vars\nfiltered_indexes = merged_indexes\noutput_coords.append(filtered_coords)\noutput_indexes.append(filtered_indexes)\n\nreturn output_coords, output_indexes\n\n\ndef apply_dataarray_vfunc(\nfunc,\n*args,\nsignature,\njoin=\"inner\",\nexclude_dims=frozenset(),\nkeep_attrs=\"override\",\n):\n\"\"\"Apply a variable level function over DataArray, Variable and/or ndarray\nobjects.\n\"\"\"\nfrom .dataarray import DataArray\n\nif len(args) > 1:\nargs = deep_align(\nargs, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\n)\n\nobjs = _all_of_type(args, DataArray)\n\nif keep_attrs == \"drop\":\nname = result_name(args)\nelse:\nfirst_obj = _first_of_type(args, DataArray)\nname = first_obj.name\nresult_coords, result_indexes = build_output_coords_and_indexes(\nargs, signature, exclude_dims, combine_attrs=keep_attrs\n)\n\ndata_vars = [getattr(a, \"variable\", a) for a in args]\nresult_var = func(*data_vars)\n\nif signature.num_outputs > 1:\nout = tuple(\nDataArray(\nvariable, coords=coords, indexes=indexes, name=name, fastpath=True\n)\nfor variable, coords, indexes in zip(\nresult_var, result_coords, result_indexes\n)\n)\nelse:\n(coords,) = result_coords\n(indexes,) = result_indexes\nout = DataArray(\nresult_var, coords=coords, indexes=indexes, name=name, fastpath=True\n)\n\nattrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)\nif isinstance(out, tuple):\nfor da in out:\nda.attrs = attrs\nelse:\nout.attrs = attrs\n\nreturn out\n\n\ndef ordered_set_union(all_keys: list[Iterable]) -> Iterable:\nreturn {key: None for keys in all_keys for key in keys}.keys()\n\n\ndef ordered_set_intersection(all_keys: list[Iterable]) -> Iterable:\nintersection = set(all_keys[0])\nfor keys in all_keys[1:]:\nintersection.intersection_update(keys)\nreturn [key for key in all_keys[0] if key in intersection]\n\n\ndef assert_and_return_exact_match(all_keys):\nfirst_keys = all_keys[0]\nfor keys in all_keys[1:]:\nif keys != first_keys:\nraise ValueError(\n\"exact match required for all data variable names, \"\nf\"but {keys!r} != {first_keys!r}\"\n)\nreturn first_keys\n\n\n_JOINERS: dict[str, Callable] = {\n\"inner\": ordered_set_intersection,\n\"outer\": ordered_set_union,\n\"left\": operator.itemgetter(0),\n\"right\": operator.itemgetter(-1),\n\"exact\": assert_and_return_exact_match,\n}\n\n\ndef join_dict_keys(objects: Iterable[Mapping | Any], how: str = \"inner\") -> Iterable:\njoiner = _JOINERS[how]\nall_keys = [obj.keys() for obj in objects if hasattr(obj, \"keys\")]\nreturn joiner(all_keys)\n\n\ndef collect_dict_values(\nobjects: Iterable[Mapping | Any], keys: Iterable, fill_value: object = None\n) -> list[list]:\nreturn [\n[obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]\nfor key in keys\n]\n\n\ndef _as_variables_or_variable(arg):\ntry:\nreturn arg.variables\nexcept AttributeError:\ntry:\nreturn arg.variable\nexcept AttributeError:\nreturn arg\n\n\ndef _unpack_dict_tuples(\nresult_vars: Mapping[Any, tuple[Variable, ...]], num_outputs: int\n) -> tuple[dict[Hashable, Variable], ...]:\nout: tuple[dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))\nfor name, values in result_vars.items():\nfor value, results_dict in zip(values, out):\nresults_dict[name] = value\nreturn out\n\n\ndef apply_dict_of_variables_vfunc(\nfunc, *args, signature, join=\"inner\", fill_value=None\n):\n\"\"\"Apply a variable level function over dicts of DataArray, DataArray,\nVariable and ndarray objects.\n\"\"\"\nargs = [_as_variables_or_variable(arg) for arg in args]\nnames = join_dict_keys(args, how=join)\ngrouped_by_name = collect_dict_values(args, names, fill_value)\n\nresult_vars = {}\nfor name, variable_args in zip(names, grouped_by_name):\nresult_vars[name] = func(*variable_args)\n\nif signature.num_outputs > 1:\nreturn _unpack_dict_tuples(result_vars, signature.num_outputs)\nelse:\nreturn result_vars\n\n\ndef _fast_dataset(\nvariables: dict[Hashable, Variable],\ncoord_variables: Mapping[Hashable, Variable],\nindexes: dict[Hashable, Index],\n) -> Dataset:\n\"\"\"Create a dataset as quickly as possible.\n\nBeware: the `variables` dict is modified INPLACE.\n\"\"\"\nfrom .dataset import Dataset\n\nvariables.update(coord_variables)\ncoord_names = set(coord_variables)\nreturn Dataset._construct_direct(variables, coord_names, indexes=indexes)\n\n\ndef apply_dataset_vfunc(\nfunc,\n*args,\nsignature,\njoin=\"inner\",\ndataset_join=\"exact\",\nfill_value=_NO_FILL_VALUE,\nexclude_dims=frozenset(),\nkeep_attrs=\"override\",\n):\n\"\"\"Apply a variable level function over Dataset, dict of DataArray,\nDataArray, Variable and/or ndarray objects.\n\"\"\"\nfrom .dataset import Dataset\n\nif dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:\nraise TypeError(\n\"to apply an operation to datasets with different \"\n\"data variables with apply_ufunc, you must supply the \"\n\"dataset_fill_value argument.\"\n)\n\nobjs = _all_of_type(args, Dataset)\n\nif len(args) > 1:\nargs = deep_align(\nargs, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False\n)\n\nlist_of_coords, list_of_indexes = build_output_coords_and_indexes(\nargs, signature, exclude_dims, combine_attrs=keep_attrs\n)\nargs = [getattr(arg, \"data_vars\", arg) for arg in args]\n\nresult_vars = apply_dict_of_variables_vfunc(\nfunc, *args, signature=signature, join=dataset_join, fill_value=fill_value\n)\n\nif signature.num_outputs > 1:\nout = tuple(\n_fast_dataset(*args)\nfor args in zip(result_vars, list_of_coords, list_of_indexes)\n)\nelse:\n(coord_vars,) = list_of_coords\n(indexes,) = list_of_indexes\nout = _fast_dataset(result_vars, coord_vars, indexes=indexes)\n\nattrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)\nif isinstance(out, tuple):\nfor ds in out:\nds.attrs = attrs\nelse:\nout.attrs = attrs\n\nreturn out\n\n\ndef _iter_over_selections(obj, dim, values):\n\"\"\"Iterate over selections of an xarray object in the provided order.\"\"\"\nfrom .groupby import _dummy_copy\n\ndummy = None\nfor value in values:\ntry:\nobj_sel = obj.sel(**{dim: value})\nexcept (KeyError, IndexError):\nif dummy is None:\ndummy = _dummy_copy(obj)\nobj_sel = dummy\nyield obj_sel\n\n\ndef apply_groupby_func(func, *args):\n\"\"\"Apply a dataset or datarray level function over GroupBy, Dataset,\nDataArray, Variable and/or ndarray objects.\n\"\"\"\nfrom .groupby import GroupBy, peek_at\nfrom .variable import Variable\n\ngroupbys = [arg for arg in args if isinstance(arg, GroupBy)]\nassert groupbys, \"must have at least one groupby to iterate over\"\nfirst_groupby = groupbys[0]\nif any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):\nraise ValueError(\n\"apply_ufunc can only perform operations over \"\n\"multiple GroupBy objects at once if they are all \"\n\"grouped the same way\"\n)\n\ngrouped_dim = first_groupby._group.name\nunique_values = first_groupby._unique_coord.values\n\niterators = []\nfor arg in args:\nif isinstance(arg, GroupBy):\niterator = (value for _, value in arg)\nelif hasattr(arg, \"dims\") and grouped_dim in arg.dims:\nif isinstance(arg, Variable):\nraise ValueError(\n\"groupby operations cannot be performed with \"\n\"xarray.Variable objects that share a dimension with \"\n\"the grouped dimension\"\n)\niterator = _iter_over_selections(arg, grouped_dim, unique_values)\nelse:\niterator = itertools.repeat(arg)\niterators.append(iterator)\n\napplied = (func(*zipped_args) for zipped_args in zip(*iterators))\napplied_example, applied = peek_at(applied)\ncombine = first_groupby._combine\nif isinstance(applied_example, tuple):\ncombined = tuple(combine(output) for output in zip(*applied))\nelse:\ncombined = combine(applied)\nreturn combined\n\n\ndef unified_dim_sizes(\nvariables: Iterable[Variable], exclude_dims: AbstractSet = frozenset()\n) -> dict[Hashable, int]:\n\ndim_sizes: dict[Hashable, int] = {}\n\nfor var in variables:\nif len(set(var.dims)) < len(var.dims):\nraise ValueError(\n\"broadcasting cannot handle duplicate \"\nf\"dimensions on a variable: {list(var.dims)}\"\n)\nfor dim, size in zip(var.dims, var.shape):\nif dim not in exclude_dims:\nif dim not in dim_sizes:\ndim_sizes[dim] = size\nelif dim_sizes[dim] != size:\nraise ValueError(\n\"operands cannot be broadcast together \"\n\"with mismatched lengths for dimension \"\nf\"{dim}: {dim_sizes[dim]} vs {size}\"\n)\nreturn dim_sizes\n\n\nSLICE_NONE = slice(None)\n\n\ndef broadcast_compat_data(\nvariable: Variable,\nbroadcast_dims: tuple[Hashable, ...],\ncore_dims: tuple[Hashable, ...],\n) -> Any:\ndata = variable.data\n\nold_dims = variable.dims\nnew_dims = broadcast_dims + core_dims\n\nif new_dims == old_dims:\n# optimize for the typical case\nreturn data\n\nset_old_dims = set(old_dims)\nmissing_core_dims = [d for d in core_dims if d not in set_old_dims]\nif missing_core_dims:\nraise ValueError(\n\"operand to apply_ufunc has required core dimensions {}, but \"\n\"some of these dimensions are absent on an input variable: {}\".format(\nlist(core_dims), missing_core_dims\n)\n)\n\nset_new_dims = set(new_dims)\nunexpected_dims = [d for d in old_dims if d not in set_new_dims]\nif unexpected_dims:\nraise ValueError(\n\"operand to apply_ufunc encountered unexpected \"\nf\"dimensions {unexpected_dims!r} on an input variable: these are core \"\n\"dimensions on other input or output variables\"\n)\n\n# for consistency with numpy, keep broadcast dimensions to the left\nold_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)\nreordered_dims = old_broadcast_dims + core_dims\nif reordered_dims != old_dims:\norder = tuple(old_dims.index(d) for d in reordered_dims)\ndata = duck_array_ops.transpose(data, order)\n\nif new_dims != reordered_dims:\nkey_parts: list[slice | None] = []\nfor dim in new_dims:\nif dim in set_old_dims:\nkey_parts.append(SLICE_NONE)\nelif key_parts:\n# no need to insert new axes at the beginning that are already\n# handled by broadcasting\nkey_parts.append(np.newaxis)\ndata = data[tuple(key_parts)]\n\nreturn data\n\n\ndef _vectorize(func, signature, output_dtypes, exclude_dims):\nif signature.all_core_dims:\nfunc = np.vectorize(\nfunc,\notypes=output_dtypes,\nsignature=signature.to_gufunc_string(exclude_dims),\n)\nelse:\nfunc = np.vectorize(func, otypes=output_dtypes)\n\nreturn func\n\n\ndef apply_variable_ufunc(\nfunc,\n*args,\nsignature,\nexclude_dims=frozenset(),\ndask=\"forbidden\",\noutput_dtypes=None,\nvectorize=False,\nkeep_attrs=\"override\",\ndask_gufunc_kwargs=None,\n):\n\"\"\"Apply a ndarray level function over Variable and/or ndarray objects.\"\"\"\nfrom .variable import Variable, as_compatible_data\n\ndim_sizes = unified_dim_sizes(\n(a for a in args if hasattr(a, \"dims\")), exclude_dims=exclude_dims\n)\nbroadcast_dims = tuple(\ndim for dim in dim_sizes if dim not in signature.all_core_dims\n)\noutput_dims = [broadcast_dims + out for out in signature.output_core_dims]\n\ninput_data = [\nbroadcast_compat_data(arg, broadcast_dims, core_dims)\nif isinstance(arg, Variable)\nelse arg\nfor arg, core_dims in zip(args, signature.input_core_dims)\n]\n\nif any(is_duck_dask_array(array) for array in input_data):\nif dask == \"forbidden\":\nraise ValueError(\n\"apply_ufunc encountered a dask array on an \"\n\"argument, but handling for dask arrays has not \"\n\"been enabled. Either set the ``dask`` argument \"\n\"or load your data into memory first with \"\n\"``.load()`` or ``.compute()``\"\n)\nelif dask == \"parallelized\":\nnumpy_func = func\n\nif dask_gufunc_kwargs is None:\ndask_gufunc_kwargs = {}\nelse:\ndask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n\nallow_rechunk = dask_gufunc_kwargs.get(\"allow_rechunk\", None)\nif allow_rechunk is None:\nfor n, (data, core_dims) in enumerate(\nzip(input_data, signature.input_core_dims)\n):\nif is_duck_dask_array(data):\n# core dimensions cannot span multiple chunks\nfor axis, dim in enumerate(core_dims, start=-len(core_dims)):\nif len(data.chunks[axis]) != 1:\nraise ValueError(\nf\"dimension {dim} on {n}th function argument to \"\n\"apply_ufunc with dask='parallelized' consists of \"\n\"multiple chunks, but is also a core dimension. To \"\n\"fix, either rechunk into a single dask array chunk along \"\nf\"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or \"\n\"pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` \"\n\"but beware that this may significantly increase memory usage.\"\n)\ndask_gufunc_kwargs[\"allow_rechunk\"] = True\n\noutput_sizes = dask_gufunc_kwargs.pop(\"output_sizes\", {})\nif output_sizes:\noutput_sizes_renamed = {}\nfor key, value in output_sizes.items():\nif key not in signature.all_output_core_dims:\nraise ValueError(\nf\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\"\n)\noutput_sizes_renamed[signature.dims_map[key]] = value\ndask_gufunc_kwargs[\"output_sizes\"] = output_sizes_renamed\n\nfor key in signature.all_output_core_dims:\nif key not in signature.all_input_core_dims and key not in output_sizes:\nraise ValueError(\nf\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\"\n)\n\ndef func(*arrays):\nimport dask.array as da\n\nres = da.apply_gufunc(\nnumpy_func,\nsignature.to_gufunc_string(exclude_dims),\n*arrays,\nvectorize=vectorize,\noutput_dtypes=output_dtypes,\n**dask_gufunc_kwargs,\n)\n\nreturn res\n\nelif dask == \"allowed\":\npass\nelse:\nraise ValueError(\n\"unknown setting for dask array handling in \"\n\"apply_ufunc: {}\".format(dask)\n)\nelse:\nif vectorize:\nfunc = _vectorize(\nfunc, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims\n)\n\nresult_data = func(*input_data)\n\nif signature.num_outputs == 1:\nresult_data = (result_data,)\nelif (\nnot isinstance(result_data, tuple) or len(result_data) != signature.num_outputs\n):\nraise ValueError(\n\"applied function does not have the number of \"\n\"outputs specified in the ufunc signature. \"\n\"Result is not a tuple of {} elements: {!r}\".format(\nsignature.num_outputs, result_data\n)\n)\n\nobjs = _all_of_type(args, Variable)\nattrs = merge_attrs(\n[obj.attrs for obj in objs],\ncombine_attrs=keep_attrs,\n)\n\noutput = []\nfor dims, data in zip(output_dims, result_data):\ndata = as_compatible_data(data)\nif data.ndim != len(dims):\nraise ValueError(\n\"applied function returned data with unexpected \"\nf\"number of dimensions. Received {data.ndim} dimension(s) but \"\nf\"expected {len(dims)} dimensions with names: {dims!r}\"\n)\n\nvar = Variable(dims, data, fastpath=True)\nfor dim, new_size in var.sizes.items():\nif dim in dim_sizes and new_size != dim_sizes[dim]:\nraise ValueError(\n\"size of dimension {!r} on inputs was unexpectedly \"\n\"changed by applied function from {} to {}. Only \"\n\"dimensions specified in ``exclude_dims`` with \"\n\"xarray.apply_ufunc are allowed to change size.\".format(\ndim, dim_sizes[dim], new_size\n)\n)\n\nvar.attrs = attrs\noutput.append(var)\n\nif signature.num_outputs == 1:\nreturn output[0]\nelse:\nreturn tuple(output)\n\n\ndef apply_array_ufunc(func, *args, dask=\"forbidden\"):\n\"\"\"Apply a ndarray level function over ndarray objects.\"\"\"\nif any(is_duck_dask_array(arg) for arg in args):\nif dask == \"forbidden\":\nraise ValueError(\n\"apply_ufunc encountered a dask array on an \"\n\"argument, but handling for dask arrays has not \"\n\"been enabled. Either set the ``dask`` argument \"\n\"or load your data into memory first with \"\n\"``.load()`` or ``.compute()``\"\n)\nelif dask == \"parallelized\":\nraise ValueError(\n\"cannot use dask='parallelized' for apply_ufunc \"\n\"unless at least one input is an xarray object\"\n)\nelif dask == \"allowed\":\npass\nelse:\nraise ValueError(f\"unknown setting for dask array handling: {dask}\")\nreturn func(*args)\n\n\ndef apply_ufunc(\nfunc: Callable,\n*args: Any,\ninput_core_dims: Sequence[Sequence] = None,\noutput_core_dims: Sequence[Sequence] | None = ((),),\nexclude_dims: AbstractSet = frozenset(),\nvectorize: bool = False,\njoin: str = \"exact\",\ndataset_join: str = \"exact\",\ndataset_fill_value: object = _NO_FILL_VALUE,\nkeep_attrs: bool | str | None = None,\nkwargs: Mapping | None = None,\ndask: str = \"forbidden\",\noutput_dtypes: Sequence | None = None,\noutput_sizes: Mapping[Any, int] | None = None,\nmeta: Any = None,\ndask_gufunc_kwargs: dict[str, Any] | None = None,\n) -> Any:\n\"\"\"Apply a vectorized function for unlabeled arrays on xarray objects.\n\nThe function will be mapped over the data variable(s) of the input\narguments using xarray's standard rules for labeled computation, including\nalignment, broadcasting, looping over GroupBy/Dataset variables, and\nmerging of coordinates.\n\nParameters\n----------\nfunc : callable\nFunction to call like ``func(*args, **kwargs)`` on unlabeled arrays\n(``.data``) that returns an array or tuple of arrays. If multiple\narguments with non-matching dimensions are supplied, this function is\nexpected to vectorize (broadcast) over axes of positional arguments in\nthe style of NumPy universal functions [1]_ (if this is not the case,\nset ``vectorize=True``). If this function returns multiple outputs, you\nmust set ``output_core_dims`` as well.\n*args : Dataset, DataArray, DataArrayGroupBy, DatasetGroupBy, Variable, numpy.ndarray, dask.array.Array or scalar\nMix of labeled and/or unlabeled arrays to which to apply the function.\ninput_core_dims : sequence of sequence, optional\nList of the same length as ``args`` giving the list of core dimensions\non each input argument that should not be broadcast. By default, we\nassume there are no core dimensions on any input arguments.\n\nFor example, ``input_core_dims=[[], ['time']]`` indicates that all\ndimensions on the first argument and all dimensions other than 'time'\non the second argument should be broadcast.\n\nCore dimensions are automatically moved to the last axes of input\nvariables before applying ``func``, which facilitates using NumPy style\ngeneralized ufuncs [2]_.\noutput_core_dims : list of tuple, optional\nList of the same length as the number of output arguments from\n``func``, giving the list of core dimensions on each output that were\nnot broadcast on the inputs. By default, we assume that ``func``\noutputs exactly one array, with axes corresponding to each broadcast\ndimension.\n\nCore dimensions are assumed to appear as the last dimensions of each\noutput in the provided order.\nexclude_dims : set, optional\nCore dimensions on the inputs to exclude from alignment and\nbroadcasting entirely. Any input coordinates along these dimensions\nwill be dropped. Each excluded dimension must also appear in\n``input_core_dims`` for at least one argument. Only dimensions listed\nhere are allowed to change size between input and output objects.\nvectorize : bool, optional\nIf True, then assume ``func`` only takes arrays defined over core\ndimensions as input and vectorize it automatically with\n:py:func:`numpy.vectorize`. This option exists for convenience, but is\nalmost always slower than supplying a pre-vectorized function.\nUsing this option requires NumPy version 1.12 or newer.\njoin : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, default: \"exact\"\nMethod for joining the indexes of the passed objects along each\ndimension, and the variables of Dataset objects with mismatched\ndata variables:\n\n- 'outer': use the union of object indexes\n- 'inner': use the intersection of object indexes\n- 'left': use indexes from the first object with each dimension\n- 'right': use indexes from the last object with each dimension\n- 'exact': raise `ValueError` instead of aligning when indexes to be\naligned are not equal\ndataset_join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, default: \"exact\"\nMethod for joining variables of Dataset objects with mismatched\ndata variables.\n\n- 'outer': take variables from both Dataset objects\n- 'inner': take only overlapped variables\n- 'left': take only variables from the first object\n- 'right': take only variables from the last object\n- 'exact': data variables on all Dataset objects must match exactly\ndataset_fill_value : optional\nValue used in place of missing variables on Dataset inputs when the\ndatasets do not share the exact same ``data_vars``. Required if\n``dataset_join not in {'inner', 'exact'}``, otherwise ignored.\nkeep_attrs : bool, optional\nWhether to copy attributes from the first argument to the output.\nkwargs : dict, optional\nOptional keyword arguments passed directly on to call ``func``.\ndask : {\"forbidden\", \"allowed\", \"parallelized\"}, default: \"forbidden\"\nHow to handle applying to objects containing lazy data in the form of\ndask arrays:\n\n- 'forbidden' (default): raise an error if a dask array is encountered.\n- 'allowed': pass dask arrays directly on to ``func``. Prefer this option if\n``func`` natively supports dask arrays.\n- 'parallelized': automatically parallelize ``func`` if any of the\ninputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output\narguments are supported. Only use this option if ``func`` does not natively\nsupport dask arrays (e.g. converts them to numpy arrays).\ndask_gufunc_kwargs : dict, optional\nOptional keyword arguments passed to :py:func:`dask.array.apply_gufunc` if\ndask='parallelized'. Possible keywords are ``output_sizes``, ``allow_rechunk``\nand ``meta``.\noutput_dtypes : list of dtype, optional\nOptional list of output dtypes. Only used if ``dask='parallelized'`` or\n``vectorize=True``.\noutput_sizes : dict, optional\nOptional mapping from dimension names to sizes for outputs. Only used\nif dask='parallelized' and new dimensions (not found on inputs) appear\non outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``\nparameter. It will be removed as direct parameter in a future version.\nmeta : optional\nSize-0 object representing the type of array wrapped by dask array. Passed on to\n:py:func:`dask.array.apply_gufunc`. ``meta`` should be given in the\n``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter\na future version.\n\nReturns\n-------\nSingle value or tuple of Dataset, DataArray, Variable, dask.array.Array or\nnumpy.ndarray, the first type on that list to appear on an input.\n\nNotes\n-----\nThis function is designed for the more common case where ``func`` can work on numpy\narrays. If ``func`` needs to manipulate a whole xarray object subset to each block\nit is possible to use :py:func:`xarray.map_blocks`.\n\nNote that due to the overhead :py:func:`xarray.map_blocks` is considerably slower than ``apply_ufunc``.\n\nExamples\n--------\nCalculate the vector magnitude of two arguments:\n\n>>> def magnitude(a, b):\n...     func = lambda x, y: np.sqrt(x**2 + y**2)\n...     return xr.apply_ufunc(func, a, b)\n...\n\nYou can now apply ``magnitude()`` to :py:class:`DataArray` and :py:class:`Dataset`\nobjects, with automatically preserved dimensions and coordinates, e.g.,\n\n>>> array = xr.DataArray([1, 2, 3], coords=[(\"x\", [0.1, 0.2, 0.3])])\n>>> magnitude(array, -array)\n<xarray.DataArray (x: 3)>\narray([1.41421356, 2.82842712, 4.24264069])\nCoordinates:\n* x        (x) float64 0.1 0.2 0.3\n\nPlain scalars, numpy arrays and a mix of these with xarray objects is also\nsupported:\n\n>>> magnitude(3, 4)\n5.0\n>>> magnitude(3, np.array([0, 4]))\narray([3., 5.])\n>>> magnitude(array, 0)\n<xarray.DataArray (x: 3)>\narray([1., 2., 3.])\nCoordinates:\n* x        (x) float64 0.1 0.2 0.3\n\nOther examples of how you could use ``apply_ufunc`` to write functions to\n(very nearly) replicate existing xarray functionality:\n\nCompute the mean (``.mean``) over one dimension:\n\n>>> def mean(obj, dim):\n...     # note: apply always moves core dimensions to the end\n...     return apply_ufunc(\n...         np.mean, obj, input_core_dims=[[dim]], kwargs={\"axis\": -1}\n...     )\n...\n\nInner product over a specific dimension (like :py:func:`dot`):\n\n>>> def _inner(x, y):\n...     result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])\n...     return result[..., 0, 0]\n...\n>>> def inner_product(a, b, dim):\n...     return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])\n...\n\nStack objects along a new dimension (like :py:func:`concat`):\n\n>>> def stack(objects, dim, new_coord):\n...     # note: this version does not stack coordinates\n...     func = lambda *x: np.stack(x, axis=-1)\n...     result = apply_ufunc(\n...         func,\n...         *objects,\n...         output_core_dims=[[dim]],\n...         join=\"outer\",\n...         dataset_fill_value=np.nan\n...     )\n...     result[dim] = new_coord\n...     return result\n...\n\nIf your function is not vectorized but can be applied only to core\ndimensions, you can use ``vectorize=True`` to turn into a vectorized\nfunction. This wraps :py:func:`numpy.vectorize`, so the operation isn't\nterribly fast. Here we'll use it to calculate the distance between\nempirical samples from two probability distributions, using a scipy\nfunction that needs to be applied to vectors:\n\n>>> import scipy.stats\n>>> def earth_mover_distance(first_samples, second_samples, dim=\"ensemble\"):\n...     return apply_ufunc(\n...         scipy.stats.wasserstein_distance,\n...         first_samples,\n...         second_samples,\n...         input_core_dims=[[dim], [dim]],\n...         vectorize=True,\n...     )\n...\n\nMost of NumPy's builtin functions already broadcast their inputs\nappropriately for use in ``apply_ufunc``. You may find helper functions such as\n:py:func:`numpy.broadcast_arrays` helpful in writing your function. ``apply_ufunc`` also\nworks well with :py:func:`numba.vectorize` and :py:func:`numba.guvectorize`.\n\nSee Also\n--------\nnumpy.broadcast_arrays\nnumba.vectorize\nnumba.guvectorize\ndask.array.apply_gufunc\nxarray.map_blocks\n:ref:`dask.automatic-parallelization`\nUser guide describing :py:func:`apply_ufunc` and :py:func:`map_blocks`.\n\nReferences\n----------\n.. [1] https://numpy.org/doc/stable/reference/ufuncs.html\n.. [2] https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html\n\"\"\"\nfrom .dataarray import DataArray\nfrom .groupby import GroupBy\nfrom .variable import Variable\n\nif input_core_dims is None:\ninput_core_dims = ((),) * (len(args))\nelif len(input_core_dims) != len(args):\nraise ValueError(\nf\"input_core_dims must be None or a tuple with the length same to \"\nf\"the number of arguments. \"\nf\"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, \"\nf\" but number of args is {len(args)}.\"\n)\n\nif kwargs is None:\nkwargs = {}\n\nsignature = _UFuncSignature(input_core_dims, output_core_dims)\n\nif exclude_dims:\nif not isinstance(exclude_dims, set):\nraise TypeError(\nf\"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead.\"\n)\nif not exclude_dims <= signature.all_core_dims:\nraise ValueError(\nf\"each dimension in `exclude_dims` must also be a \"\nf\"core dimension in the function signature. \"\nf\"Please make {(exclude_dims - signature.all_core_dims)} a core dimension\"\n)\n\n# handle dask_gufunc_kwargs\nif dask == \"parallelized\":\nif dask_gufunc_kwargs is None:\ndask_gufunc_kwargs = {}\nelse:\ndask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n# todo: remove warnings after deprecation cycle\nif meta is not None:\nwarnings.warn(\n\"``meta`` should be given in the ``dask_gufunc_kwargs`` parameter.\"\n\" It will be removed as direct parameter in a future version.\",\nFutureWarning,\nstacklevel=2,\n)\ndask_gufunc_kwargs.setdefault(\"meta\", meta)\nif output_sizes is not None:\nwarnings.warn(\n\"``output_sizes`` should be given in the ``dask_gufunc_kwargs`` \"\n\"parameter. It will be removed as direct parameter in a future \"\n\"version.\",\nFutureWarning,\nstacklevel=2,\n)\ndask_gufunc_kwargs.setdefault(\"output_sizes\", output_sizes)\n\nif kwargs:\nfunc = functools.partial(func, **kwargs)\n\nif keep_attrs is None:\nkeep_attrs = _get_keep_attrs(default=False)\n\nif isinstance(keep_attrs, bool):\nkeep_attrs = \"override\" if keep_attrs else \"drop\"\n\nvariables_vfunc = functools.partial(\napply_variable_ufunc,\nfunc,\nsignature=signature,\nexclude_dims=exclude_dims,\nkeep_attrs=keep_attrs,\ndask=dask,\nvectorize=vectorize,\noutput_dtypes=output_dtypes,\ndask_gufunc_kwargs=dask_gufunc_kwargs,\n)\n\n# feed groupby-apply_ufunc through apply_groupby_func\nif any(isinstance(a, GroupBy) for a in args):\nthis_apply = functools.partial(\napply_ufunc,\nfunc,\ninput_core_dims=input_core_dims,\noutput_core_dims=output_core_dims,\nexclude_dims=exclude_dims,\njoin=join,\ndataset_join=dataset_join,\ndataset_fill_value=dataset_fill_value,\nkeep_attrs=keep_attrs,\ndask=dask,\nvectorize=vectorize,\noutput_dtypes=output_dtypes,\ndask_gufunc_kwargs=dask_gufunc_kwargs,\n)\nreturn apply_groupby_func(this_apply, *args)\n# feed datasets apply_variable_ufunc through apply_dataset_vfunc\nelif any(is_dict_like(a) for a in args):\nreturn apply_dataset_vfunc(\nvariables_vfunc,\n*args,\nsignature=signature,\njoin=join,\nexclude_dims=exclude_dims,\ndataset_join=dataset_join,\nfill_value=dataset_fill_value,\nkeep_attrs=keep_attrs,\n)\n# feed DataArray apply_variable_ufunc through apply_dataarray_vfunc\nelif any(isinstance(a, DataArray) for a in args):\nreturn apply_dataarray_vfunc(\nvariables_vfunc,\n*args,\nsignature=signature,\njoin=join,\nexclude_dims=exclude_dims,\nkeep_attrs=keep_attrs,\n)\n# feed Variables directly through apply_variable_ufunc\nelif any(isinstance(a, Variable) for a in args):\nreturn variables_vfunc(*args)\nelse:\n# feed anything else through apply_array_ufunc\nreturn apply_array_ufunc(func, *args, dask=dask)\n\n\ndef cov(da_a, da_b, dim=None, ddof=1):\n\"\"\"\nCompute covariance between two DataArray objects along a shared dimension.\n\nParameters\n----------\nda_a : DataArray\nArray to compute.\nda_b : DataArray\nArray to compute.\ndim : str, optional\nThe dimension along which the covariance will be computed\nddof : int, optional\nIf ddof=1, covariance is normalized by N-1, giving an unbiased estimate,\nelse normalization is by N.\n\nReturns\n-------\ncovariance : DataArray\n\nSee Also\n--------\npandas.Series.cov : corresponding pandas function\nxarray.corr : respective function to calculate correlation\n\nExamples\n--------\n>>> from xarray import DataArray\n>>> da_a = DataArray(\n...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\n...     dims=(\"space\", \"time\"),\n...     coords=[\n...         (\"space\", [\"IA\", \"IL\", \"IN\"]),\n...         (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n...     ],\n... )\n>>> da_a\n<xarray.DataArray (space: 3, time: 3)>\narray([[1. , 2. , 3. ],\n[0.1, 0.2, 0.3],\n[3.2, 0.6, 1.8]])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n* time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n>>> da_b = DataArray(\n...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n...     dims=(\"space\", \"time\"),\n...     coords=[\n...         (\"space\", [\"IA\", \"IL\", \"IN\"]),\n...         (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n...     ],\n... )\n>>> da_b\n<xarray.DataArray (space: 3, time: 3)>\narray([[ 0.2,  0.4,  0.6],\n[15. , 10. ,  5. ],\n[ 3.2,  0.6,  1.8]])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n* time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n>>> xr.cov(da_a, da_b)\n<xarray.DataArray ()>\narray(-3.53055556)\n>>> xr.cov(da_a, da_b, dim=\"time\")\n<xarray.DataArray (space: 3)>\narray([ 0.2       , -0.5       ,  1.69333333])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n\"\"\"\nfrom .dataarray import DataArray\n\nif any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\nraise TypeError(\n\"Only xr.DataArray is supported.\"\n\"Given {}.\".format([type(arr) for arr in [da_a, da_b]])\n)\n\nreturn _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method=\"cov\")\n\n\ndef corr(da_a, da_b, dim=None):\n\"\"\"\nCompute the Pearson correlation coefficient between\ntwo DataArray objects along a shared dimension.\n\nParameters\n----------\nda_a : DataArray\nArray to compute.\nda_b : DataArray\nArray to compute.\ndim : str, optional\nThe dimension along which the correlation will be computed\n\nReturns\n-------\ncorrelation: DataArray\n\nSee Also\n--------\npandas.Series.corr : corresponding pandas function\nxarray.cov : underlying covariance function\n\nExamples\n--------\n>>> from xarray import DataArray\n>>> da_a = DataArray(\n...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),\n...     dims=(\"space\", \"time\"),\n...     coords=[\n...         (\"space\", [\"IA\", \"IL\", \"IN\"]),\n...         (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n...     ],\n... )\n>>> da_a\n<xarray.DataArray (space: 3, time: 3)>\narray([[1. , 2. , 3. ],\n[0.1, 0.2, 0.3],\n[3.2, 0.6, 1.8]])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n* time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n>>> da_b = DataArray(\n...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),\n...     dims=(\"space\", \"time\"),\n...     coords=[\n...         (\"space\", [\"IA\", \"IL\", \"IN\"]),\n...         (\"time\", pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)),\n...     ],\n... )\n>>> da_b\n<xarray.DataArray (space: 3, time: 3)>\narray([[ 0.2,  0.4,  0.6],\n[15. , 10. ,  5. ],\n[ 3.2,  0.6,  1.8]])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n* time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03\n>>> xr.corr(da_a, da_b)\n<xarray.DataArray ()>\narray(-0.57087777)\n>>> xr.corr(da_a, da_b, dim=\"time\")\n<xarray.DataArray (space: 3)>\narray([ 1., -1.,  1.])\nCoordinates:\n* space    (space) <U2 'IA' 'IL' 'IN'\n\"\"\"\nfrom .dataarray import DataArray\n\nif any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):\nraise TypeError(\n\"Only xr.DataArray is supported.\"\n\"Given {}.\".format([type(arr) for arr in [da_a, da_b]])\n)\n\nreturn _cov_corr(da_a, da_b, dim=dim, method=\"corr\")\n\n\ndef _cov_corr(da_a, da_b, dim=None, ddof=0, method=None):\n\"\"\"\nInternal method for xr.cov() and xr.corr() so only have to\nsanitize the input arrays once and we don't repeat code.\n\"\"\"\n# 1. Broadcast the two arrays\nda_a, da_b = align(da_a, da_b, join=\"inner\", copy=False)\n\n# 2. Ignore the nans\nvalid_values = da_a.notnull() & da_b.notnull()\nda_a = da_a.where(valid_values)\nda_b = da_b.where(valid_values)\nvalid_count = valid_values.sum(dim) - ddof\n\n# 3. Detrend along the given dim\ndemeaned_da_a = da_a - da_a.mean(dim=dim)\ndemeaned_da_b = da_b - da_b.mean(dim=dim)\n\n# 4. Compute covariance along the given dim\n# N.B. `skipna=False` is required or there is a bug when computing\n# auto-covariance. E.g. Try xr.cov(da,da) for\n# da = xr.DataArray([[1, 2], [1, np.nan]], dims=[\"x\", \"time\"])\ncov = (demeaned_da_a * demeaned_da_b).sum(dim=dim, skipna=True, min_count=1) / (\nvalid_count\n)\n\nif method == \"cov\":\nreturn cov\n\nelse:\n# compute std + corr\nda_a_std = da_a.std(dim=dim)\nda_b_std = da_b.std(dim=dim)\ncorr = cov / (da_a_std * da_b_std)\nreturn corr\n\n\ndef cross(\na: DataArray | Variable, b: DataArray | Variable, *, dim: Hashable\n) -> DataArray | Variable:\n\"\"\"\nCompute the cross product of two (arrays of) vectors.\n\nThe cross product of `a` and `b` in :math:`R^3` is a vector\nperpendicular to both `a` and `b`. The vectors in `a` and `b` are\ndefined by the values along the dimension `dim` and can have sizes\n1, 2 or 3. Where the size of either `a` or `b` is\n1 or 2, the remaining components of the input vector is assumed to\nbe zero and the cross product calculated accordingly. In cases where\nboth input vectors have dimension 2, the z-component of the cross\nproduct is returned.\n\nParameters\n----------\na, b : DataArray or Variable\nComponents of the first and second vector(s).\ndim : hashable\nThe dimension along which the cross product will be computed.\nMust be available in both vectors.\n\nExamples\n--------\nVector cross-product with 3 dimensions:\n\n>>> a = xr.DataArray([1, 2, 3])\n>>> b = xr.DataArray([4, 5, 6])\n>>> xr.cross(a, b, dim=\"dim_0\")\n<xarray.DataArray (dim_0: 3)>\narray([-3,  6, -3])\nDimensions without coordinates: dim_0\n\nVector cross-product with 2 dimensions, returns in the perpendicular\ndirection:\n\n>>> a = xr.DataArray([1, 2])\n>>> b = xr.DataArray([4, 5])\n>>> xr.cross(a, b, dim=\"dim_0\")\n<xarray.DataArray ()>\narray(-3)\n\nVector cross-product with 3 dimensions but zeros at the last axis\nyields the same results as with 2 dimensions:\n\n>>> a = xr.DataArray([1, 2, 0])\n>>> b = xr.DataArray([4, 5, 0])\n>>> xr.cross(a, b, dim=\"dim_0\")\n<xarray.DataArray (dim_0: 3)>\narray([ 0,  0, -3])\nDimensions without coordinates: dim_0\n\nOne vector with dimension 2:\n\n>>> a = xr.DataArray(\n...     [1, 2],\n...     dims=[\"cartesian\"],\n...     coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n... )\n>>> b = xr.DataArray(\n...     [4, 5, 6],\n...     dims=[\"cartesian\"],\n...     coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n... )\n>>> xr.cross(a, b, dim=\"cartesian\")\n<xarray.DataArray (cartesian: 3)>\narray([12, -6, -3])\nCoordinates:\n* cartesian  (cartesian) <U1 'x' 'y' 'z'\n\nOne vector with dimension 2 but coords in other positions:\n\n>>> a = xr.DataArray(\n...     [1, 2],\n...     dims=[\"cartesian\"],\n...     coords=dict(cartesian=([\"cartesian\"], [\"x\", \"z\"])),\n... )\n>>> b = xr.DataArray(\n...     [4, 5, 6],\n...     dims=[\"cartesian\"],\n...     coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n... )\n>>> xr.cross(a, b, dim=\"cartesian\")\n<xarray.DataArray (cartesian: 3)>\narray([-10,   2,   5])\nCoordinates:\n* cartesian  (cartesian) <U1 'x' 'y' 'z'\n\nMultiple vector cross-products. Note that the direction of the\ncross product vector is defined by the right-hand rule:\n\n>>> a = xr.DataArray(\n...     [[1, 2, 3], [4, 5, 6]],\n...     dims=(\"time\", \"cartesian\"),\n...     coords=dict(\n...         time=([\"time\"], [0, 1]),\n...         cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n...     ),\n... )\n>>> b = xr.DataArray(\n...     [[4, 5, 6], [1, 2, 3]],\n...     dims=(\"time\", \"cartesian\"),\n...     coords=dict(\n...         time=([\"time\"], [0, 1]),\n...         cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n...     ),\n... )\n>>> xr.cross(a, b, dim=\"cartesian\")\n<xarray.DataArray (time: 2, cartesian: 3)>\narray([[-3,  6, -3],\n[ 3, -6,  3]])\nCoordinates:\n* time       (time) int64 0 1\n* cartesian  (cartesian) <U1 'x' 'y' 'z'\n\nCross can be called on Datasets by converting to DataArrays and later\nback to a Dataset:\n\n>>> ds_a = xr.Dataset(dict(x=(\"dim_0\", [1]), y=(\"dim_0\", [2]), z=(\"dim_0\", [3])))\n>>> ds_b = xr.Dataset(dict(x=(\"dim_0\", [4]), y=(\"dim_0\", [5]), z=(\"dim_0\", [6])))\n>>> c = xr.cross(\n...     ds_a.to_array(\"cartesian\"), ds_b.to_array(\"cartesian\"), dim=\"cartesian\"\n... )\n>>> c.to_dataset(dim=\"cartesian\")\n<xarray.Dataset>\nDimensions:  (dim_0: 1)\nDimensions without coordinates: dim_0\nData variables:\nx        (dim_0) int64 -3\ny        (dim_0) int64 6\nz        (dim_0) int64 -3\n\nSee Also\n--------\nnumpy.cross : Corresponding numpy function\n\"\"\"\n\nif dim not in a.dims:\nraise ValueError(f\"Dimension {dim!r} not on a\")\nelif dim not in b.dims:\nraise ValueError(f\"Dimension {dim!r} not on b\")\n\nif not 1 <= a.sizes[dim] <= 3:\nraise ValueError(\nf\"The size of {dim!r} on a must be 1, 2, or 3 to be \"\nf\"compatible with a cross product but is {a.sizes[dim]}\"\n)\nelif not 1 <= b.sizes[dim] <= 3:\nraise ValueError(\nf\"The size of {dim!r} on b must be 1, 2, or 3 to be \"\nf\"compatible with a cross product but is {b.sizes[dim]}\"\n)\n\nall_dims = list(dict.fromkeys(a.dims + b.dims))\n\nif a.sizes[dim] != b.sizes[dim]:\n# Arrays have different sizes. Append zeros where the smaller\n# array is missing a value, zeros will not affect np.cross:\n\nif (\nnot isinstance(a, Variable)  # Only used to make mypy happy.\nand dim in getattr(a, \"coords\", {})\nand not isinstance(b, Variable)  # Only used to make mypy happy.\nand dim in getattr(b, \"coords\", {})\n):\n# If the arrays have coords we know which indexes to fill\n# with zeros:\na, b = align(\na,\nb,\nfill_value=0,\njoin=\"outer\",\nexclude=set(all_dims) - {dim},\n)\nelif min(a.sizes[dim], b.sizes[dim]) == 2:\n# If the array doesn't have coords we can only infer\n# that it has composite values if the size is at least 2.\n# Once padded, rechunk the padded array because apply_ufunc\n# requires core dimensions not to be chunked:\nif a.sizes[dim] < b.sizes[dim]:\na = a.pad({dim: (0, 1)}, constant_values=0)\n# TODO: Should pad or apply_ufunc handle correct chunking?\na = a.chunk({dim: -1}) if is_duck_dask_array(a.data) else a\nelse:\nb = b.pad({dim: (0, 1)}, constant_values=0)\n# TODO: Should pad or apply_ufunc handle correct chunking?\nb = b.chunk({dim: -1}) if is_duck_dask_array(b.data) else b\nelse:\nraise ValueError(\nf\"{dim!r} on {'a' if a.sizes[dim] == 1 else 'b'} is incompatible:\"\n\" dimensions without coordinates must have have a length of 2 or 3\"\n)\n\nc = apply_ufunc(\nnp.cross,\na,\nb,\ninput_core_dims=[[dim], [dim]],\noutput_core_dims=[[dim] if a.sizes[dim] == 3 else []],\ndask=\"parallelized\",\noutput_dtypes=[np.result_type(a, b)],\n)\nc = c.transpose(*all_dims, missing_dims=\"ignore\")\n\nreturn c\n\n\ndef dot(*arrays, dims=None, **kwargs):\n\"\"\"Generalized dot product for xarray objects. Like np.einsum, but\nprovides a simpler interface based on array dimensions.\n\nParameters\n----------\n*arrays : DataArray or Variable\nArrays to compute.\ndims : ..., str or tuple of str, optional\nWhich dimensions to sum over. Ellipsis ('...') sums over all dimensions.\nIf not specified, then all the common dimensions are summed over.\n**kwargs : dict\nAdditional keyword arguments passed to numpy.einsum or\ndask.array.einsum\n\nReturns\n-------\nDataArray\n\nExamples\n--------\n>>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"])\n>>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"])\n>>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=[\"c\", \"d\"])\n\n>>> da_a\n<xarray.DataArray (a: 3, b: 2)>\narray([[0, 1],\n[2, 3],\n[4, 5]])\nDimensions without coordinates: a, b\n\n>>> da_b\n<xarray.DataArray (a: 3, b: 2, c: 2)>\narray([[[ 0,  1],\n[ 2,  3]],\n<BLANKLINE>\n[[ 4,  5],\n[ 6,  7]],\n<BLANKLINE>\n[[ 8,  9],\n[10, 11]]])\nDimensions without coordinates: a, b, c\n\n>>> da_c\n<xarray.DataArray (c: 2, d: 3)>\narray([[0, 1, 2],\n[3, 4, 5]])\nDimensions without coordinates: c, d\n\n>>> xr.dot(da_a, da_b, dims=[\"a\", \"b\"])\n<xarray.DataArray (c: 2)>\narray([110, 125])\nDimensions without coordinates: c\n\n>>> xr.dot(da_a, da_b, dims=[\"a\"])\n<xarray.DataArray (b: 2, c: 2)>\narray([[40, 46],\n[70, 79]])\nDimensions without coordinates: b, c\n\n>>> xr.dot(da_a, da_b, da_c, dims=[\"b\", \"c\"])\n<xarray.DataArray (a: 3, d: 3)>\narray([[  9,  14,  19],\n[ 93, 150, 207],\n[273, 446, 619]])\nDimensions without coordinates: a, d\n\n>>> xr.dot(da_a, da_b)\n<xarray.DataArray (c: 2)>\narray([110, 125])\nDimensions without coordinates: c\n\n>>> xr.dot(da_a, da_b, dims=...)\n<xarray.DataArray ()>\narray(235)\n\"\"\"\nfrom .dataarray import DataArray\nfrom .variable import Variable\n\nif any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):\nraise TypeError(\n\"Only xr.DataArray and xr.Variable are supported.\"\n\"Given {}.\".format([type(arr) for arr in arrays])\n)\n\nif len(arrays) == 0:\nraise TypeError(\"At least one array should be given.\")\n\nif isinstance(dims, str):\ndims = (dims,)\n\ncommon_dims = set.intersection(*[set(arr.dims) for arr in arrays])\nall_dims = []\nfor arr in arrays:\nall_dims += [d for d in arr.dims if d not in all_dims]\n\neinsum_axes = \"abcdefghijklmnopqrstuvwxyz\"\ndim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}\n\nif dims is ...:\ndims = all_dims\nelif dims is None:\n# find dimensions that occur more than one times\ndim_counts = Counter()\nfor arr in arrays:\ndim_counts.update(arr.dims)\ndims = tuple(d for d, c in dim_counts.items() if c > 1)\n\ndims = tuple(dims)  # make dims a tuple\n\n# dimensions to be parallelized\nbroadcast_dims = tuple(d for d in all_dims if d in common_dims and d not in dims)\ninput_core_dims = [\n[d for d in arr.dims if d not in broadcast_dims] for arr in arrays\n]\noutput_core_dims = [tuple(d for d in all_dims if d not in dims + broadcast_dims)]\n\n# construct einsum subscripts, such as '...abc,...ab->...c'\n# Note: input_core_dims are always moved to the last position\nsubscripts_list = [\n\"...\" + \"\".join(dim_map[d] for d in ds) for ds in input_core_dims\n]\nsubscripts = \",\".join(subscripts_list)\nsubscripts += \"->...\" + \"\".join(dim_map[d] for d in output_core_dims[0])\n\njoin = OPTIONS[\"arithmetic_join\"]\n# using \"inner\" emulates `(a * b).sum()` for all joins (except \"exact\")\nif join != \"exact\":\njoin = \"inner\"\n\n# subscripts should be passed to np.einsum as arg, not as kwargs. We need\n# to construct a partial function for apply_ufunc to work.\nfunc = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)\nresult = apply_ufunc(\nfunc,\n*arrays,\ninput_core_dims=input_core_dims,\noutput_core_dims=output_core_dims,\njoin=join,\ndask=\"allowed\",\n)\nreturn result.transpose(*all_dims, missing_dims=\"ignore\")\n\n\ndef where(cond, x, y, keep_attrs=None):\n\"\"\"Return elements from `x` or `y` depending on `cond`.\n\nPerforms xarray-like broadcasting across input arguments.\n\nAll dimension coordinates on `x` and `y`  must be aligned with each\nother and with `cond`.\n\nParameters\n----------\ncond : scalar, array, Variable, DataArray or Dataset\nWhen True, return values from `x`, otherwise returns values from `y`.\nx : scalar, array, Variable, DataArray or Dataset\nvalues to choose from where `cond` is True\ny : scalar, array, Variable, DataArray or Dataset\nvalues to choose from where `cond` is False\nkeep_attrs : bool or str or callable, optional\nHow to treat attrs. If True, keep the attrs of `x`.\n\nReturns\n-------\nDataset, DataArray, Variable or array\nIn priority order: Dataset, DataArray, Variable or array, whichever\ntype appears as an input argument.\n\nExamples\n--------\n>>> x = xr.DataArray(\n...     0.1 * np.arange(10),\n...     dims=[\"lat\"],\n...     coords={\"lat\": np.arange(10)},\n...     name=\"sst\",\n... )\n>>> x\n<xarray.DataArray 'sst' (lat: 10)>\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\nCoordinates:\n* lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\n\n>>> xr.where(x < 0.5, x, x * 100)\n<xarray.DataArray 'sst' (lat: 10)>\narray([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])\nCoordinates:\n* lat      (lat) int64 0 1 2 3 4 5 6 7 8 9\n\n>>> y = xr.DataArray(\n...     0.1 * np.arange(9).reshape(3, 3),\n...     dims=[\"lat\", \"lon\"],\n...     coords={\"lat\": np.arange(3), \"lon\": 10 + np.arange(3)},\n...     name=\"sst\",\n... )\n>>> y\n<xarray.DataArray 'sst' (lat: 3, lon: 3)>\narray([[0. , 0.1, 0.2],\n[0.3, 0.4, 0.5],\n[0.6, 0.7, 0.8]])\nCoordinates:\n* lat      (lat) int64 0 1 2\n* lon      (lon) int64 10 11 12\n\n>>> xr.where(y.lat < 1, y, -1)\n<xarray.DataArray (lat: 3, lon: 3)>\narray([[ 0. ,  0.1,  0.2],\n[-1. , -1. , -1. ],\n[-1. , -1. , -1. ]])\nCoordinates:\n* lat      (lat) int64 0 1 2\n* lon      (lon) int64 10 11 12\n\n>>> cond = xr.DataArray([True, False], dims=[\"x\"])\n>>> x = xr.DataArray([1, 2], dims=[\"y\"])\n>>> xr.where(cond, x, 0)\n<xarray.DataArray (x: 2, y: 2)>\narray([[1, 2],\n[0, 0]])\nDimensions without coordinates: x, y\n\nSee Also\n--------\nnumpy.where : corresponding numpy function\nDataset.where, DataArray.where :\nequivalent methods\n\"\"\"\nif keep_attrs is None:\nkeep_attrs = _get_keep_attrs(default=False)\n\nif keep_attrs is True:\n# keep the attributes of x, the second parameter, by default to\n# be consistent with the `where` method of `DataArray` and `Dataset`\nkeep_attrs = lambda attrs, context: attrs[1]\n\n# alignment for three arguments is complicated, so don't support it yet\nreturn apply_ufunc(\nduck_array_ops.where,\ncond,\nx,\ny,\njoin=\"exact\",\ndataset_join=\"exact\",\ndask=\"allowed\",\nkeep_attrs=keep_attrs,\n)\n\n\ndef polyval(coord, coeffs, degree_dim=\"degree\"):\n\"\"\"Evaluate a polynomial at specific values\n\nParameters\n----------\ncoord : DataArray\nThe 1D coordinate along which to evaluate the polynomial.\ncoeffs : DataArray\nCoefficients of the polynomials.\ndegree_dim : str, default: \"degree\"\nName of the polynomial degree dimension in `coeffs`.\n\nSee Also\n--------\nxarray.DataArray.polyfit\nnumpy.polyval\n\"\"\"\nfrom .dataarray import DataArray\nfrom .missing import get_clean_interp_index\n\nx = get_clean_interp_index(coord, coord.name, strict=False)\n\ndeg_coord = coeffs[degree_dim]\n\nlhs = DataArray(\nnp.vander(x, int(deg_coord.max()) + 1),\ndims=(coord.name, degree_dim),\ncoords={coord.name: coord, degree_dim: np.arange(deg_coord.max() + 1)[::-1]},\n)\nreturn (lhs * coeffs).sum(degree_dim)\n\n\ndef _calc_idxminmax(\n*,\narray,\nfunc: Callable,\ndim: Hashable = None,\nskipna: bool = None,\nfill_value: Any = dtypes.NA,\nkeep_attrs: bool = None,\n):\n\"\"\"Apply common operations for idxmin and idxmax.\"\"\"\n# This function doesn't make sense for scalars so don't try\nif not array.ndim:\nraise ValueError(\"This function does not apply for scalars\")\n\nif dim is not None:\npass  # Use the dim if available\nelif array.ndim == 1:\n# it is okay to guess the dim if there is only 1\ndim = array.dims[0]\nelse:\n# The dim is not specified and ambiguous.  Don't guess.\nraise ValueError(\"Must supply 'dim' argument for multidimensional arrays\")\n\nif dim not in array.dims:\nraise KeyError(f'Dimension \"{dim}\" not in dimension')\nif dim not in array.coords:\nraise KeyError(f'Dimension \"{dim}\" does not have coordinates')\n\n# These are dtypes with NaN values argmin and argmax can handle\nna_dtypes = \"cfO\"\n\nif skipna or (skipna is None and array.dtype.kind in na_dtypes):\n# Need to skip NaN values since argmin and argmax can't handle them\nallna = array.isnull().all(dim)\narray = array.where(~allna, 0)\n\n# This will run argmin or argmax.\nindx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)\n\n# Handle dask arrays.\nif is_duck_dask_array(array.data):\nimport dask.array\n\nchunks = dict(zip(array.dims, array.chunks))\ndask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])\nres = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))\n# we need to attach back the dim name\nres.name = dim\nelse:\nres = array[dim][(indx,)]\n# The dim is gone but we need to remove the corresponding coordinate.\ndel res.coords[dim]\n\nif skipna or (skipna is None and array.dtype.kind in na_dtypes):\n# Put the NaN values back in after removing them\nres = res.where(~allna, fill_value)\n\n# Copy attributes from argmin/argmax, if any\nres.attrs = indx.attrs\n\nreturn res\n\n\ndef unify_chunks(*objects: T_Xarray) -> tuple[T_Xarray, ...]:\n\"\"\"\nGiven any number of Dataset and/or DataArray objects, returns\nnew objects with unified chunk size along all chunked dimensions.\n\nReturns\n-------\nunified (DataArray or Dataset) \u2013 Tuple of objects with the same type as\n*objects with consistent chunk sizes for all dask-array variables\n\nSee Also\n--------\ndask.array.core.unify_chunks\n\"\"\"\nfrom .dataarray import DataArray\n\n# Convert all objects to datasets\ndatasets = [\nobj._to_temp_dataset() if isinstance(obj, DataArray) else obj.copy()\nfor obj in objects\n]\n\n# Get arguments to pass into dask.array.core.unify_chunks\nunify_chunks_args = []\nsizes: dict[Hashable, int] = {}\nfor ds in datasets:\nfor v in ds._variables.values():\nif v.chunks is not None:\n# Check that sizes match across different datasets\nfor dim, size in v.sizes.items():\ntry:\nif sizes[dim] != size:\nraise ValueError(\nf\"Dimension {dim!r} size mismatch: {sizes[dim]} != {size}\"\n)\nexcept KeyError:\nsizes[dim] = size\nunify_chunks_args += [v._data, v._dims]\n\n# No dask arrays: Return inputs\nif not unify_chunks_args:\nreturn objects\n\n# Run dask.array.core.unify_chunks\nfrom dask.array.core import unify_chunks\n\n_, dask_data = unify_chunks(*unify_chunks_args)\ndask_data_iter = iter(dask_data)\nout = []\nfor obj, ds in zip(objects, datasets):\nfor k, v in ds._variables.items():\nif v.chunks is not None:\nds._variables[k] = v.copy(data=next(dask_data_iter))\nout.append(obj._from_temp_dataset(ds) if isinstance(obj, DataArray) else ds)\n\nreturn tuple(out)\n"
    },
    "ground_truth_patch": {
        "xarray/core/computation.py": "     \"\"\"\n     if keep_attrs is None:\n         keep_attrs = _get_keep_attrs(default=False)\n     if keep_attrs is True:\n         # keep the attributes of x, the second parameter, by default to\n         # be consistent with the `where` method of `DataArray` and `Dataset`\nkeep_attrs = lambda attrs, context: getattr(x, \"attrs\", {})\n \n     # alignment for three arguments is complicated, so don't support it yet\n     return apply_ufunc(\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 includes a conditional check to determine if `keep_attrs` is None, and if so, it fetches default attributes via `_get_keep_attrs`. If `keep_attrs` is True, it establishes a lambda function to access the attributes from the variable `x`. It includes a comment indicating the alignment complexity for three arguments. The line connects to a function call `apply_ufunc()` to apply a ufunc in an xarray context. On the other hand, code sample 2 has a similar function setup that checks if `keep_attrs` is None or True, then defines `keep_attrs` with a lambda function that retrieves `attrs` from `x`, and includes the same comment about alignment for three arguments before returning the call to `apply_ufunc()`. Both samples share similar logic and structure but exhibit different definitions for `keep_attrs` functionality in relation to accessing `x`'s attributes, marking it slightly altered.\", \"is_clone\": true, \"classification\": \"type-2\", \"reasoning\": \"These code samples are type-2 clones because they are identical in structure and logic except for the differences in identifier names and the small alteration of retrieving attributes from the variable `x`.\"}"
}