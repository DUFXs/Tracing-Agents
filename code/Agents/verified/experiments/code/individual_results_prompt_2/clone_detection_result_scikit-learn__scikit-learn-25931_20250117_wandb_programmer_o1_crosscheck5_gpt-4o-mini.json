{
    "instance_id": "scikit-learn__scikit-learn-25931",
    "agent": "20250117_wandb_programmer_o1_crosscheck5",
    "agent_patch": {
        "sklearn/ensemble/_iforest.py": "index 4d6c1c3f0..3d4392b89 100644\n     def fit(self, X, y=None, sample_weight=None):\n         \"\"\"\n         Fit estimator.\n\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_samples, n_features)\nThe input samples. Use dtype=np.float32 for maximum\n             efficiency. Sparse matrices are also supported, use sparse\ncsc_matrix for maximum efficiency.\n\n         y : Ignored\n             Not used, present for API consistency by convention.\n\n         sample_weight : array-like of shape (n_samples,), default=None\n             Sample weights. If None, then samples are equally weighted.\n\n         Returns\n         -------\n         self : object\n             # Pre-sort indices to avoid that each individual tree of the\n             # ensemble sorts the indices.\n             X.sort_indices()\n\n         rnd = check_random_state(self.random_state)\n         y = rnd.uniform(size=X.shape[0])\n\n         # ensure that max_sample is in [1, n_samples]:\n         n_samples = X.shape[0]\n\n         if isinstance(self.max_samples, str) and self.max_samples == \"auto\":\n             max_samples = min(256, n_samples)\n         elif isinstance(self.max_samples, numbers.Integral):\n             if self.max_samples > n_samples:\n                 warn(\n                 max_samples = self.max_samples\n         else:  # max_samples is float\n             max_samples = int(self.max_samples * X.shape[0])\n\n         self.max_samples_ = max_samples\n         max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n         super()._fit(\n             sample_weight=sample_weight,\n             check_input=False,\n         )\n\n         self._average_path_length_per_tree, self._decision_path_lengths = zip(\n             *[\n                 (\n                 for tree in self.estimators_\n             ]\n         )\n\n         if self.contamination == \"auto\":\n             # 0.5 plays a special role as described in the original paper.\n             # we take the opposite as we consider the opposite of their score.\n             self.offset_ = -0.5\n             return self\n\n         # else, define offset_ wrt contamination parameter\n# skip the second data validation step:\nraw_scores = self._compute_chunked_score_samples(X)\nfinal_scores = -raw_scores\nself.offset_ = np.percentile(final_scores, 100.0 * self.contamination)\n\n         return self\n\n \n     def predict(self, X):\n         \"\"\"\n     def score_samples(self, X):\n         \"\"\"\n         Opposite of the anomaly score defined in the original paper.\n\n         The anomaly score of an input sample is computed as\n         the mean anomaly score of the trees in the forest.\n\n         The measure of normality of an observation given a tree is the depth\n         of the leaf containing this observation, which is equivalent to\n         the number of splittings required to isolate this point. In case of\n         several observations n_left in the leaf, the average path length of\n         a n_left samples isolation tree is added.\n\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n             The input samples.\n\n         Returns\n         -------\n         scores : ndarray of shape (n_samples,)\n             The anomaly score of the input samples.\n             The lower, the more abnormal.\n         \"\"\"\nfrom ..tree._tree import DTYPE as tree_dtype\n         check_is_fitted(self)\n\n         # Check data\nX = self._validate_data(X, accept_sparse=\"csr\", dtype=tree_dtype, reset=False)\n\n         # Take the opposite of the scores as bigger is better (here less\n         # abnormal)\n         return -self._compute_chunked_score_samples(X)\n\n \n     def _compute_chunked_score_samples(self, X):\n         n_samples = _num_samples(X)\n",
        "test_iforest_warning.py": "new file mode 100644\nindex 000000000..c27f24e74\nimport sys\nimport warnings\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\n\ndef main():\nprint(\"Starting test for IsolationForest feature-name warning...\")\nX = pd.DataFrame({\"a\": [-1.1, 0.3, 0.5, 100]})\nwith warnings.catch_warnings(record=True) as w:\nIsolationForest(random_state=0, contamination=0.05).fit(X)\n# Check for the undesired warning\nfor warn_msg in w:\nmsg_str = str(warn_msg.message)\nprint(f\"Warning captured: {msg_str}\")\nif \"X does not have valid feature names\" in msg_str:\nprint(\"Test failed. Found the undesired warning.\")\nsys.exit(1)\nprint(\"No undesired warning found. Test passed.\")\nsys.exit(0)\n\nif __name__ == \"__main__\":\nmain()\n"
    },
    "ground_truth_patch": {
        "sklearn/ensemble/_iforest.py": "             self.offset_ = -0.5\n             return self\n \n# Else, define offset_ wrt contamination parameter\n# To avoid performing input validation a second time we call\n# _score_samples rather than score_samples\nself.offset_ = np.percentile(self._score_samples(X), 100.0 * self.contamination)\n \n         return self\n \n             The anomaly score of the input samples.\n             The lower, the more abnormal.\n         \"\"\"\n         # Check data\n         X = self._validate_data(X, accept_sparse=\"csr\", dtype=np.float32, reset=False)\n \nreturn self._score_samples(X)\n\ndef _score_samples(self, X):\n\"\"\"Private version of score_samples without input validation.\n\nInput validation would remove feature names, so we disable it.\n\"\"\"\n# Code structure from ForestClassifier/predict_proba\n\ncheck_is_fitted(self)\n\n# Take the opposite of the scores as bigger is better (here less abnormal)\n         return -self._compute_chunked_score_samples(X)\n \n     def _compute_chunked_score_samples(self, X):\n"
    },
    "response": "{\"explanation\": \"The code in code sample 1 contains the implementation of the Isolation Forest algorithm in scikit-learn, specifically the methods for fitting the model and predicting scores based on input samples. It includes functions for fitting the estimator, computing anomaly scores, and handling input data validation. The code sample also manages parameters like sample weight and contamination, and encapsulates internal logic regarding decision paths. The second code sample appears to complement the first by containing a portion focusing on the anomaly scoring process, particularly a private variant of the score_samples method without input validation. The second code instead of merely validating input directly computes scores without that validation. They both refer to how anomalies are detected within Isolation Forest, but the second sample's focus is narrower. Additionally, while code sample 1 is more extensive, code sample 2 seems to refer to some of the same functionalities but is incomplete and mixed with comments and slight structural variances. Hence, they are not identical in structure and functionality while being related in domain.\", \"is_clone\": false, \"classification\": \"N/A\", \"reasoning\": \"These code samples are not clones because they demonstrate different functionalities within the isolation forest algorithms; the first implements the fit and score logic, while the second provides a specialized version of score calculation without validating input. The structural and functional differences classify them as non-clones.\"}"
}